{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Colab_TrainNetwork_VideoAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "xNi9s1dboEJN",
        "c4FczXGDoEJU",
        "xZygsb2DoEJc"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lolaBerkowitz/position_tracking/blob/master/Colab_TrainNetwork_VideoAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RK255E7YoEIt"
      },
      "source": [
        "# DeepLabCut Toolbox - Colab\n",
        "https://github.com/AlexEMG/DeepLabCut\n",
        "\n",
        "This notebook illustrates how to use the cloud to:\n",
        "- create a training set\n",
        "- train a network\n",
        "- evaluate a network\n",
        "- create simple quality check plots\n",
        "- analyze novel videos!\n",
        "\n",
        "###This notebook assumes you already have a project folder with labeled data! \n",
        "\n",
        "This notebook demonstrates the necessary steps to use DeepLabCut for your own project.\n",
        "\n",
        "This shows the most simple code to do so, but many of the functions have additional features, so please check out the overview & the protocol paper!\n",
        "\n",
        "Nath\\*, Mathis\\* et al.: Using DeepLabCut for markerless pose estimation during behavior across species. Nature Protocols, 2019.\n",
        "\n",
        "\n",
        "Paper: https://www.nature.com/articles/s41596-019-0176-0\n",
        "\n",
        "Pre-print: https://www.biorxiv.org/content/biorxiv/early/2018/11/24/476531.full.pdf\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAL104n3GriG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4be1fb39-74e8-49ba-a44e-dc238fc10ffe"
      },
      "source": [
        "# Link google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "txoddlM8hLKm"
      },
      "source": [
        "## First, go to \"Runtime\" ->\"change runtime type\"->select \"Python3\", and then select \"GPU\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q23BzhA6CXxu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "94d08d06-5ced-4119-d00e-29822deb799f"
      },
      "source": [
        "#(this will take a few minutes to install all the dependences!)\n",
        "!pip install deeplabcut"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deeplabcut\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/01/8b669887369739ccfcaac13f55800eefb56d5667b9fdca952a1d70017089/deeplabcut-2.1.8.2-py3-none-any.whl (400kB)\n",
            "\r\u001b[K     |▉                               | 10kB 23.0MB/s eta 0:00:01\r\u001b[K     |█▋                              | 20kB 3.5MB/s eta 0:00:01\r\u001b[K     |██▌                             | 30kB 4.7MB/s eta 0:00:01\r\u001b[K     |███▎                            | 40kB 5.0MB/s eta 0:00:01\r\u001b[K     |████                            | 51kB 4.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 61kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 71kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 81kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 92kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 102kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████                       | 112kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 122kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 133kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 143kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 153kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 163kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 174kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 184kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 194kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 204kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 215kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 225kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 235kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 245kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 256kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 266kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 276kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 286kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 296kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 307kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 317kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 327kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 337kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 348kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 358kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 368kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 378kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 389kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 399kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 409kB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (50.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (2.23.0)\n",
            "Collecting numpy==1.16.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/2d/e4656149cbadd3a8a0369fcd1a9c7d61cc7b87b3903b85389c70c989a696/numpy-1.16.4-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3MB 200kB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (1.15.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (1.4.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (5.5.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (3.0.4)\n",
            "Collecting opencv-python~=3.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/6b/d8541f8890f166f050126c234e37c4535e74851f7625851abf58a6f37560/opencv_python-3.4.11.41-cp36-cp36m-manylinux2014_x86_64.whl (49.0MB)\n",
            "\u001b[K     |████████████████████████████████| 49.0MB 137kB/s \n",
            "\u001b[?25hRequirement already satisfied: patsy in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (0.5.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (0.2.0)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (0.10.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (0.22.2.post1)\n",
            "Requirement already satisfied: intel-openmp in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (2020.0.133)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (0.35.1)\n",
            "Collecting pyyaml>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 48.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (2020.6.20)\n",
            "Requirement already satisfied: imgaug in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (0.2.9)\n",
            "Requirement already satisfied: tables in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (3.4.4)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (0.16.2)\n",
            "Collecting tensorpack>=0.9.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/3d/b26490e53e40bb2891ff5d2e50adf51a1fc7eff894ed5646d6dc56df2067/tensorpack-0.10.1-py2.py3-none-any.whl (291kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 46.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (4.41.1)\n",
            "Collecting matplotlib==3.0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/69/f5e05f578585ed9935247be3788b374f90701296a70c8871bcd6d21edb00/matplotlib-3.0.3-cp36-cp36m-manylinux1_x86_64.whl (13.0MB)\n",
            "\u001b[K     |████████████████████████████████| 13.0MB 243kB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py~=2.7 in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (2.10.0)\n",
            "Collecting ruamel.yaml~=0.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/39/186f14f3836ac5d2a6a042c8de69988770e8b9abb537610edc429e4914aa/ruamel.yaml-0.16.12-py2.py3-none-any.whl (111kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 58.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.0. in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (1.0.5)\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (1.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (2.8.1)\n",
            "Requirement already satisfied: moviepy<=1.0.1 in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (0.2.3.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->deeplabcut) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->deeplabcut) (2.10)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->deeplabcut) (1.0.18)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->deeplabcut) (4.3.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->deeplabcut) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->deeplabcut) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->deeplabcut) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->deeplabcut) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->deeplabcut) (2.6.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->deeplabcut) (0.16.0)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.6/dist-packages (from imgaug->deeplabcut) (1.7.1)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (from imgaug->deeplabcut) (2.4.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from imgaug->deeplabcut) (7.0.0)\n",
            "Requirement already satisfied: numexpr>=2.5.2 in /usr/local/lib/python3.6/dist-packages (from tables->deeplabcut) (2.7.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->deeplabcut) (2.5)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->deeplabcut) (1.1.1)\n",
            "Requirement already satisfied: pyzmq>=16 in /usr/local/lib/python3.6/dist-packages (from tensorpack>=0.9.7.1->deeplabcut) (19.0.2)\n",
            "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.6/dist-packages (from tensorpack>=0.9.7.1->deeplabcut) (1.0.0)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.6/dist-packages (from tensorpack>=0.9.7.1->deeplabcut) (1.1.0)\n",
            "Collecting msgpack-numpy>=0.4.4.2\n",
            "  Downloading https://files.pythonhosted.org/packages/57/8c/901d65deb827c0d9f7680ea808a0d63eab71fbfac5c6a868b6c9e92be4cb/msgpack_numpy-0.4.7-py2.py3-none-any.whl\n",
            "Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.6/dist-packages (from tensorpack>=0.9.7.1->deeplabcut) (5.4.8)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from tensorpack>=0.9.7.1->deeplabcut) (0.8.7)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3->deeplabcut) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3->deeplabcut) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3->deeplabcut) (0.10.0)\n",
            "Collecting ruamel.yaml.clib>=0.1.2; platform_python_implementation == \"CPython\" and python_version < \"3.9\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/ff/ec25dc01ef04232a9e68ff18492e37dfa01f1f58172e702ad4f38536d41b/ruamel.yaml.clib-0.2.2-cp36-cp36m-manylinux1_x86_64.whl (549kB)\n",
            "\u001b[K     |████████████████████████████████| 552kB 42.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.0.->deeplabcut) (2018.9)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->deeplabcut) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->deeplabcut) (0.6.0)\n",
            "Building wheels for collected packages: pyyaml\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44619 sha256=9347b99c3e9dfef78f0eb1eac3e285ebe00237254fd940184d706e198b58ca03\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "Successfully built pyyaml\n",
            "\u001b[31mERROR: umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.0.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: mizani 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.0.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, opencv-python, pyyaml, msgpack-numpy, tensorpack, matplotlib, ruamel.yaml.clib, ruamel.yaml, deeplabcut\n",
            "  Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "  Found existing installation: opencv-python 4.1.2.30\n",
            "    Uninstalling opencv-python-4.1.2.30:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "25wSj6TlVclR"
      },
      "source": [
        "**(Be sure to click \"RESTART RUNTIME\" is it is displayed above above before moving on !)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y36K4Eux3h-X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c3bc7b0c-986b-45af-892e-3433dc5ef441"
      },
      "source": [
        "# Use TensorFlow 1.x:\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Frnj1RVDyEqs"
      },
      "source": [
        "YOU WILL NEED TO EDIT THE PROJECT PATH **in the config.yaml file** TO BE SET TO YOUR GOOGLE DRIVE LINK!\n",
        "\n",
        "Typically, this will be: /content/drive/My Drive/yourProjectFolderName\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vhENAlQnFENJ",
        "colab": {}
      },
      "source": [
        "#Setup your project variables:\n",
        "# PLEASE EDIT THESE:\n",
        "ProjectPath = '/content/drive/My Drive/DLC_analysis/'  \n",
        "ProjectFolderName = 'ephys-Berkowitz-2020-09-18'\n",
        "VideoType = 'avi' \n",
        "\n",
        "#don't edit these:\n",
        "videofile_path = [ProjectPath+'Videos/'] #Enter the list of videos or folder to analyze.\n",
        "videofile_path\n",
        "\n",
        "#dest list \n",
        "dest_path = videofile_path\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sXufoX6INe6w",
        "colab": {}
      },
      "source": [
        "#GUIs don't work on the cloud, so label your data locally on your computer! This will suppress the GUI support\n",
        "import os\n",
        "os.environ[\"DLClight\"]=\"True\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3K9Ndy1beyfG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "af684903-31f7-4e47-bb25-efdd68ee43ef"
      },
      "source": [
        "import deeplabcut"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o4orkg9QTHKK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "777d8156-bdd8-476c-c27a-667bfd2b761a"
      },
      "source": [
        "deeplabcut.__version__"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.1.8.2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z7ZlDr3wV4D1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c90ad9ab-ea7a-48a6-8058-3e524c2cae1e"
      },
      "source": [
        "#This creates a path variable that links to your google drive copy\n",
        "#No need to edit this, as you set it up before: \n",
        "path_config_file = ProjectPath+ProjectFolderName+'/config.yaml'\n",
        "path_config_file"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/DLC_analysis/ephys-Berkowitz-2020-09-18/config.yaml'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xNi9s1dboEJN"
      },
      "source": [
        "## Create a training dataset:\n",
        "### You must do this step inside of Colab:\n",
        "After running this script the training dataset is created and saved in the project directory under the subdirectory **'training-datasets'**\n",
        "\n",
        "This function also creates new subdirectories under **dlc-models** and appends the project config.yaml file with the correct path to the training and testing pose configuration file. These files hold the parameters for training the network. Such an example file is provided with the toolbox and named as **pose_cfg.yaml**.\n",
        "\n",
        "Now it is the time to start training the network!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfdzmLLri6Ra",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "39d478ce-0b1e-4571-ed20-b5d0b31888ee"
      },
      "source": [
        "# Note: if you are using the demo data (i.e. examples/Reaching-Mackenzie-2018-08-30/), first delete the folder called dlc-models! \n",
        "#Then, run this cell. There are many more functions you can set here, including which netowkr to use!\n",
        "#check the docstring for full options you can do!\n",
        "deeplabcut.create_training_dataset(path_config_file, augmenter_type='imgaug')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/DLC_analysis/ephys-Berkowitz-2020-09-18/training-datasets/iteration-0/UnaugmentedDataSet_ephysSep18  already exists!\n",
            "It appears that the images were labeled on a Windows system, but you are currently trying to create a training set on a Unix system. \n",
            " In this case the paths should be converted. Do you want to proceed with the conversion?\n",
            "yes/noyes\n",
            "Annotation data converted to unix format...\n",
            "Downloading a ImageNet-pretrained model from http://download.tensorflow.org/models/resnet_v1_101_2016_08_28.tar.gz....\n",
            "/content/drive/My Drive/DLC_analysis/ephys-Berkowitz-2020-09-18/dlc-models/iteration-0/ephysSep18-trainset95shuffle1  already exists!\n",
            "/content/drive/My Drive/DLC_analysis/ephys-Berkowitz-2020-09-18/dlc-models/iteration-0/ephysSep18-trainset95shuffle1/train  already exists!\n",
            "/content/drive/My Drive/DLC_analysis/ephys-Berkowitz-2020-09-18/dlc-models/iteration-0/ephysSep18-trainset95shuffle1/test  already exists!\n",
            "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.95,\n",
              "  1,\n",
              "  (array([ 19,  56,  29,  54,  65,   7,   8,  28,   3, 105,  67,  49,  57,\n",
              "            6,  63,  32,  75, 118,  47,  77, 110,   2,  70, 113,   0,  15,\n",
              "           93,  42,  46,  81,  69,  59, 111,  98,  40,  53,  23, 102,  92,\n",
              "           50,  94,  95,  24,  31,  74, 109,  76,  64,   5,  17,  37,  35,\n",
              "          100,  39,   9, 119,  30, 117,  80,  96,  52,  43,  25,  91,  33,\n",
              "           26,  20,  44, 116,  97,  36, 115, 112, 107,  73,   4,  86,  58,\n",
              "           79,  21,  27, 114,  18,  14,  10, 106,  55,  61,  38,  22,  72,\n",
              "           83,  51,  16,  41,  82,  34,  88,  78,  11,  48,  60, 103,  90,\n",
              "           12,  87, 101,  85,  66,  71, 104,  84,  13,  62]),\n",
              "   array([ 99,   1,  45, 108,  89,  68])))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c4FczXGDoEJU"
      },
      "source": [
        "## Start training:\n",
        "This function trains the network for a specific shuffle of the training dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_pOvDq_2oEJW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b13f47cc-e1c2-4321-f759-06906eae0f7a"
      },
      "source": [
        "\n",
        "#let's also change the display and save_iters just in case Colab takes away the GPU... \n",
        "#if that happens, you can reload from a saved point. Typically, you want to train to 200,000 + iterations.\n",
        "#more info and there are more things you can set: https://github.com/AlexEMG/DeepLabCut/blob/master/docs/functionDetails.md#g-train-the-network\n",
        "\n",
        "deeplabcut.train_network(path_config_file, shuffle=1, displayiters=10,saveiters=500)\n",
        "\n",
        "#this will run until you stop it (CTRL+C), or hit \"STOP\" icon, or when it hits the end (default, 1.03M iterations). \n",
        "#Whichever you chose, you will see what looks like an error message, but it's not an error - don't worry...."
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Config:\n",
            "{'all_joints': [[0], [1]],\n",
            " 'all_joints_names': ['red_led', 'green_led'],\n",
            " 'batch_size': 1,\n",
            " 'bottomheight': 400,\n",
            " 'crop': True,\n",
            " 'crop_pad': 0,\n",
            " 'cropratio': 0.4,\n",
            " 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_ephysSep18/ephys_Berkowitz95shuffle1.mat',\n",
            " 'dataset_type': 'imgaug',\n",
            " 'deconvolutionstride': 2,\n",
            " 'deterministic': False,\n",
            " 'display_iters': 1000,\n",
            " 'fg_fraction': 0.25,\n",
            " 'global_scale': 0.8,\n",
            " 'init_weights': '/usr/local/lib/python3.6/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_101.ckpt',\n",
            " 'intermediate_supervision': False,\n",
            " 'intermediate_supervision_layer': 12,\n",
            " 'leftwidth': 400,\n",
            " 'location_refinement': True,\n",
            " 'locref_huber_loss': True,\n",
            " 'locref_loss_weight': 0.05,\n",
            " 'locref_stdev': 7.2801,\n",
            " 'log_dir': 'log',\n",
            " 'max_input_size': 1500,\n",
            " 'mean_pixel': [123.68, 116.779, 103.939],\n",
            " 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_ephysSep18/Documentation_data-ephys_95shuffle1.pickle',\n",
            " 'min_input_size': 64,\n",
            " 'minsize': 100,\n",
            " 'mirror': False,\n",
            " 'multi_step': [[0.005, 10000],\n",
            "                [0.02, 430000],\n",
            "                [0.002, 730000],\n",
            "                [0.001, 1030000]],\n",
            " 'net_type': 'resnet_101',\n",
            " 'num_joints': 2,\n",
            " 'optimizer': 'sgd',\n",
            " 'output_stride': 16,\n",
            " 'pos_dist_thresh': 17,\n",
            " 'project_path': '/content/drive/My '\n",
            "                 'Drive/DLC_analysis/ephys-Berkowitz-2020-09-18',\n",
            " 'regularize': False,\n",
            " 'rightwidth': 400,\n",
            " 'save_iters': 50000,\n",
            " 'scale_jitter_lo': 0.5,\n",
            " 'scale_jitter_up': 1.25,\n",
            " 'scoremap_dir': 'test',\n",
            " 'shuffle': True,\n",
            " 'snapshot_prefix': '/content/drive/My '\n",
            "                    'Drive/DLC_analysis/ephys-Berkowitz-2020-09-18/dlc-models/iteration-0/ephysSep18-trainset95shuffle1/train/snapshot',\n",
            " 'stride': 8.0,\n",
            " 'topheight': 400,\n",
            " 'weigh_negatives': False,\n",
            " 'weigh_only_present_joints': False,\n",
            " 'weigh_part_predictions': False,\n",
            " 'weight_decay': 0.0001}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Starting with imgaug pose-dataset loader.\n",
            "Batch Size is 1\n",
            "Initializing ResNet\n",
            "Loading ImageNet-pretrained resnet_101\n",
            "INFO:tensorflow:Restoring parameters from /usr/local/lib/python3.6/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_101.ckpt\n",
            "Display_iters overwritten as 10\n",
            "Save_iters overwritten as 500\n",
            "Training parameter:\n",
            "{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'weigh_only_present_joints': False, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': '/content/drive/My Drive/DLC_analysis/ephys-Berkowitz-2020-09-18/dlc-models/iteration-0/ephysSep18-trainset95shuffle1/train/snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'sgd', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'mirror': False, 'crop_pad': 0, 'scoremap_dir': 'test', 'batch_size': 1, 'dataset_type': 'imgaug', 'deterministic': False, 'crop': True, 'cropratio': 0.4, 'minsize': 100, 'leftwidth': 400, 'rightwidth': 400, 'topheight': 400, 'bottomheight': 400, 'all_joints': [[0], [1]], 'all_joints_names': ['red_led', 'green_led'], 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_ephysSep18/ephys_Berkowitz95shuffle1.mat', 'display_iters': 1000, 'init_weights': '/usr/local/lib/python3.6/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_101.ckpt', 'max_input_size': 1500, 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_ephysSep18/Documentation_data-ephys_95shuffle1.pickle', 'min_input_size': 64, 'multi_step': [[0.005, 10000], [0.02, 430000], [0.002, 730000], [0.001, 1030000]], 'net_type': 'resnet_101', 'num_joints': 2, 'pos_dist_thresh': 17, 'project_path': '/content/drive/My Drive/DLC_analysis/ephys-Berkowitz-2020-09-18', 'save_iters': 50000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25, 'output_stride': 16, 'deconvolutionstride': 2}\n",
            "Starting training....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "iteration: 192550 loss: 0.0009 lr: 0.02\n",
            "iteration: 192560 loss: 0.0008 lr: 0.02\n",
            "iteration: 192570 loss: 0.0008 lr: 0.02\n",
            "iteration: 192580 loss: 0.0007 lr: 0.02\n",
            "iteration: 192590 loss: 0.0008 lr: 0.02\n",
            "iteration: 192600 loss: 0.0005 lr: 0.02\n",
            "iteration: 192610 loss: 0.0009 lr: 0.02\n",
            "iteration: 192620 loss: 0.0008 lr: 0.02\n",
            "iteration: 192630 loss: 0.0008 lr: 0.02\n",
            "iteration: 192640 loss: 0.0008 lr: 0.02\n",
            "iteration: 192650 loss: 0.0012 lr: 0.02\n",
            "iteration: 192660 loss: 0.0010 lr: 0.02\n",
            "iteration: 192670 loss: 0.0009 lr: 0.02\n",
            "iteration: 192680 loss: 0.0009 lr: 0.02\n",
            "iteration: 192690 loss: 0.0009 lr: 0.02\n",
            "iteration: 192700 loss: 0.0006 lr: 0.02\n",
            "iteration: 192710 loss: 0.0011 lr: 0.02\n",
            "iteration: 192720 loss: 0.0011 lr: 0.02\n",
            "iteration: 192730 loss: 0.0007 lr: 0.02\n",
            "iteration: 192740 loss: 0.0008 lr: 0.02\n",
            "iteration: 192750 loss: 0.0011 lr: 0.02\n",
            "iteration: 192760 loss: 0.0009 lr: 0.02\n",
            "iteration: 192770 loss: 0.0011 lr: 0.02\n",
            "iteration: 192780 loss: 0.0008 lr: 0.02\n",
            "iteration: 192790 loss: 0.0009 lr: 0.02\n",
            "iteration: 192800 loss: 0.0009 lr: 0.02\n",
            "iteration: 192810 loss: 0.0009 lr: 0.02\n",
            "iteration: 192820 loss: 0.0008 lr: 0.02\n",
            "iteration: 192830 loss: 0.0009 lr: 0.02\n",
            "iteration: 192840 loss: 0.0010 lr: 0.02\n",
            "iteration: 192850 loss: 0.0009 lr: 0.02\n",
            "iteration: 192860 loss: 0.0007 lr: 0.02\n",
            "iteration: 192870 loss: 0.0007 lr: 0.02\n",
            "iteration: 192880 loss: 0.0010 lr: 0.02\n",
            "iteration: 192890 loss: 0.0006 lr: 0.02\n",
            "iteration: 192900 loss: 0.0008 lr: 0.02\n",
            "iteration: 192910 loss: 0.0009 lr: 0.02\n",
            "iteration: 192920 loss: 0.0007 lr: 0.02\n",
            "iteration: 192930 loss: 0.0007 lr: 0.02\n",
            "iteration: 192940 loss: 0.0011 lr: 0.02\n",
            "iteration: 192950 loss: 0.0011 lr: 0.02\n",
            "iteration: 192960 loss: 0.0012 lr: 0.02\n",
            "iteration: 192970 loss: 0.0009 lr: 0.02\n",
            "iteration: 192980 loss: 0.0006 lr: 0.02\n",
            "iteration: 192990 loss: 0.0007 lr: 0.02\n",
            "iteration: 193000 loss: 0.0008 lr: 0.02\n",
            "iteration: 193010 loss: 0.0010 lr: 0.02\n",
            "iteration: 193020 loss: 0.0007 lr: 0.02\n",
            "iteration: 193030 loss: 0.0008 lr: 0.02\n",
            "iteration: 193040 loss: 0.0008 lr: 0.02\n",
            "iteration: 193050 loss: 0.0006 lr: 0.02\n",
            "iteration: 193060 loss: 0.0008 lr: 0.02\n",
            "iteration: 193070 loss: 0.0011 lr: 0.02\n",
            "iteration: 193080 loss: 0.0012 lr: 0.02\n",
            "iteration: 193090 loss: 0.0006 lr: 0.02\n",
            "iteration: 193100 loss: 0.0008 lr: 0.02\n",
            "iteration: 193110 loss: 0.0008 lr: 0.02\n",
            "iteration: 193120 loss: 0.0007 lr: 0.02\n",
            "iteration: 193130 loss: 0.0008 lr: 0.02\n",
            "iteration: 193140 loss: 0.0008 lr: 0.02\n",
            "iteration: 193150 loss: 0.0009 lr: 0.02\n",
            "iteration: 193160 loss: 0.0008 lr: 0.02\n",
            "iteration: 193170 loss: 0.0008 lr: 0.02\n",
            "iteration: 193180 loss: 0.0006 lr: 0.02\n",
            "iteration: 193190 loss: 0.0007 lr: 0.02\n",
            "iteration: 193200 loss: 0.0009 lr: 0.02\n",
            "iteration: 193210 loss: 0.0008 lr: 0.02\n",
            "iteration: 193220 loss: 0.0009 lr: 0.02\n",
            "iteration: 193230 loss: 0.0008 lr: 0.02\n",
            "iteration: 193240 loss: 0.0009 lr: 0.02\n",
            "iteration: 193250 loss: 0.0007 lr: 0.02\n",
            "iteration: 193260 loss: 0.0006 lr: 0.02\n",
            "iteration: 193270 loss: 0.0009 lr: 0.02\n",
            "iteration: 193280 loss: 0.0006 lr: 0.02\n",
            "iteration: 193290 loss: 0.0006 lr: 0.02\n",
            "iteration: 193300 loss: 0.0007 lr: 0.02\n",
            "iteration: 193310 loss: 0.0007 lr: 0.02\n",
            "iteration: 193320 loss: 0.0009 lr: 0.02\n",
            "iteration: 193330 loss: 0.0007 lr: 0.02\n",
            "iteration: 193340 loss: 0.0008 lr: 0.02\n",
            "iteration: 193350 loss: 0.0007 lr: 0.02\n",
            "iteration: 193360 loss: 0.0009 lr: 0.02\n",
            "iteration: 193370 loss: 0.0007 lr: 0.02\n",
            "iteration: 193380 loss: 0.0008 lr: 0.02\n",
            "iteration: 193390 loss: 0.0006 lr: 0.02\n",
            "iteration: 193400 loss: 0.0006 lr: 0.02\n",
            "iteration: 193410 loss: 0.0009 lr: 0.02\n",
            "iteration: 193420 loss: 0.0009 lr: 0.02\n",
            "iteration: 193430 loss: 0.0011 lr: 0.02\n",
            "iteration: 193440 loss: 0.0007 lr: 0.02\n",
            "iteration: 193450 loss: 0.0010 lr: 0.02\n",
            "iteration: 193460 loss: 0.0006 lr: 0.02\n",
            "iteration: 193470 loss: 0.0010 lr: 0.02\n",
            "iteration: 193480 loss: 0.0007 lr: 0.02\n",
            "iteration: 193490 loss: 0.0008 lr: 0.02\n",
            "iteration: 193500 loss: 0.0006 lr: 0.02\n",
            "iteration: 193510 loss: 0.0007 lr: 0.02\n",
            "iteration: 193520 loss: 0.0006 lr: 0.02\n",
            "iteration: 193530 loss: 0.0008 lr: 0.02\n",
            "iteration: 193540 loss: 0.0007 lr: 0.02\n",
            "iteration: 193550 loss: 0.0008 lr: 0.02\n",
            "iteration: 193560 loss: 0.0008 lr: 0.02\n",
            "iteration: 193570 loss: 0.0007 lr: 0.02\n",
            "iteration: 193580 loss: 0.0007 lr: 0.02\n",
            "iteration: 193590 loss: 0.0008 lr: 0.02\n",
            "iteration: 193600 loss: 0.0007 lr: 0.02\n",
            "iteration: 193610 loss: 0.0007 lr: 0.02\n",
            "iteration: 193620 loss: 0.0005 lr: 0.02\n",
            "iteration: 193630 loss: 0.0008 lr: 0.02\n",
            "iteration: 193640 loss: 0.0007 lr: 0.02\n",
            "iteration: 193650 loss: 0.0008 lr: 0.02\n",
            "iteration: 193660 loss: 0.0009 lr: 0.02\n",
            "iteration: 193670 loss: 0.0008 lr: 0.02\n",
            "iteration: 193680 loss: 0.0013 lr: 0.02\n",
            "iteration: 193690 loss: 0.0011 lr: 0.02\n",
            "iteration: 193700 loss: 0.0007 lr: 0.02\n",
            "iteration: 193710 loss: 0.0007 lr: 0.02\n",
            "iteration: 193720 loss: 0.0008 lr: 0.02\n",
            "iteration: 193730 loss: 0.0008 lr: 0.02\n",
            "iteration: 193740 loss: 0.0011 lr: 0.02\n",
            "iteration: 193750 loss: 0.0010 lr: 0.02\n",
            "iteration: 193760 loss: 0.0008 lr: 0.02\n",
            "iteration: 193770 loss: 0.0008 lr: 0.02\n",
            "iteration: 193780 loss: 0.0006 lr: 0.02\n",
            "iteration: 193790 loss: 0.0007 lr: 0.02\n",
            "iteration: 193800 loss: 0.0009 lr: 0.02\n",
            "iteration: 193810 loss: 0.0010 lr: 0.02\n",
            "iteration: 193820 loss: 0.0011 lr: 0.02\n",
            "iteration: 193830 loss: 0.0009 lr: 0.02\n",
            "iteration: 193840 loss: 0.0009 lr: 0.02\n",
            "iteration: 193850 loss: 0.0009 lr: 0.02\n",
            "iteration: 193860 loss: 0.0008 lr: 0.02\n",
            "iteration: 193870 loss: 0.0007 lr: 0.02\n",
            "iteration: 193880 loss: 0.0008 lr: 0.02\n",
            "iteration: 193890 loss: 0.0007 lr: 0.02\n",
            "iteration: 193900 loss: 0.0008 lr: 0.02\n",
            "iteration: 193910 loss: 0.0008 lr: 0.02\n",
            "iteration: 193920 loss: 0.0008 lr: 0.02\n",
            "iteration: 193930 loss: 0.0008 lr: 0.02\n",
            "iteration: 193940 loss: 0.0008 lr: 0.02\n",
            "iteration: 193950 loss: 0.0016 lr: 0.02\n",
            "iteration: 193960 loss: 0.0007 lr: 0.02\n",
            "iteration: 193970 loss: 0.0009 lr: 0.02\n",
            "iteration: 193980 loss: 0.0008 lr: 0.02\n",
            "iteration: 193990 loss: 0.0008 lr: 0.02\n",
            "iteration: 194000 loss: 0.0004 lr: 0.02\n",
            "iteration: 194010 loss: 0.0007 lr: 0.02\n",
            "iteration: 194020 loss: 0.0006 lr: 0.02\n",
            "iteration: 194030 loss: 0.0006 lr: 0.02\n",
            "iteration: 194040 loss: 0.0007 lr: 0.02\n",
            "iteration: 194050 loss: 0.0008 lr: 0.02\n",
            "iteration: 194060 loss: 0.0008 lr: 0.02\n",
            "iteration: 194070 loss: 0.0007 lr: 0.02\n",
            "iteration: 194080 loss: 0.0006 lr: 0.02\n",
            "iteration: 194090 loss: 0.0010 lr: 0.02\n",
            "iteration: 194100 loss: 0.0008 lr: 0.02\n",
            "iteration: 194110 loss: 0.0009 lr: 0.02\n",
            "iteration: 194120 loss: 0.0008 lr: 0.02\n",
            "iteration: 194130 loss: 0.0010 lr: 0.02\n",
            "iteration: 194140 loss: 0.0010 lr: 0.02\n",
            "iteration: 194150 loss: 0.0009 lr: 0.02\n",
            "iteration: 194160 loss: 0.0007 lr: 0.02\n",
            "iteration: 194170 loss: 0.0007 lr: 0.02\n",
            "iteration: 194180 loss: 0.0009 lr: 0.02\n",
            "iteration: 194190 loss: 0.0006 lr: 0.02\n",
            "iteration: 194200 loss: 0.0008 lr: 0.02\n",
            "iteration: 194210 loss: 0.0006 lr: 0.02\n",
            "iteration: 194220 loss: 0.0005 lr: 0.02\n",
            "iteration: 194230 loss: 0.0009 lr: 0.02\n",
            "iteration: 194240 loss: 0.0009 lr: 0.02\n",
            "iteration: 194250 loss: 0.0011 lr: 0.02\n",
            "iteration: 194260 loss: 0.0012 lr: 0.02\n",
            "iteration: 194270 loss: 0.0007 lr: 0.02\n",
            "iteration: 194280 loss: 0.0005 lr: 0.02\n",
            "iteration: 194290 loss: 0.0007 lr: 0.02\n",
            "iteration: 194300 loss: 0.0009 lr: 0.02\n",
            "iteration: 194310 loss: 0.0008 lr: 0.02\n",
            "iteration: 194320 loss: 0.0011 lr: 0.02\n",
            "iteration: 194330 loss: 0.0007 lr: 0.02\n",
            "iteration: 194340 loss: 0.0014 lr: 0.02\n",
            "iteration: 194350 loss: 0.0011 lr: 0.02\n",
            "iteration: 194360 loss: 0.0011 lr: 0.02\n",
            "iteration: 194370 loss: 0.0009 lr: 0.02\n",
            "iteration: 194380 loss: 0.0009 lr: 0.02\n",
            "iteration: 194390 loss: 0.0008 lr: 0.02\n",
            "iteration: 194400 loss: 0.0007 lr: 0.02\n",
            "iteration: 194410 loss: 0.0008 lr: 0.02\n",
            "iteration: 194420 loss: 0.0006 lr: 0.02\n",
            "iteration: 194430 loss: 0.0008 lr: 0.02\n",
            "iteration: 194440 loss: 0.0006 lr: 0.02\n",
            "iteration: 194450 loss: 0.0006 lr: 0.02\n",
            "iteration: 194460 loss: 0.0010 lr: 0.02\n",
            "iteration: 194470 loss: 0.0014 lr: 0.02\n",
            "iteration: 194480 loss: 0.0008 lr: 0.02\n",
            "iteration: 194490 loss: 0.0011 lr: 0.02\n",
            "iteration: 194500 loss: 0.0010 lr: 0.02\n",
            "iteration: 194510 loss: 0.0007 lr: 0.02\n",
            "iteration: 194520 loss: 0.0007 lr: 0.02\n",
            "iteration: 194530 loss: 0.0009 lr: 0.02\n",
            "iteration: 194540 loss: 0.0006 lr: 0.02\n",
            "iteration: 194550 loss: 0.0009 lr: 0.02\n",
            "iteration: 194560 loss: 0.0009 lr: 0.02\n",
            "iteration: 194570 loss: 0.0010 lr: 0.02\n",
            "iteration: 194580 loss: 0.0010 lr: 0.02\n",
            "iteration: 194590 loss: 0.0008 lr: 0.02\n",
            "iteration: 194600 loss: 0.0013 lr: 0.02\n",
            "iteration: 194610 loss: 0.0008 lr: 0.02\n",
            "iteration: 194620 loss: 0.0013 lr: 0.02\n",
            "iteration: 194630 loss: 0.0010 lr: 0.02\n",
            "iteration: 194640 loss: 0.0009 lr: 0.02\n",
            "iteration: 194650 loss: 0.0010 lr: 0.02\n",
            "iteration: 194660 loss: 0.0017 lr: 0.02\n",
            "iteration: 194670 loss: 0.0012 lr: 0.02\n",
            "iteration: 194680 loss: 0.0017 lr: 0.02\n",
            "iteration: 194690 loss: 0.0010 lr: 0.02\n",
            "iteration: 194700 loss: 0.0009 lr: 0.02\n",
            "iteration: 194710 loss: 0.0014 lr: 0.02\n",
            "iteration: 194720 loss: 0.0010 lr: 0.02\n",
            "iteration: 194730 loss: 0.0008 lr: 0.02\n",
            "iteration: 194740 loss: 0.0009 lr: 0.02\n",
            "iteration: 194750 loss: 0.0009 lr: 0.02\n",
            "iteration: 194760 loss: 0.0008 lr: 0.02\n",
            "iteration: 194770 loss: 0.0010 lr: 0.02\n",
            "iteration: 194780 loss: 0.0005 lr: 0.02\n",
            "iteration: 194790 loss: 0.0010 lr: 0.02\n",
            "iteration: 194800 loss: 0.0005 lr: 0.02\n",
            "iteration: 194810 loss: 0.0009 lr: 0.02\n",
            "iteration: 194820 loss: 0.0007 lr: 0.02\n",
            "iteration: 194830 loss: 0.0010 lr: 0.02\n",
            "iteration: 194840 loss: 0.0007 lr: 0.02\n",
            "iteration: 194850 loss: 0.0008 lr: 0.02\n",
            "iteration: 194860 loss: 0.0008 lr: 0.02\n",
            "iteration: 194870 loss: 0.0007 lr: 0.02\n",
            "iteration: 194880 loss: 0.0009 lr: 0.02\n",
            "iteration: 194890 loss: 0.0007 lr: 0.02\n",
            "iteration: 194900 loss: 0.0008 lr: 0.02\n",
            "iteration: 194910 loss: 0.0005 lr: 0.02\n",
            "iteration: 194920 loss: 0.0007 lr: 0.02\n",
            "iteration: 194930 loss: 0.0013 lr: 0.02\n",
            "iteration: 194940 loss: 0.0009 lr: 0.02\n",
            "iteration: 194950 loss: 0.0009 lr: 0.02\n",
            "iteration: 194960 loss: 0.0007 lr: 0.02\n",
            "iteration: 194970 loss: 0.0006 lr: 0.02\n",
            "iteration: 194980 loss: 0.0008 lr: 0.02\n",
            "iteration: 194990 loss: 0.0009 lr: 0.02\n",
            "iteration: 195000 loss: 0.0007 lr: 0.02\n",
            "iteration: 195010 loss: 0.0007 lr: 0.02\n",
            "iteration: 195020 loss: 0.0009 lr: 0.02\n",
            "iteration: 195030 loss: 0.0008 lr: 0.02\n",
            "iteration: 195040 loss: 0.0008 lr: 0.02\n",
            "iteration: 195050 loss: 0.0008 lr: 0.02\n",
            "iteration: 195060 loss: 0.0008 lr: 0.02\n",
            "iteration: 195070 loss: 0.0009 lr: 0.02\n",
            "iteration: 195080 loss: 0.0008 lr: 0.02\n",
            "iteration: 195090 loss: 0.0009 lr: 0.02\n",
            "iteration: 195100 loss: 0.0008 lr: 0.02\n",
            "iteration: 195110 loss: 0.0009 lr: 0.02\n",
            "iteration: 195120 loss: 0.0008 lr: 0.02\n",
            "iteration: 195130 loss: 0.0008 lr: 0.02\n",
            "iteration: 195140 loss: 0.0011 lr: 0.02\n",
            "iteration: 195150 loss: 0.0008 lr: 0.02\n",
            "iteration: 195160 loss: 0.0007 lr: 0.02\n",
            "iteration: 195170 loss: 0.0011 lr: 0.02\n",
            "iteration: 195180 loss: 0.0010 lr: 0.02\n",
            "iteration: 195190 loss: 0.0008 lr: 0.02\n",
            "iteration: 195200 loss: 0.0007 lr: 0.02\n",
            "iteration: 195210 loss: 0.0008 lr: 0.02\n",
            "iteration: 195220 loss: 0.0008 lr: 0.02\n",
            "iteration: 195230 loss: 0.0011 lr: 0.02\n",
            "iteration: 195240 loss: 0.0007 lr: 0.02\n",
            "iteration: 195250 loss: 0.0007 lr: 0.02\n",
            "iteration: 195260 loss: 0.0009 lr: 0.02\n",
            "iteration: 195270 loss: 0.0010 lr: 0.02\n",
            "iteration: 195280 loss: 0.0010 lr: 0.02\n",
            "iteration: 195290 loss: 0.0008 lr: 0.02\n",
            "iteration: 195300 loss: 0.0006 lr: 0.02\n",
            "iteration: 195310 loss: 0.0008 lr: 0.02\n",
            "iteration: 195320 loss: 0.0008 lr: 0.02\n",
            "iteration: 195330 loss: 0.0009 lr: 0.02\n",
            "iteration: 195340 loss: 0.0007 lr: 0.02\n",
            "iteration: 195350 loss: 0.0006 lr: 0.02\n",
            "iteration: 195360 loss: 0.0010 lr: 0.02\n",
            "iteration: 195370 loss: 0.0013 lr: 0.02\n",
            "iteration: 195380 loss: 0.0012 lr: 0.02\n",
            "iteration: 195390 loss: 0.0007 lr: 0.02\n",
            "iteration: 195400 loss: 0.0007 lr: 0.02\n",
            "iteration: 195410 loss: 0.0010 lr: 0.02\n",
            "iteration: 195420 loss: 0.0009 lr: 0.02\n",
            "iteration: 195430 loss: 0.0007 lr: 0.02\n",
            "iteration: 195440 loss: 0.0008 lr: 0.02\n",
            "iteration: 195450 loss: 0.0006 lr: 0.02\n",
            "iteration: 195460 loss: 0.0009 lr: 0.02\n",
            "iteration: 195470 loss: 0.0008 lr: 0.02\n",
            "iteration: 195480 loss: 0.0007 lr: 0.02\n",
            "iteration: 195490 loss: 0.0010 lr: 0.02\n",
            "iteration: 195500 loss: 0.0008 lr: 0.02\n",
            "iteration: 195510 loss: 0.0008 lr: 0.02\n",
            "iteration: 195520 loss: 0.0010 lr: 0.02\n",
            "iteration: 195530 loss: 0.0008 lr: 0.02\n",
            "iteration: 195540 loss: 0.0010 lr: 0.02\n",
            "iteration: 195550 loss: 0.0012 lr: 0.02\n",
            "iteration: 195560 loss: 0.0012 lr: 0.02\n",
            "iteration: 195570 loss: 0.0011 lr: 0.02\n",
            "iteration: 195580 loss: 0.0009 lr: 0.02\n",
            "iteration: 195590 loss: 0.0010 lr: 0.02\n",
            "iteration: 195600 loss: 0.0008 lr: 0.02\n",
            "iteration: 195610 loss: 0.0007 lr: 0.02\n",
            "iteration: 195620 loss: 0.0009 lr: 0.02\n",
            "iteration: 195630 loss: 0.0006 lr: 0.02\n",
            "iteration: 195640 loss: 0.0008 lr: 0.02\n",
            "iteration: 195650 loss: 0.0009 lr: 0.02\n",
            "iteration: 195660 loss: 0.0010 lr: 0.02\n",
            "iteration: 195670 loss: 0.0016 lr: 0.02\n",
            "iteration: 195680 loss: 0.0008 lr: 0.02\n",
            "iteration: 195690 loss: 0.0013 lr: 0.02\n",
            "iteration: 195700 loss: 0.0007 lr: 0.02\n",
            "iteration: 195710 loss: 0.0006 lr: 0.02\n",
            "iteration: 195720 loss: 0.0007 lr: 0.02\n",
            "iteration: 195730 loss: 0.0010 lr: 0.02\n",
            "iteration: 195740 loss: 0.0008 lr: 0.02\n",
            "iteration: 195750 loss: 0.0010 lr: 0.02\n",
            "iteration: 195760 loss: 0.0008 lr: 0.02\n",
            "iteration: 195770 loss: 0.0007 lr: 0.02\n",
            "iteration: 195780 loss: 0.0007 lr: 0.02\n",
            "iteration: 195790 loss: 0.0010 lr: 0.02\n",
            "iteration: 195800 loss: 0.0009 lr: 0.02\n",
            "iteration: 195810 loss: 0.0012 lr: 0.02\n",
            "iteration: 195820 loss: 0.0010 lr: 0.02\n",
            "iteration: 195830 loss: 0.0009 lr: 0.02\n",
            "iteration: 195840 loss: 0.0008 lr: 0.02\n",
            "iteration: 195850 loss: 0.0009 lr: 0.02\n",
            "iteration: 195860 loss: 0.0009 lr: 0.02\n",
            "iteration: 195870 loss: 0.0007 lr: 0.02\n",
            "iteration: 195880 loss: 0.0007 lr: 0.02\n",
            "iteration: 195890 loss: 0.0007 lr: 0.02\n",
            "iteration: 195900 loss: 0.0006 lr: 0.02\n",
            "iteration: 195910 loss: 0.0007 lr: 0.02\n",
            "iteration: 195920 loss: 0.0007 lr: 0.02\n",
            "iteration: 195930 loss: 0.0010 lr: 0.02\n",
            "iteration: 195940 loss: 0.0007 lr: 0.02\n",
            "iteration: 195950 loss: 0.0005 lr: 0.02\n",
            "iteration: 195960 loss: 0.0009 lr: 0.02\n",
            "iteration: 195970 loss: 0.0007 lr: 0.02\n",
            "iteration: 195980 loss: 0.0009 lr: 0.02\n",
            "iteration: 195990 loss: 0.0007 lr: 0.02\n",
            "iteration: 196000 loss: 0.0012 lr: 0.02\n",
            "iteration: 196010 loss: 0.0006 lr: 0.02\n",
            "iteration: 196020 loss: 0.0009 lr: 0.02\n",
            "iteration: 196030 loss: 0.0008 lr: 0.02\n",
            "iteration: 196040 loss: 0.0006 lr: 0.02\n",
            "iteration: 196050 loss: 0.0006 lr: 0.02\n",
            "iteration: 196060 loss: 0.0009 lr: 0.02\n",
            "iteration: 196070 loss: 0.0012 lr: 0.02\n",
            "iteration: 196080 loss: 0.0008 lr: 0.02\n",
            "iteration: 196090 loss: 0.0007 lr: 0.02\n",
            "iteration: 196100 loss: 0.0008 lr: 0.02\n",
            "iteration: 196110 loss: 0.0009 lr: 0.02\n",
            "iteration: 196120 loss: 0.0009 lr: 0.02\n",
            "iteration: 196130 loss: 0.0012 lr: 0.02\n",
            "iteration: 196140 loss: 0.0011 lr: 0.02\n",
            "iteration: 196150 loss: 0.0014 lr: 0.02\n",
            "iteration: 196160 loss: 0.0008 lr: 0.02\n",
            "iteration: 196170 loss: 0.0011 lr: 0.02\n",
            "iteration: 196180 loss: 0.0013 lr: 0.02\n",
            "iteration: 196190 loss: 0.0013 lr: 0.02\n",
            "iteration: 196200 loss: 0.0005 lr: 0.02\n",
            "iteration: 196210 loss: 0.0007 lr: 0.02\n",
            "iteration: 196220 loss: 0.0010 lr: 0.02\n",
            "iteration: 196230 loss: 0.0008 lr: 0.02\n",
            "iteration: 196240 loss: 0.0008 lr: 0.02\n",
            "iteration: 196250 loss: 0.0007 lr: 0.02\n",
            "iteration: 196260 loss: 0.0009 lr: 0.02\n",
            "iteration: 196270 loss: 0.0011 lr: 0.02\n",
            "iteration: 196280 loss: 0.0006 lr: 0.02\n",
            "iteration: 196290 loss: 0.0007 lr: 0.02\n",
            "iteration: 196300 loss: 0.0008 lr: 0.02\n",
            "iteration: 196310 loss: 0.0009 lr: 0.02\n",
            "iteration: 196320 loss: 0.0009 lr: 0.02\n",
            "iteration: 196330 loss: 0.0009 lr: 0.02\n",
            "iteration: 196340 loss: 0.0007 lr: 0.02\n",
            "iteration: 196350 loss: 0.0007 lr: 0.02\n",
            "iteration: 196360 loss: 0.0008 lr: 0.02\n",
            "iteration: 196370 loss: 0.0010 lr: 0.02\n",
            "iteration: 196380 loss: 0.0008 lr: 0.02\n",
            "iteration: 196390 loss: 0.0006 lr: 0.02\n",
            "iteration: 196400 loss: 0.0007 lr: 0.02\n",
            "iteration: 196410 loss: 0.0007 lr: 0.02\n",
            "iteration: 196420 loss: 0.0008 lr: 0.02\n",
            "iteration: 196430 loss: 0.0013 lr: 0.02\n",
            "iteration: 196440 loss: 0.0011 lr: 0.02\n",
            "iteration: 196450 loss: 0.0007 lr: 0.02\n",
            "iteration: 196460 loss: 0.0010 lr: 0.02\n",
            "iteration: 196470 loss: 0.0009 lr: 0.02\n",
            "iteration: 196480 loss: 0.0006 lr: 0.02\n",
            "iteration: 196490 loss: 0.0010 lr: 0.02\n",
            "iteration: 196500 loss: 0.0010 lr: 0.02\n",
            "iteration: 196510 loss: 0.0010 lr: 0.02\n",
            "iteration: 196520 loss: 0.0009 lr: 0.02\n",
            "iteration: 196530 loss: 0.0008 lr: 0.02\n",
            "iteration: 196540 loss: 0.0009 lr: 0.02\n",
            "iteration: 196550 loss: 0.0006 lr: 0.02\n",
            "iteration: 196560 loss: 0.0009 lr: 0.02\n",
            "iteration: 196570 loss: 0.0007 lr: 0.02\n",
            "iteration: 196580 loss: 0.0006 lr: 0.02\n",
            "iteration: 196590 loss: 0.0009 lr: 0.02\n",
            "iteration: 196600 loss: 0.0010 lr: 0.02\n",
            "iteration: 196610 loss: 0.0010 lr: 0.02\n",
            "iteration: 196620 loss: 0.0010 lr: 0.02\n",
            "iteration: 196630 loss: 0.0011 lr: 0.02\n",
            "iteration: 196640 loss: 0.0009 lr: 0.02\n",
            "iteration: 196650 loss: 0.0008 lr: 0.02\n",
            "iteration: 196660 loss: 0.0009 lr: 0.02\n",
            "iteration: 196670 loss: 0.0008 lr: 0.02\n",
            "iteration: 196680 loss: 0.0007 lr: 0.02\n",
            "iteration: 196690 loss: 0.0007 lr: 0.02\n",
            "iteration: 196700 loss: 0.0008 lr: 0.02\n",
            "iteration: 196710 loss: 0.0007 lr: 0.02\n",
            "iteration: 196720 loss: 0.0006 lr: 0.02\n",
            "iteration: 196730 loss: 0.0008 lr: 0.02\n",
            "iteration: 196740 loss: 0.0009 lr: 0.02\n",
            "iteration: 196750 loss: 0.0008 lr: 0.02\n",
            "iteration: 196760 loss: 0.0006 lr: 0.02\n",
            "iteration: 196770 loss: 0.0008 lr: 0.02\n",
            "iteration: 196780 loss: 0.0007 lr: 0.02\n",
            "iteration: 196790 loss: 0.0009 lr: 0.02\n",
            "iteration: 196800 loss: 0.0009 lr: 0.02\n",
            "iteration: 196810 loss: 0.0007 lr: 0.02\n",
            "iteration: 196820 loss: 0.0010 lr: 0.02\n",
            "iteration: 196830 loss: 0.0009 lr: 0.02\n",
            "iteration: 196840 loss: 0.0008 lr: 0.02\n",
            "iteration: 196850 loss: 0.0006 lr: 0.02\n",
            "iteration: 196860 loss: 0.0007 lr: 0.02\n",
            "iteration: 196870 loss: 0.0007 lr: 0.02\n",
            "iteration: 196880 loss: 0.0008 lr: 0.02\n",
            "iteration: 196890 loss: 0.0009 lr: 0.02\n",
            "iteration: 196900 loss: 0.0006 lr: 0.02\n",
            "iteration: 196910 loss: 0.0007 lr: 0.02\n",
            "iteration: 196920 loss: 0.0007 lr: 0.02\n",
            "iteration: 196930 loss: 0.0007 lr: 0.02\n",
            "iteration: 196940 loss: 0.0009 lr: 0.02\n",
            "iteration: 196950 loss: 0.0007 lr: 0.02\n",
            "iteration: 196960 loss: 0.0006 lr: 0.02\n",
            "iteration: 196970 loss: 0.0008 lr: 0.02\n",
            "iteration: 196980 loss: 0.0007 lr: 0.02\n",
            "iteration: 196990 loss: 0.0007 lr: 0.02\n",
            "iteration: 197000 loss: 0.0006 lr: 0.02\n",
            "iteration: 197010 loss: 0.0008 lr: 0.02\n",
            "iteration: 197020 loss: 0.0010 lr: 0.02\n",
            "iteration: 197030 loss: 0.0007 lr: 0.02\n",
            "iteration: 197040 loss: 0.0008 lr: 0.02\n",
            "iteration: 197050 loss: 0.0006 lr: 0.02\n",
            "iteration: 197060 loss: 0.0008 lr: 0.02\n",
            "iteration: 197070 loss: 0.0005 lr: 0.02\n",
            "iteration: 197080 loss: 0.0008 lr: 0.02\n",
            "iteration: 197090 loss: 0.0013 lr: 0.02\n",
            "iteration: 197100 loss: 0.0009 lr: 0.02\n",
            "iteration: 197110 loss: 0.0014 lr: 0.02\n",
            "iteration: 197120 loss: 0.0010 lr: 0.02\n",
            "iteration: 197130 loss: 0.0009 lr: 0.02\n",
            "iteration: 197140 loss: 0.0012 lr: 0.02\n",
            "iteration: 197150 loss: 0.0010 lr: 0.02\n",
            "iteration: 197160 loss: 0.0008 lr: 0.02\n",
            "iteration: 197170 loss: 0.0013 lr: 0.02\n",
            "iteration: 197180 loss: 0.0007 lr: 0.02\n",
            "iteration: 197190 loss: 0.0007 lr: 0.02\n",
            "iteration: 197200 loss: 0.0008 lr: 0.02\n",
            "iteration: 197210 loss: 0.0010 lr: 0.02\n",
            "iteration: 197220 loss: 0.0005 lr: 0.02\n",
            "iteration: 197230 loss: 0.0009 lr: 0.02\n",
            "iteration: 197240 loss: 0.0005 lr: 0.02\n",
            "iteration: 197250 loss: 0.0006 lr: 0.02\n",
            "iteration: 197260 loss: 0.0008 lr: 0.02\n",
            "iteration: 197270 loss: 0.0007 lr: 0.02\n",
            "iteration: 197280 loss: 0.0008 lr: 0.02\n",
            "iteration: 197290 loss: 0.0004 lr: 0.02\n",
            "iteration: 197300 loss: 0.0008 lr: 0.02\n",
            "iteration: 197310 loss: 0.0009 lr: 0.02\n",
            "iteration: 197320 loss: 0.0007 lr: 0.02\n",
            "iteration: 197330 loss: 0.0009 lr: 0.02\n",
            "iteration: 197340 loss: 0.0009 lr: 0.02\n",
            "iteration: 197350 loss: 0.0008 lr: 0.02\n",
            "iteration: 197360 loss: 0.0006 lr: 0.02\n",
            "iteration: 197370 loss: 0.0008 lr: 0.02\n",
            "iteration: 197380 loss: 0.0006 lr: 0.02\n",
            "iteration: 197390 loss: 0.0008 lr: 0.02\n",
            "iteration: 197400 loss: 0.0009 lr: 0.02\n",
            "iteration: 197410 loss: 0.0011 lr: 0.02\n",
            "iteration: 197420 loss: 0.0007 lr: 0.02\n",
            "iteration: 197430 loss: 0.0006 lr: 0.02\n",
            "iteration: 197440 loss: 0.0009 lr: 0.02\n",
            "iteration: 197450 loss: 0.0007 lr: 0.02\n",
            "iteration: 197460 loss: 0.0011 lr: 0.02\n",
            "iteration: 197470 loss: 0.0008 lr: 0.02\n",
            "iteration: 197480 loss: 0.0005 lr: 0.02\n",
            "iteration: 197490 loss: 0.0008 lr: 0.02\n",
            "iteration: 197500 loss: 0.0008 lr: 0.02\n",
            "iteration: 197510 loss: 0.0010 lr: 0.02\n",
            "iteration: 197520 loss: 0.0006 lr: 0.02\n",
            "iteration: 197530 loss: 0.0007 lr: 0.02\n",
            "iteration: 197540 loss: 0.0008 lr: 0.02\n",
            "iteration: 197550 loss: 0.0008 lr: 0.02\n",
            "iteration: 197560 loss: 0.0009 lr: 0.02\n",
            "iteration: 197570 loss: 0.0007 lr: 0.02\n",
            "iteration: 197580 loss: 0.0010 lr: 0.02\n",
            "iteration: 197590 loss: 0.0009 lr: 0.02\n",
            "iteration: 197600 loss: 0.0008 lr: 0.02\n",
            "iteration: 197610 loss: 0.0008 lr: 0.02\n",
            "iteration: 197620 loss: 0.0007 lr: 0.02\n",
            "iteration: 197630 loss: 0.0011 lr: 0.02\n",
            "iteration: 197640 loss: 0.0007 lr: 0.02\n",
            "iteration: 197650 loss: 0.0008 lr: 0.02\n",
            "iteration: 197660 loss: 0.0008 lr: 0.02\n",
            "iteration: 197670 loss: 0.0013 lr: 0.02\n",
            "iteration: 197680 loss: 0.0008 lr: 0.02\n",
            "iteration: 197690 loss: 0.0007 lr: 0.02\n",
            "iteration: 197700 loss: 0.0008 lr: 0.02\n",
            "iteration: 197710 loss: 0.0006 lr: 0.02\n",
            "iteration: 197720 loss: 0.0007 lr: 0.02\n",
            "iteration: 197730 loss: 0.0009 lr: 0.02\n",
            "iteration: 197740 loss: 0.0010 lr: 0.02\n",
            "iteration: 197750 loss: 0.0011 lr: 0.02\n",
            "iteration: 197760 loss: 0.0007 lr: 0.02\n",
            "iteration: 197770 loss: 0.0007 lr: 0.02\n",
            "iteration: 197780 loss: 0.0009 lr: 0.02\n",
            "iteration: 197790 loss: 0.0009 lr: 0.02\n",
            "iteration: 197800 loss: 0.0006 lr: 0.02\n",
            "iteration: 197810 loss: 0.0011 lr: 0.02\n",
            "iteration: 197820 loss: 0.0011 lr: 0.02\n",
            "iteration: 197830 loss: 0.0007 lr: 0.02\n",
            "iteration: 197840 loss: 0.0013 lr: 0.02\n",
            "iteration: 197850 loss: 0.0009 lr: 0.02\n",
            "iteration: 197860 loss: 0.0006 lr: 0.02\n",
            "iteration: 197870 loss: 0.0008 lr: 0.02\n",
            "iteration: 197880 loss: 0.0008 lr: 0.02\n",
            "iteration: 197890 loss: 0.0008 lr: 0.02\n",
            "iteration: 197900 loss: 0.0010 lr: 0.02\n",
            "iteration: 197910 loss: 0.0013 lr: 0.02\n",
            "iteration: 197920 loss: 0.0007 lr: 0.02\n",
            "iteration: 197930 loss: 0.0007 lr: 0.02\n",
            "iteration: 197940 loss: 0.0009 lr: 0.02\n",
            "iteration: 197950 loss: 0.0010 lr: 0.02\n",
            "iteration: 197960 loss: 0.0006 lr: 0.02\n",
            "iteration: 197970 loss: 0.0010 lr: 0.02\n",
            "iteration: 197980 loss: 0.0009 lr: 0.02\n",
            "iteration: 197990 loss: 0.0009 lr: 0.02\n",
            "iteration: 198000 loss: 0.0014 lr: 0.02\n",
            "iteration: 198010 loss: 0.0007 lr: 0.02\n",
            "iteration: 198020 loss: 0.0009 lr: 0.02\n",
            "iteration: 198030 loss: 0.0009 lr: 0.02\n",
            "iteration: 198040 loss: 0.0009 lr: 0.02\n",
            "iteration: 198050 loss: 0.0010 lr: 0.02\n",
            "iteration: 198060 loss: 0.0010 lr: 0.02\n",
            "iteration: 198070 loss: 0.0006 lr: 0.02\n",
            "iteration: 198080 loss: 0.0007 lr: 0.02\n",
            "iteration: 198090 loss: 0.0007 lr: 0.02\n",
            "iteration: 198100 loss: 0.0007 lr: 0.02\n",
            "iteration: 198110 loss: 0.0009 lr: 0.02\n",
            "iteration: 198120 loss: 0.0005 lr: 0.02\n",
            "iteration: 198130 loss: 0.0006 lr: 0.02\n",
            "iteration: 198140 loss: 0.0008 lr: 0.02\n",
            "iteration: 198150 loss: 0.0010 lr: 0.02\n",
            "iteration: 198160 loss: 0.0012 lr: 0.02\n",
            "iteration: 198170 loss: 0.0009 lr: 0.02\n",
            "iteration: 198180 loss: 0.0007 lr: 0.02\n",
            "iteration: 198190 loss: 0.0006 lr: 0.02\n",
            "iteration: 198200 loss: 0.0009 lr: 0.02\n",
            "iteration: 198210 loss: 0.0007 lr: 0.02\n",
            "iteration: 198220 loss: 0.0007 lr: 0.02\n",
            "iteration: 198230 loss: 0.0009 lr: 0.02\n",
            "iteration: 198240 loss: 0.0008 lr: 0.02\n",
            "iteration: 198250 loss: 0.0007 lr: 0.02\n",
            "iteration: 198260 loss: 0.0005 lr: 0.02\n",
            "iteration: 198270 loss: 0.0009 lr: 0.02\n",
            "iteration: 198280 loss: 0.0007 lr: 0.02\n",
            "iteration: 198290 loss: 0.0009 lr: 0.02\n",
            "iteration: 198300 loss: 0.0006 lr: 0.02\n",
            "iteration: 198310 loss: 0.0008 lr: 0.02\n",
            "iteration: 198320 loss: 0.0006 lr: 0.02\n",
            "iteration: 198330 loss: 0.0009 lr: 0.02\n",
            "iteration: 198340 loss: 0.0006 lr: 0.02\n",
            "iteration: 198350 loss: 0.0010 lr: 0.02\n",
            "iteration: 198360 loss: 0.0015 lr: 0.02\n",
            "iteration: 198370 loss: 0.0015 lr: 0.02\n",
            "iteration: 198380 loss: 0.0010 lr: 0.02\n",
            "iteration: 198390 loss: 0.0013 lr: 0.02\n",
            "iteration: 198400 loss: 0.0008 lr: 0.02\n",
            "iteration: 198410 loss: 0.0010 lr: 0.02\n",
            "iteration: 198420 loss: 0.0010 lr: 0.02\n",
            "iteration: 198430 loss: 0.0007 lr: 0.02\n",
            "iteration: 198440 loss: 0.0009 lr: 0.02\n",
            "iteration: 198450 loss: 0.0009 lr: 0.02\n",
            "iteration: 198460 loss: 0.0007 lr: 0.02\n",
            "iteration: 198470 loss: 0.0009 lr: 0.02\n",
            "iteration: 198480 loss: 0.0007 lr: 0.02\n",
            "iteration: 198490 loss: 0.0013 lr: 0.02\n",
            "iteration: 198500 loss: 0.0011 lr: 0.02\n",
            "iteration: 198510 loss: 0.0009 lr: 0.02\n",
            "iteration: 198520 loss: 0.0008 lr: 0.02\n",
            "iteration: 198530 loss: 0.0009 lr: 0.02\n",
            "iteration: 198540 loss: 0.0007 lr: 0.02\n",
            "iteration: 198550 loss: 0.0009 lr: 0.02\n",
            "iteration: 198560 loss: 0.0013 lr: 0.02\n",
            "iteration: 198570 loss: 0.0008 lr: 0.02\n",
            "iteration: 198580 loss: 0.0011 lr: 0.02\n",
            "iteration: 198590 loss: 0.0007 lr: 0.02\n",
            "iteration: 198600 loss: 0.0007 lr: 0.02\n",
            "iteration: 198610 loss: 0.0009 lr: 0.02\n",
            "iteration: 198620 loss: 0.0008 lr: 0.02\n",
            "iteration: 198630 loss: 0.0009 lr: 0.02\n",
            "iteration: 198640 loss: 0.0008 lr: 0.02\n",
            "iteration: 198650 loss: 0.0007 lr: 0.02\n",
            "iteration: 198660 loss: 0.0008 lr: 0.02\n",
            "iteration: 198670 loss: 0.0007 lr: 0.02\n",
            "iteration: 198680 loss: 0.0007 lr: 0.02\n",
            "iteration: 198690 loss: 0.0007 lr: 0.02\n",
            "iteration: 198700 loss: 0.0007 lr: 0.02\n",
            "iteration: 198710 loss: 0.0005 lr: 0.02\n",
            "iteration: 198720 loss: 0.0010 lr: 0.02\n",
            "iteration: 198730 loss: 0.0006 lr: 0.02\n",
            "iteration: 198740 loss: 0.0005 lr: 0.02\n",
            "iteration: 198750 loss: 0.0006 lr: 0.02\n",
            "iteration: 198760 loss: 0.0006 lr: 0.02\n",
            "iteration: 198770 loss: 0.0011 lr: 0.02\n",
            "iteration: 198780 loss: 0.0005 lr: 0.02\n",
            "iteration: 198790 loss: 0.0007 lr: 0.02\n",
            "iteration: 198800 loss: 0.0006 lr: 0.02\n",
            "iteration: 198810 loss: 0.0008 lr: 0.02\n",
            "iteration: 198820 loss: 0.0010 lr: 0.02\n",
            "iteration: 198830 loss: 0.0011 lr: 0.02\n",
            "iteration: 198840 loss: 0.0009 lr: 0.02\n",
            "iteration: 198850 loss: 0.0007 lr: 0.02\n",
            "iteration: 198860 loss: 0.0009 lr: 0.02\n",
            "iteration: 198870 loss: 0.0008 lr: 0.02\n",
            "iteration: 198880 loss: 0.0007 lr: 0.02\n",
            "iteration: 198890 loss: 0.0008 lr: 0.02\n",
            "iteration: 198900 loss: 0.0006 lr: 0.02\n",
            "iteration: 198910 loss: 0.0006 lr: 0.02\n",
            "iteration: 198920 loss: 0.0015 lr: 0.02\n",
            "iteration: 198930 loss: 0.0008 lr: 0.02\n",
            "iteration: 198940 loss: 0.0010 lr: 0.02\n",
            "iteration: 198950 loss: 0.0006 lr: 0.02\n",
            "iteration: 198960 loss: 0.0007 lr: 0.02\n",
            "iteration: 198970 loss: 0.0007 lr: 0.02\n",
            "iteration: 198980 loss: 0.0007 lr: 0.02\n",
            "iteration: 198990 loss: 0.0005 lr: 0.02\n",
            "iteration: 199000 loss: 0.0008 lr: 0.02\n",
            "iteration: 199010 loss: 0.0005 lr: 0.02\n",
            "iteration: 199020 loss: 0.0007 lr: 0.02\n",
            "iteration: 199030 loss: 0.0005 lr: 0.02\n",
            "iteration: 199040 loss: 0.0011 lr: 0.02\n",
            "iteration: 199050 loss: 0.0010 lr: 0.02\n",
            "iteration: 199060 loss: 0.0005 lr: 0.02\n",
            "iteration: 199070 loss: 0.0009 lr: 0.02\n",
            "iteration: 199080 loss: 0.0008 lr: 0.02\n",
            "iteration: 199090 loss: 0.0008 lr: 0.02\n",
            "iteration: 199100 loss: 0.0007 lr: 0.02\n",
            "iteration: 199110 loss: 0.0008 lr: 0.02\n",
            "iteration: 199120 loss: 0.0008 lr: 0.02\n",
            "iteration: 199130 loss: 0.0007 lr: 0.02\n",
            "iteration: 199140 loss: 0.0011 lr: 0.02\n",
            "iteration: 199150 loss: 0.0010 lr: 0.02\n",
            "iteration: 199160 loss: 0.0008 lr: 0.02\n",
            "iteration: 199170 loss: 0.0007 lr: 0.02\n",
            "iteration: 199180 loss: 0.0009 lr: 0.02\n",
            "iteration: 199190 loss: 0.0007 lr: 0.02\n",
            "iteration: 199200 loss: 0.0006 lr: 0.02\n",
            "iteration: 199210 loss: 0.0012 lr: 0.02\n",
            "iteration: 199220 loss: 0.0006 lr: 0.02\n",
            "iteration: 199230 loss: 0.0006 lr: 0.02\n",
            "iteration: 199240 loss: 0.0007 lr: 0.02\n",
            "iteration: 199250 loss: 0.0006 lr: 0.02\n",
            "iteration: 199260 loss: 0.0010 lr: 0.02\n",
            "iteration: 199270 loss: 0.0008 lr: 0.02\n",
            "iteration: 199280 loss: 0.0007 lr: 0.02\n",
            "iteration: 199290 loss: 0.0010 lr: 0.02\n",
            "iteration: 199300 loss: 0.0009 lr: 0.02\n",
            "iteration: 199310 loss: 0.0010 lr: 0.02\n",
            "iteration: 199320 loss: 0.0006 lr: 0.02\n",
            "iteration: 199330 loss: 0.0007 lr: 0.02\n",
            "iteration: 199340 loss: 0.0008 lr: 0.02\n",
            "iteration: 199350 loss: 0.0009 lr: 0.02\n",
            "iteration: 199360 loss: 0.0008 lr: 0.02\n",
            "iteration: 199370 loss: 0.0007 lr: 0.02\n",
            "iteration: 199380 loss: 0.0008 lr: 0.02\n",
            "iteration: 199390 loss: 0.0007 lr: 0.02\n",
            "iteration: 199400 loss: 0.0005 lr: 0.02\n",
            "iteration: 199410 loss: 0.0005 lr: 0.02\n",
            "iteration: 199420 loss: 0.0007 lr: 0.02\n",
            "iteration: 199430 loss: 0.0006 lr: 0.02\n",
            "iteration: 199440 loss: 0.0008 lr: 0.02\n",
            "iteration: 199450 loss: 0.0008 lr: 0.02\n",
            "iteration: 199460 loss: 0.0006 lr: 0.02\n",
            "iteration: 199470 loss: 0.0006 lr: 0.02\n",
            "iteration: 199480 loss: 0.0006 lr: 0.02\n",
            "iteration: 199490 loss: 0.0006 lr: 0.02\n",
            "iteration: 199500 loss: 0.0005 lr: 0.02\n",
            "iteration: 199510 loss: 0.0009 lr: 0.02\n",
            "iteration: 199520 loss: 0.0006 lr: 0.02\n",
            "iteration: 199530 loss: 0.0009 lr: 0.02\n",
            "iteration: 199540 loss: 0.0007 lr: 0.02\n",
            "iteration: 199550 loss: 0.0006 lr: 0.02\n",
            "iteration: 199560 loss: 0.0009 lr: 0.02\n",
            "iteration: 199570 loss: 0.0008 lr: 0.02\n",
            "iteration: 199580 loss: 0.0006 lr: 0.02\n",
            "iteration: 199590 loss: 0.0009 lr: 0.02\n",
            "iteration: 199600 loss: 0.0008 lr: 0.02\n",
            "iteration: 199610 loss: 0.0008 lr: 0.02\n",
            "iteration: 199620 loss: 0.0010 lr: 0.02\n",
            "iteration: 199630 loss: 0.0009 lr: 0.02\n",
            "iteration: 199640 loss: 0.0009 lr: 0.02\n",
            "iteration: 199650 loss: 0.0007 lr: 0.02\n",
            "iteration: 199660 loss: 0.0010 lr: 0.02\n",
            "iteration: 199670 loss: 0.0012 lr: 0.02\n",
            "iteration: 199680 loss: 0.0009 lr: 0.02\n",
            "iteration: 199690 loss: 0.0009 lr: 0.02\n",
            "iteration: 199700 loss: 0.0008 lr: 0.02\n",
            "iteration: 199710 loss: 0.0009 lr: 0.02\n",
            "iteration: 199720 loss: 0.0009 lr: 0.02\n",
            "iteration: 199730 loss: 0.0008 lr: 0.02\n",
            "iteration: 199740 loss: 0.0007 lr: 0.02\n",
            "iteration: 199750 loss: 0.0008 lr: 0.02\n",
            "iteration: 199760 loss: 0.0008 lr: 0.02\n",
            "iteration: 199770 loss: 0.0007 lr: 0.02\n",
            "iteration: 199780 loss: 0.0009 lr: 0.02\n",
            "iteration: 199790 loss: 0.0007 lr: 0.02\n",
            "iteration: 199800 loss: 0.0007 lr: 0.02\n",
            "iteration: 199810 loss: 0.0008 lr: 0.02\n",
            "iteration: 199820 loss: 0.0010 lr: 0.02\n",
            "iteration: 199830 loss: 0.0006 lr: 0.02\n",
            "iteration: 199840 loss: 0.0007 lr: 0.02\n",
            "iteration: 199850 loss: 0.0004 lr: 0.02\n",
            "iteration: 199860 loss: 0.0011 lr: 0.02\n",
            "iteration: 199870 loss: 0.0009 lr: 0.02\n",
            "iteration: 199880 loss: 0.0005 lr: 0.02\n",
            "iteration: 199890 loss: 0.0007 lr: 0.02\n",
            "iteration: 199900 loss: 0.0012 lr: 0.02\n",
            "iteration: 199910 loss: 0.0008 lr: 0.02\n",
            "iteration: 199920 loss: 0.0007 lr: 0.02\n",
            "iteration: 199930 loss: 0.0007 lr: 0.02\n",
            "iteration: 199940 loss: 0.0008 lr: 0.02\n",
            "iteration: 199950 loss: 0.0008 lr: 0.02\n",
            "iteration: 199960 loss: 0.0011 lr: 0.02\n",
            "iteration: 199970 loss: 0.0009 lr: 0.02\n",
            "iteration: 199980 loss: 0.0008 lr: 0.02\n",
            "iteration: 199990 loss: 0.0006 lr: 0.02\n",
            "iteration: 200000 loss: 0.0006 lr: 0.02\n",
            "iteration: 200010 loss: 0.0006 lr: 0.02\n",
            "iteration: 200020 loss: 0.0007 lr: 0.02\n",
            "iteration: 200030 loss: 0.0008 lr: 0.02\n",
            "iteration: 200040 loss: 0.0009 lr: 0.02\n",
            "iteration: 200050 loss: 0.0009 lr: 0.02\n",
            "iteration: 200060 loss: 0.0012 lr: 0.02\n",
            "iteration: 200070 loss: 0.0013 lr: 0.02\n",
            "iteration: 200080 loss: 0.0009 lr: 0.02\n",
            "iteration: 200090 loss: 0.0007 lr: 0.02\n",
            "iteration: 200100 loss: 0.0008 lr: 0.02\n",
            "iteration: 200110 loss: 0.0009 lr: 0.02\n",
            "iteration: 200120 loss: 0.0012 lr: 0.02\n",
            "iteration: 200130 loss: 0.0010 lr: 0.02\n",
            "iteration: 200140 loss: 0.0011 lr: 0.02\n",
            "iteration: 200150 loss: 0.0008 lr: 0.02\n",
            "iteration: 200160 loss: 0.0008 lr: 0.02\n",
            "iteration: 200170 loss: 0.0007 lr: 0.02\n",
            "iteration: 200180 loss: 0.0007 lr: 0.02\n",
            "iteration: 200190 loss: 0.0007 lr: 0.02\n",
            "iteration: 200200 loss: 0.0007 lr: 0.02\n",
            "iteration: 200210 loss: 0.0007 lr: 0.02\n",
            "iteration: 200220 loss: 0.0008 lr: 0.02\n",
            "iteration: 200230 loss: 0.0008 lr: 0.02\n",
            "iteration: 200240 loss: 0.0008 lr: 0.02\n",
            "iteration: 200250 loss: 0.0008 lr: 0.02\n",
            "iteration: 200260 loss: 0.0009 lr: 0.02\n",
            "iteration: 200270 loss: 0.0009 lr: 0.02\n",
            "iteration: 200280 loss: 0.0006 lr: 0.02\n",
            "iteration: 200290 loss: 0.0008 lr: 0.02\n",
            "iteration: 200300 loss: 0.0007 lr: 0.02\n",
            "iteration: 200310 loss: 0.0009 lr: 0.02\n",
            "iteration: 200320 loss: 0.0009 lr: 0.02\n",
            "iteration: 200330 loss: 0.0007 lr: 0.02\n",
            "iteration: 200340 loss: 0.0006 lr: 0.02\n",
            "iteration: 200350 loss: 0.0007 lr: 0.02\n",
            "iteration: 200360 loss: 0.0006 lr: 0.02\n",
            "iteration: 200370 loss: 0.0012 lr: 0.02\n",
            "iteration: 200380 loss: 0.0008 lr: 0.02\n",
            "iteration: 200390 loss: 0.0012 lr: 0.02\n",
            "iteration: 200400 loss: 0.0014 lr: 0.02\n",
            "iteration: 200410 loss: 0.0009 lr: 0.02\n",
            "iteration: 200420 loss: 0.0007 lr: 0.02\n",
            "iteration: 200430 loss: 0.0008 lr: 0.02\n",
            "iteration: 200440 loss: 0.0008 lr: 0.02\n",
            "iteration: 200450 loss: 0.0010 lr: 0.02\n",
            "iteration: 200460 loss: 0.0008 lr: 0.02\n",
            "iteration: 200470 loss: 0.0006 lr: 0.02\n",
            "iteration: 200480 loss: 0.0008 lr: 0.02\n",
            "iteration: 200490 loss: 0.0007 lr: 0.02\n",
            "iteration: 200500 loss: 0.0006 lr: 0.02\n",
            "iteration: 200510 loss: 0.0010 lr: 0.02\n",
            "iteration: 200520 loss: 0.0006 lr: 0.02\n",
            "iteration: 200530 loss: 0.0010 lr: 0.02\n",
            "iteration: 200540 loss: 0.0007 lr: 0.02\n",
            "iteration: 200550 loss: 0.0006 lr: 0.02\n",
            "iteration: 200560 loss: 0.0008 lr: 0.02\n",
            "iteration: 200570 loss: 0.0007 lr: 0.02\n",
            "iteration: 200580 loss: 0.0006 lr: 0.02\n",
            "iteration: 200590 loss: 0.0008 lr: 0.02\n",
            "iteration: 200600 loss: 0.0006 lr: 0.02\n",
            "iteration: 200610 loss: 0.0012 lr: 0.02\n",
            "iteration: 200620 loss: 0.0007 lr: 0.02\n",
            "iteration: 200630 loss: 0.0008 lr: 0.02\n",
            "iteration: 200640 loss: 0.0009 lr: 0.02\n",
            "iteration: 200650 loss: 0.0007 lr: 0.02\n",
            "iteration: 200660 loss: 0.0007 lr: 0.02\n",
            "iteration: 200670 loss: 0.0008 lr: 0.02\n",
            "iteration: 200680 loss: 0.0007 lr: 0.02\n",
            "iteration: 200690 loss: 0.0008 lr: 0.02\n",
            "iteration: 200700 loss: 0.0008 lr: 0.02\n",
            "iteration: 200710 loss: 0.0006 lr: 0.02\n",
            "iteration: 200720 loss: 0.0008 lr: 0.02\n",
            "iteration: 200730 loss: 0.0007 lr: 0.02\n",
            "iteration: 200740 loss: 0.0009 lr: 0.02\n",
            "iteration: 200750 loss: 0.0006 lr: 0.02\n",
            "iteration: 200760 loss: 0.0009 lr: 0.02\n",
            "iteration: 200770 loss: 0.0010 lr: 0.02\n",
            "iteration: 200780 loss: 0.0006 lr: 0.02\n",
            "iteration: 200790 loss: 0.0007 lr: 0.02\n",
            "iteration: 200800 loss: 0.0007 lr: 0.02\n",
            "iteration: 200810 loss: 0.0006 lr: 0.02\n",
            "iteration: 200820 loss: 0.0006 lr: 0.02\n",
            "iteration: 200830 loss: 0.0010 lr: 0.02\n",
            "iteration: 200840 loss: 0.0006 lr: 0.02\n",
            "iteration: 200850 loss: 0.0006 lr: 0.02\n",
            "iteration: 200860 loss: 0.0008 lr: 0.02\n",
            "iteration: 200870 loss: 0.0007 lr: 0.02\n",
            "iteration: 200880 loss: 0.0010 lr: 0.02\n",
            "iteration: 200890 loss: 0.0007 lr: 0.02\n",
            "iteration: 200900 loss: 0.0010 lr: 0.02\n",
            "iteration: 200910 loss: 0.0007 lr: 0.02\n",
            "iteration: 200920 loss: 0.0006 lr: 0.02\n",
            "iteration: 200930 loss: 0.0008 lr: 0.02\n",
            "iteration: 200940 loss: 0.0011 lr: 0.02\n",
            "iteration: 200950 loss: 0.0009 lr: 0.02\n",
            "iteration: 200960 loss: 0.0008 lr: 0.02\n",
            "iteration: 200970 loss: 0.0007 lr: 0.02\n",
            "iteration: 200980 loss: 0.0008 lr: 0.02\n",
            "iteration: 200990 loss: 0.0007 lr: 0.02\n",
            "iteration: 201000 loss: 0.0007 lr: 0.02\n",
            "iteration: 201010 loss: 0.0008 lr: 0.02\n",
            "iteration: 201020 loss: 0.0008 lr: 0.02\n",
            "iteration: 201030 loss: 0.0007 lr: 0.02\n",
            "iteration: 201040 loss: 0.0011 lr: 0.02\n",
            "iteration: 201050 loss: 0.0007 lr: 0.02\n",
            "iteration: 201060 loss: 0.0008 lr: 0.02\n",
            "iteration: 201070 loss: 0.0009 lr: 0.02\n",
            "iteration: 201080 loss: 0.0008 lr: 0.02\n",
            "iteration: 201090 loss: 0.0008 lr: 0.02\n",
            "iteration: 201100 loss: 0.0008 lr: 0.02\n",
            "iteration: 201110 loss: 0.0007 lr: 0.02\n",
            "iteration: 201120 loss: 0.0008 lr: 0.02\n",
            "iteration: 201130 loss: 0.0009 lr: 0.02\n",
            "iteration: 201140 loss: 0.0007 lr: 0.02\n",
            "iteration: 201150 loss: 0.0009 lr: 0.02\n",
            "iteration: 201160 loss: 0.0010 lr: 0.02\n",
            "iteration: 201170 loss: 0.0009 lr: 0.02\n",
            "iteration: 201180 loss: 0.0010 lr: 0.02\n",
            "iteration: 201190 loss: 0.0008 lr: 0.02\n",
            "iteration: 201200 loss: 0.0013 lr: 0.02\n",
            "iteration: 201210 loss: 0.0006 lr: 0.02\n",
            "iteration: 201220 loss: 0.0010 lr: 0.02\n",
            "iteration: 201230 loss: 0.0011 lr: 0.02\n",
            "iteration: 201240 loss: 0.0008 lr: 0.02\n",
            "iteration: 201250 loss: 0.0006 lr: 0.02\n",
            "iteration: 201260 loss: 0.0010 lr: 0.02\n",
            "iteration: 201270 loss: 0.0008 lr: 0.02\n",
            "iteration: 201280 loss: 0.0010 lr: 0.02\n",
            "iteration: 201290 loss: 0.0011 lr: 0.02\n",
            "iteration: 201300 loss: 0.0008 lr: 0.02\n",
            "iteration: 201310 loss: 0.0008 lr: 0.02\n",
            "iteration: 201320 loss: 0.0006 lr: 0.02\n",
            "iteration: 201330 loss: 0.0009 lr: 0.02\n",
            "iteration: 201340 loss: 0.0006 lr: 0.02\n",
            "iteration: 201350 loss: 0.0014 lr: 0.02\n",
            "iteration: 201360 loss: 0.0007 lr: 0.02\n",
            "iteration: 201370 loss: 0.0007 lr: 0.02\n",
            "iteration: 201380 loss: 0.0006 lr: 0.02\n",
            "iteration: 201390 loss: 0.0014 lr: 0.02\n",
            "iteration: 201400 loss: 0.0008 lr: 0.02\n",
            "iteration: 201410 loss: 0.0008 lr: 0.02\n",
            "iteration: 201420 loss: 0.0008 lr: 0.02\n",
            "iteration: 201430 loss: 0.0006 lr: 0.02\n",
            "iteration: 201440 loss: 0.0008 lr: 0.02\n",
            "iteration: 201450 loss: 0.0010 lr: 0.02\n",
            "iteration: 201460 loss: 0.0006 lr: 0.02\n",
            "iteration: 201470 loss: 0.0008 lr: 0.02\n",
            "iteration: 201480 loss: 0.0008 lr: 0.02\n",
            "iteration: 201490 loss: 0.0007 lr: 0.02\n",
            "iteration: 201500 loss: 0.0008 lr: 0.02\n",
            "iteration: 201510 loss: 0.0010 lr: 0.02\n",
            "iteration: 201520 loss: 0.0010 lr: 0.02\n",
            "iteration: 201530 loss: 0.0007 lr: 0.02\n",
            "iteration: 201540 loss: 0.0006 lr: 0.02\n",
            "iteration: 201550 loss: 0.0010 lr: 0.02\n",
            "iteration: 201560 loss: 0.0007 lr: 0.02\n",
            "iteration: 201570 loss: 0.0007 lr: 0.02\n",
            "iteration: 201580 loss: 0.0014 lr: 0.02\n",
            "iteration: 201590 loss: 0.0007 lr: 0.02\n",
            "iteration: 201600 loss: 0.0013 lr: 0.02\n",
            "iteration: 201610 loss: 0.0010 lr: 0.02\n",
            "iteration: 201620 loss: 0.0008 lr: 0.02\n",
            "iteration: 201630 loss: 0.0008 lr: 0.02\n",
            "iteration: 201640 loss: 0.0007 lr: 0.02\n",
            "iteration: 201650 loss: 0.0008 lr: 0.02\n",
            "iteration: 201660 loss: 0.0007 lr: 0.02\n",
            "iteration: 201670 loss: 0.0008 lr: 0.02\n",
            "iteration: 201680 loss: 0.0012 lr: 0.02\n",
            "iteration: 201690 loss: 0.0010 lr: 0.02\n",
            "iteration: 201700 loss: 0.0010 lr: 0.02\n",
            "iteration: 201710 loss: 0.0012 lr: 0.02\n",
            "iteration: 201720 loss: 0.0010 lr: 0.02\n",
            "iteration: 201730 loss: 0.0008 lr: 0.02\n",
            "iteration: 201740 loss: 0.0008 lr: 0.02\n",
            "iteration: 201750 loss: 0.0008 lr: 0.02\n",
            "iteration: 201760 loss: 0.0007 lr: 0.02\n",
            "iteration: 201770 loss: 0.0008 lr: 0.02\n",
            "iteration: 201780 loss: 0.0008 lr: 0.02\n",
            "iteration: 201790 loss: 0.0009 lr: 0.02\n",
            "iteration: 201800 loss: 0.0007 lr: 0.02\n",
            "iteration: 201810 loss: 0.0007 lr: 0.02\n",
            "iteration: 201820 loss: 0.0008 lr: 0.02\n",
            "iteration: 201830 loss: 0.0007 lr: 0.02\n",
            "iteration: 201840 loss: 0.0009 lr: 0.02\n",
            "iteration: 201850 loss: 0.0007 lr: 0.02\n",
            "iteration: 201860 loss: 0.0011 lr: 0.02\n",
            "iteration: 201870 loss: 0.0007 lr: 0.02\n",
            "iteration: 201880 loss: 0.0008 lr: 0.02\n",
            "iteration: 201890 loss: 0.0008 lr: 0.02\n",
            "iteration: 201900 loss: 0.0008 lr: 0.02\n",
            "iteration: 201910 loss: 0.0009 lr: 0.02\n",
            "iteration: 201920 loss: 0.0008 lr: 0.02\n",
            "iteration: 201930 loss: 0.0007 lr: 0.02\n",
            "iteration: 201940 loss: 0.0011 lr: 0.02\n",
            "iteration: 201950 loss: 0.0008 lr: 0.02\n",
            "iteration: 201960 loss: 0.0010 lr: 0.02\n",
            "iteration: 201970 loss: 0.0008 lr: 0.02\n",
            "iteration: 201980 loss: 0.0010 lr: 0.02\n",
            "iteration: 201990 loss: 0.0008 lr: 0.02\n",
            "iteration: 202000 loss: 0.0010 lr: 0.02\n",
            "iteration: 202010 loss: 0.0009 lr: 0.02\n",
            "iteration: 202020 loss: 0.0009 lr: 0.02\n",
            "iteration: 202030 loss: 0.0009 lr: 0.02\n",
            "iteration: 202040 loss: 0.0007 lr: 0.02\n",
            "iteration: 202050 loss: 0.0012 lr: 0.02\n",
            "iteration: 202060 loss: 0.0008 lr: 0.02\n",
            "iteration: 202070 loss: 0.0008 lr: 0.02\n",
            "iteration: 202080 loss: 0.0007 lr: 0.02\n",
            "iteration: 202090 loss: 0.0008 lr: 0.02\n",
            "iteration: 202100 loss: 0.0010 lr: 0.02\n",
            "iteration: 202110 loss: 0.0007 lr: 0.02\n",
            "iteration: 202120 loss: 0.0006 lr: 0.02\n",
            "iteration: 202130 loss: 0.0005 lr: 0.02\n",
            "iteration: 202140 loss: 0.0007 lr: 0.02\n",
            "iteration: 202150 loss: 0.0009 lr: 0.02\n",
            "iteration: 202160 loss: 0.0009 lr: 0.02\n",
            "iteration: 202170 loss: 0.0008 lr: 0.02\n",
            "iteration: 202180 loss: 0.0007 lr: 0.02\n",
            "iteration: 202190 loss: 0.0009 lr: 0.02\n",
            "iteration: 202200 loss: 0.0007 lr: 0.02\n",
            "iteration: 202210 loss: 0.0008 lr: 0.02\n",
            "iteration: 202220 loss: 0.0008 lr: 0.02\n",
            "iteration: 202230 loss: 0.0008 lr: 0.02\n",
            "iteration: 202240 loss: 0.0005 lr: 0.02\n",
            "iteration: 202250 loss: 0.0006 lr: 0.02\n",
            "iteration: 202260 loss: 0.0007 lr: 0.02\n",
            "iteration: 202270 loss: 0.0008 lr: 0.02\n",
            "iteration: 202280 loss: 0.0009 lr: 0.02\n",
            "iteration: 202290 loss: 0.0008 lr: 0.02\n",
            "iteration: 202300 loss: 0.0007 lr: 0.02\n",
            "iteration: 202310 loss: 0.0006 lr: 0.02\n",
            "iteration: 202320 loss: 0.0008 lr: 0.02\n",
            "iteration: 202330 loss: 0.0005 lr: 0.02\n",
            "iteration: 202340 loss: 0.0007 lr: 0.02\n",
            "iteration: 202350 loss: 0.0007 lr: 0.02\n",
            "iteration: 202360 loss: 0.0009 lr: 0.02\n",
            "iteration: 202370 loss: 0.0010 lr: 0.02\n",
            "iteration: 202380 loss: 0.0009 lr: 0.02\n",
            "iteration: 202390 loss: 0.0006 lr: 0.02\n",
            "iteration: 202400 loss: 0.0007 lr: 0.02\n",
            "iteration: 202410 loss: 0.0007 lr: 0.02\n",
            "iteration: 202420 loss: 0.0008 lr: 0.02\n",
            "iteration: 202430 loss: 0.0008 lr: 0.02\n",
            "iteration: 202440 loss: 0.0011 lr: 0.02\n",
            "iteration: 202450 loss: 0.0007 lr: 0.02\n",
            "iteration: 202460 loss: 0.0008 lr: 0.02\n",
            "iteration: 202470 loss: 0.0006 lr: 0.02\n",
            "iteration: 202480 loss: 0.0010 lr: 0.02\n",
            "iteration: 202490 loss: 0.0009 lr: 0.02\n",
            "iteration: 202500 loss: 0.0008 lr: 0.02\n",
            "iteration: 202510 loss: 0.0014 lr: 0.02\n",
            "iteration: 202520 loss: 0.0006 lr: 0.02\n",
            "iteration: 202530 loss: 0.0008 lr: 0.02\n",
            "iteration: 202540 loss: 0.0007 lr: 0.02\n",
            "iteration: 202550 loss: 0.0008 lr: 0.02\n",
            "iteration: 202560 loss: 0.0010 lr: 0.02\n",
            "iteration: 202570 loss: 0.0010 lr: 0.02\n",
            "iteration: 202580 loss: 0.0009 lr: 0.02\n",
            "iteration: 202590 loss: 0.0008 lr: 0.02\n",
            "iteration: 202600 loss: 0.0016 lr: 0.02\n",
            "iteration: 202610 loss: 0.0008 lr: 0.02\n",
            "iteration: 202620 loss: 0.0012 lr: 0.02\n",
            "iteration: 202630 loss: 0.0011 lr: 0.02\n",
            "iteration: 202640 loss: 0.0012 lr: 0.02\n",
            "iteration: 202650 loss: 0.0008 lr: 0.02\n",
            "iteration: 202660 loss: 0.0006 lr: 0.02\n",
            "iteration: 202670 loss: 0.0007 lr: 0.02\n",
            "iteration: 202680 loss: 0.0010 lr: 0.02\n",
            "iteration: 202690 loss: 0.0008 lr: 0.02\n",
            "iteration: 202700 loss: 0.0009 lr: 0.02\n",
            "iteration: 202710 loss: 0.0015 lr: 0.02\n",
            "iteration: 202720 loss: 0.0008 lr: 0.02\n",
            "iteration: 202730 loss: 0.0007 lr: 0.02\n",
            "iteration: 202740 loss: 0.0012 lr: 0.02\n",
            "iteration: 202750 loss: 0.0012 lr: 0.02\n",
            "iteration: 202760 loss: 0.0011 lr: 0.02\n",
            "iteration: 202770 loss: 0.0007 lr: 0.02\n",
            "iteration: 202780 loss: 0.0009 lr: 0.02\n",
            "iteration: 202790 loss: 0.0009 lr: 0.02\n",
            "iteration: 202800 loss: 0.0010 lr: 0.02\n",
            "iteration: 202810 loss: 0.0006 lr: 0.02\n",
            "iteration: 202820 loss: 0.0009 lr: 0.02\n",
            "iteration: 202830 loss: 0.0009 lr: 0.02\n",
            "iteration: 202840 loss: 0.0007 lr: 0.02\n",
            "iteration: 202850 loss: 0.0008 lr: 0.02\n",
            "iteration: 202860 loss: 0.0008 lr: 0.02\n",
            "iteration: 202870 loss: 0.0007 lr: 0.02\n",
            "iteration: 202880 loss: 0.0007 lr: 0.02\n",
            "iteration: 202890 loss: 0.0006 lr: 0.02\n",
            "iteration: 202900 loss: 0.0008 lr: 0.02\n",
            "iteration: 202910 loss: 0.0006 lr: 0.02\n",
            "iteration: 202920 loss: 0.0007 lr: 0.02\n",
            "iteration: 202930 loss: 0.0007 lr: 0.02\n",
            "iteration: 202940 loss: 0.0007 lr: 0.02\n",
            "iteration: 202950 loss: 0.0006 lr: 0.02\n",
            "iteration: 202960 loss: 0.0007 lr: 0.02\n",
            "iteration: 202970 loss: 0.0007 lr: 0.02\n",
            "iteration: 202980 loss: 0.0006 lr: 0.02\n",
            "iteration: 202990 loss: 0.0006 lr: 0.02\n",
            "iteration: 203000 loss: 0.0006 lr: 0.02\n",
            "iteration: 203010 loss: 0.0007 lr: 0.02\n",
            "iteration: 203020 loss: 0.0006 lr: 0.02\n",
            "iteration: 203030 loss: 0.0008 lr: 0.02\n",
            "iteration: 203040 loss: 0.0006 lr: 0.02\n",
            "iteration: 203050 loss: 0.0009 lr: 0.02\n",
            "iteration: 203060 loss: 0.0006 lr: 0.02\n",
            "iteration: 203070 loss: 0.0008 lr: 0.02\n",
            "iteration: 203080 loss: 0.0008 lr: 0.02\n",
            "iteration: 203090 loss: 0.0006 lr: 0.02\n",
            "iteration: 203100 loss: 0.0009 lr: 0.02\n",
            "iteration: 203110 loss: 0.0009 lr: 0.02\n",
            "iteration: 203120 loss: 0.0007 lr: 0.02\n",
            "iteration: 203130 loss: 0.0009 lr: 0.02\n",
            "iteration: 203140 loss: 0.0010 lr: 0.02\n",
            "iteration: 203150 loss: 0.0009 lr: 0.02\n",
            "iteration: 203160 loss: 0.0009 lr: 0.02\n",
            "iteration: 203170 loss: 0.0007 lr: 0.02\n",
            "iteration: 203180 loss: 0.0006 lr: 0.02\n",
            "iteration: 203190 loss: 0.0009 lr: 0.02\n",
            "iteration: 203200 loss: 0.0008 lr: 0.02\n",
            "iteration: 203210 loss: 0.0008 lr: 0.02\n",
            "iteration: 203220 loss: 0.0007 lr: 0.02\n",
            "iteration: 203230 loss: 0.0009 lr: 0.02\n",
            "iteration: 203240 loss: 0.0010 lr: 0.02\n",
            "iteration: 203250 loss: 0.0010 lr: 0.02\n",
            "iteration: 203260 loss: 0.0007 lr: 0.02\n",
            "iteration: 203270 loss: 0.0008 lr: 0.02\n",
            "iteration: 203280 loss: 0.0010 lr: 0.02\n",
            "iteration: 203290 loss: 0.0008 lr: 0.02\n",
            "iteration: 203300 loss: 0.0008 lr: 0.02\n",
            "iteration: 203310 loss: 0.0010 lr: 0.02\n",
            "iteration: 203320 loss: 0.0008 lr: 0.02\n",
            "iteration: 203330 loss: 0.0009 lr: 0.02\n",
            "iteration: 203340 loss: 0.0007 lr: 0.02\n",
            "iteration: 203350 loss: 0.0009 lr: 0.02\n",
            "iteration: 203360 loss: 0.0009 lr: 0.02\n",
            "iteration: 203370 loss: 0.0009 lr: 0.02\n",
            "iteration: 203380 loss: 0.0005 lr: 0.02\n",
            "iteration: 203390 loss: 0.0006 lr: 0.02\n",
            "iteration: 203400 loss: 0.0010 lr: 0.02\n",
            "iteration: 203410 loss: 0.0006 lr: 0.02\n",
            "iteration: 203420 loss: 0.0007 lr: 0.02\n",
            "iteration: 203430 loss: 0.0008 lr: 0.02\n",
            "iteration: 203440 loss: 0.0008 lr: 0.02\n",
            "iteration: 203450 loss: 0.0007 lr: 0.02\n",
            "iteration: 203460 loss: 0.0007 lr: 0.02\n",
            "iteration: 203470 loss: 0.0008 lr: 0.02\n",
            "iteration: 203480 loss: 0.0007 lr: 0.02\n",
            "iteration: 203490 loss: 0.0008 lr: 0.02\n",
            "iteration: 203500 loss: 0.0006 lr: 0.02\n",
            "iteration: 203510 loss: 0.0006 lr: 0.02\n",
            "iteration: 203520 loss: 0.0008 lr: 0.02\n",
            "iteration: 203530 loss: 0.0005 lr: 0.02\n",
            "iteration: 203540 loss: 0.0006 lr: 0.02\n",
            "iteration: 203550 loss: 0.0006 lr: 0.02\n",
            "iteration: 203560 loss: 0.0007 lr: 0.02\n",
            "iteration: 203570 loss: 0.0009 lr: 0.02\n",
            "iteration: 203580 loss: 0.0007 lr: 0.02\n",
            "iteration: 203590 loss: 0.0006 lr: 0.02\n",
            "iteration: 203600 loss: 0.0007 lr: 0.02\n",
            "iteration: 203610 loss: 0.0006 lr: 0.02\n",
            "iteration: 203620 loss: 0.0010 lr: 0.02\n",
            "iteration: 203630 loss: 0.0007 lr: 0.02\n",
            "iteration: 203640 loss: 0.0007 lr: 0.02\n",
            "iteration: 203650 loss: 0.0010 lr: 0.02\n",
            "iteration: 203660 loss: 0.0006 lr: 0.02\n",
            "iteration: 203670 loss: 0.0005 lr: 0.02\n",
            "iteration: 203680 loss: 0.0008 lr: 0.02\n",
            "iteration: 203690 loss: 0.0006 lr: 0.02\n",
            "iteration: 203700 loss: 0.0006 lr: 0.02\n",
            "iteration: 203710 loss: 0.0007 lr: 0.02\n",
            "iteration: 203720 loss: 0.0006 lr: 0.02\n",
            "iteration: 203730 loss: 0.0007 lr: 0.02\n",
            "iteration: 203740 loss: 0.0006 lr: 0.02\n",
            "iteration: 203750 loss: 0.0008 lr: 0.02\n",
            "iteration: 203760 loss: 0.0007 lr: 0.02\n",
            "iteration: 203770 loss: 0.0005 lr: 0.02\n",
            "iteration: 203780 loss: 0.0008 lr: 0.02\n",
            "iteration: 203790 loss: 0.0009 lr: 0.02\n",
            "iteration: 203800 loss: 0.0011 lr: 0.02\n",
            "iteration: 203810 loss: 0.0010 lr: 0.02\n",
            "iteration: 203820 loss: 0.0009 lr: 0.02\n",
            "iteration: 203830 loss: 0.0007 lr: 0.02\n",
            "iteration: 203840 loss: 0.0009 lr: 0.02\n",
            "iteration: 203850 loss: 0.0011 lr: 0.02\n",
            "iteration: 203860 loss: 0.0008 lr: 0.02\n",
            "iteration: 203870 loss: 0.0010 lr: 0.02\n",
            "iteration: 203880 loss: 0.0008 lr: 0.02\n",
            "iteration: 203890 loss: 0.0009 lr: 0.02\n",
            "iteration: 203900 loss: 0.0010 lr: 0.02\n",
            "iteration: 203910 loss: 0.0009 lr: 0.02\n",
            "iteration: 203920 loss: 0.0010 lr: 0.02\n",
            "iteration: 203930 loss: 0.0008 lr: 0.02\n",
            "iteration: 203940 loss: 0.0012 lr: 0.02\n",
            "iteration: 203950 loss: 0.0009 lr: 0.02\n",
            "iteration: 203960 loss: 0.0009 lr: 0.02\n",
            "iteration: 203970 loss: 0.0005 lr: 0.02\n",
            "iteration: 203980 loss: 0.0006 lr: 0.02\n",
            "iteration: 203990 loss: 0.0009 lr: 0.02\n",
            "iteration: 204000 loss: 0.0008 lr: 0.02\n",
            "iteration: 204010 loss: 0.0008 lr: 0.02\n",
            "iteration: 204020 loss: 0.0009 lr: 0.02\n",
            "iteration: 204030 loss: 0.0007 lr: 0.02\n",
            "iteration: 204040 loss: 0.0008 lr: 0.02\n",
            "iteration: 204050 loss: 0.0011 lr: 0.02\n",
            "iteration: 204060 loss: 0.0010 lr: 0.02\n",
            "iteration: 204070 loss: 0.0011 lr: 0.02\n",
            "iteration: 204080 loss: 0.0008 lr: 0.02\n",
            "iteration: 204090 loss: 0.0010 lr: 0.02\n",
            "iteration: 204100 loss: 0.0006 lr: 0.02\n",
            "iteration: 204110 loss: 0.0006 lr: 0.02\n",
            "iteration: 204120 loss: 0.0006 lr: 0.02\n",
            "iteration: 204130 loss: 0.0010 lr: 0.02\n",
            "iteration: 204140 loss: 0.0007 lr: 0.02\n",
            "iteration: 204150 loss: 0.0006 lr: 0.02\n",
            "iteration: 204160 loss: 0.0006 lr: 0.02\n",
            "iteration: 204170 loss: 0.0007 lr: 0.02\n",
            "iteration: 204180 loss: 0.0007 lr: 0.02\n",
            "iteration: 204190 loss: 0.0008 lr: 0.02\n",
            "iteration: 204200 loss: 0.0011 lr: 0.02\n",
            "iteration: 204210 loss: 0.0006 lr: 0.02\n",
            "iteration: 204220 loss: 0.0014 lr: 0.02\n",
            "iteration: 204230 loss: 0.0007 lr: 0.02\n",
            "iteration: 204240 loss: 0.0009 lr: 0.02\n",
            "iteration: 204250 loss: 0.0008 lr: 0.02\n",
            "iteration: 204260 loss: 0.0008 lr: 0.02\n",
            "iteration: 204270 loss: 0.0007 lr: 0.02\n",
            "iteration: 204280 loss: 0.0009 lr: 0.02\n",
            "iteration: 204290 loss: 0.0009 lr: 0.02\n",
            "iteration: 204300 loss: 0.0009 lr: 0.02\n",
            "iteration: 204310 loss: 0.0008 lr: 0.02\n",
            "iteration: 204320 loss: 0.0009 lr: 0.02\n",
            "iteration: 204330 loss: 0.0008 lr: 0.02\n",
            "iteration: 204340 loss: 0.0007 lr: 0.02\n",
            "iteration: 204350 loss: 0.0007 lr: 0.02\n",
            "iteration: 204360 loss: 0.0009 lr: 0.02\n",
            "iteration: 204370 loss: 0.0007 lr: 0.02\n",
            "iteration: 204380 loss: 0.0008 lr: 0.02\n",
            "iteration: 204390 loss: 0.0009 lr: 0.02\n",
            "iteration: 204400 loss: 0.0007 lr: 0.02\n",
            "iteration: 204410 loss: 0.0008 lr: 0.02\n",
            "iteration: 204420 loss: 0.0006 lr: 0.02\n",
            "iteration: 204430 loss: 0.0007 lr: 0.02\n",
            "iteration: 204440 loss: 0.0007 lr: 0.02\n",
            "iteration: 204450 loss: 0.0008 lr: 0.02\n",
            "iteration: 204460 loss: 0.0009 lr: 0.02\n",
            "iteration: 204470 loss: 0.0006 lr: 0.02\n",
            "iteration: 204480 loss: 0.0008 lr: 0.02\n",
            "iteration: 204490 loss: 0.0011 lr: 0.02\n",
            "iteration: 204500 loss: 0.0008 lr: 0.02\n",
            "iteration: 204510 loss: 0.0009 lr: 0.02\n",
            "iteration: 204520 loss: 0.0007 lr: 0.02\n",
            "iteration: 204530 loss: 0.0010 lr: 0.02\n",
            "iteration: 204540 loss: 0.0008 lr: 0.02\n",
            "iteration: 204550 loss: 0.0014 lr: 0.02\n",
            "iteration: 204560 loss: 0.0010 lr: 0.02\n",
            "iteration: 204570 loss: 0.0006 lr: 0.02\n",
            "iteration: 204580 loss: 0.0008 lr: 0.02\n",
            "iteration: 204590 loss: 0.0009 lr: 0.02\n",
            "iteration: 204600 loss: 0.0007 lr: 0.02\n",
            "iteration: 204610 loss: 0.0010 lr: 0.02\n",
            "iteration: 204620 loss: 0.0010 lr: 0.02\n",
            "iteration: 204630 loss: 0.0009 lr: 0.02\n",
            "iteration: 204640 loss: 0.0009 lr: 0.02\n",
            "iteration: 204650 loss: 0.0008 lr: 0.02\n",
            "iteration: 204660 loss: 0.0010 lr: 0.02\n",
            "iteration: 204670 loss: 0.0010 lr: 0.02\n",
            "iteration: 204680 loss: 0.0008 lr: 0.02\n",
            "iteration: 204690 loss: 0.0007 lr: 0.02\n",
            "iteration: 204700 loss: 0.0007 lr: 0.02\n",
            "iteration: 204710 loss: 0.0008 lr: 0.02\n",
            "iteration: 204720 loss: 0.0006 lr: 0.02\n",
            "iteration: 204730 loss: 0.0010 lr: 0.02\n",
            "iteration: 204740 loss: 0.0007 lr: 0.02\n",
            "iteration: 204750 loss: 0.0008 lr: 0.02\n",
            "iteration: 204760 loss: 0.0008 lr: 0.02\n",
            "iteration: 204770 loss: 0.0007 lr: 0.02\n",
            "iteration: 204780 loss: 0.0009 lr: 0.02\n",
            "iteration: 204790 loss: 0.0009 lr: 0.02\n",
            "iteration: 204800 loss: 0.0008 lr: 0.02\n",
            "iteration: 204810 loss: 0.0010 lr: 0.02\n",
            "iteration: 204820 loss: 0.0009 lr: 0.02\n",
            "iteration: 204830 loss: 0.0008 lr: 0.02\n",
            "iteration: 204840 loss: 0.0010 lr: 0.02\n",
            "iteration: 204850 loss: 0.0007 lr: 0.02\n",
            "iteration: 204860 loss: 0.0007 lr: 0.02\n",
            "iteration: 204870 loss: 0.0010 lr: 0.02\n",
            "iteration: 204880 loss: 0.0008 lr: 0.02\n",
            "iteration: 204890 loss: 0.0005 lr: 0.02\n",
            "iteration: 204900 loss: 0.0007 lr: 0.02\n",
            "iteration: 204910 loss: 0.0009 lr: 0.02\n",
            "iteration: 204920 loss: 0.0007 lr: 0.02\n",
            "iteration: 204930 loss: 0.0007 lr: 0.02\n",
            "iteration: 204940 loss: 0.0008 lr: 0.02\n",
            "iteration: 204950 loss: 0.0010 lr: 0.02\n",
            "iteration: 204960 loss: 0.0007 lr: 0.02\n",
            "iteration: 204970 loss: 0.0011 lr: 0.02\n",
            "iteration: 204980 loss: 0.0008 lr: 0.02\n",
            "iteration: 204990 loss: 0.0007 lr: 0.02\n",
            "iteration: 205000 loss: 0.0010 lr: 0.02\n",
            "iteration: 205010 loss: 0.0009 lr: 0.02\n",
            "iteration: 205020 loss: 0.0009 lr: 0.02\n",
            "iteration: 205030 loss: 0.0006 lr: 0.02\n",
            "iteration: 205040 loss: 0.0007 lr: 0.02\n",
            "iteration: 205050 loss: 0.0007 lr: 0.02\n",
            "iteration: 205060 loss: 0.0009 lr: 0.02\n",
            "iteration: 205070 loss: 0.0010 lr: 0.02\n",
            "iteration: 205080 loss: 0.0005 lr: 0.02\n",
            "iteration: 205090 loss: 0.0007 lr: 0.02\n",
            "iteration: 205100 loss: 0.0007 lr: 0.02\n",
            "iteration: 205110 loss: 0.0007 lr: 0.02\n",
            "iteration: 205120 loss: 0.0006 lr: 0.02\n",
            "iteration: 205130 loss: 0.0008 lr: 0.02\n",
            "iteration: 205140 loss: 0.0010 lr: 0.02\n",
            "iteration: 205150 loss: 0.0006 lr: 0.02\n",
            "iteration: 205160 loss: 0.0006 lr: 0.02\n",
            "iteration: 205170 loss: 0.0009 lr: 0.02\n",
            "iteration: 205180 loss: 0.0009 lr: 0.02\n",
            "iteration: 205190 loss: 0.0009 lr: 0.02\n",
            "iteration: 205200 loss: 0.0008 lr: 0.02\n",
            "iteration: 205210 loss: 0.0006 lr: 0.02\n",
            "iteration: 205220 loss: 0.0008 lr: 0.02\n",
            "iteration: 205230 loss: 0.0007 lr: 0.02\n",
            "iteration: 205240 loss: 0.0010 lr: 0.02\n",
            "iteration: 205250 loss: 0.0008 lr: 0.02\n",
            "iteration: 205260 loss: 0.0010 lr: 0.02\n",
            "iteration: 205270 loss: 0.0008 lr: 0.02\n",
            "iteration: 205280 loss: 0.0012 lr: 0.02\n",
            "iteration: 205290 loss: 0.0009 lr: 0.02\n",
            "iteration: 205300 loss: 0.0013 lr: 0.02\n",
            "iteration: 205310 loss: 0.0011 lr: 0.02\n",
            "iteration: 205320 loss: 0.0008 lr: 0.02\n",
            "iteration: 205330 loss: 0.0011 lr: 0.02\n",
            "iteration: 205340 loss: 0.0008 lr: 0.02\n",
            "iteration: 205350 loss: 0.0009 lr: 0.02\n",
            "iteration: 205360 loss: 0.0006 lr: 0.02\n",
            "iteration: 205370 loss: 0.0006 lr: 0.02\n",
            "iteration: 205380 loss: 0.0006 lr: 0.02\n",
            "iteration: 205390 loss: 0.0008 lr: 0.02\n",
            "iteration: 205400 loss: 0.0007 lr: 0.02\n",
            "iteration: 205410 loss: 0.0014 lr: 0.02\n",
            "iteration: 205420 loss: 0.0012 lr: 0.02\n",
            "iteration: 205430 loss: 0.0009 lr: 0.02\n",
            "iteration: 205440 loss: 0.0010 lr: 0.02\n",
            "iteration: 205450 loss: 0.0008 lr: 0.02\n",
            "iteration: 205460 loss: 0.0009 lr: 0.02\n",
            "iteration: 205470 loss: 0.0008 lr: 0.02\n",
            "iteration: 205480 loss: 0.0008 lr: 0.02\n",
            "iteration: 205490 loss: 0.0007 lr: 0.02\n",
            "iteration: 205500 loss: 0.0008 lr: 0.02\n",
            "iteration: 205510 loss: 0.0007 lr: 0.02\n",
            "iteration: 205520 loss: 0.0008 lr: 0.02\n",
            "iteration: 205530 loss: 0.0007 lr: 0.02\n",
            "iteration: 205540 loss: 0.0006 lr: 0.02\n",
            "iteration: 205550 loss: 0.0008 lr: 0.02\n",
            "iteration: 205560 loss: 0.0009 lr: 0.02\n",
            "iteration: 205570 loss: 0.0007 lr: 0.02\n",
            "iteration: 205580 loss: 0.0007 lr: 0.02\n",
            "iteration: 205590 loss: 0.0010 lr: 0.02\n",
            "iteration: 205600 loss: 0.0008 lr: 0.02\n",
            "iteration: 205610 loss: 0.0009 lr: 0.02\n",
            "iteration: 205620 loss: 0.0012 lr: 0.02\n",
            "iteration: 205630 loss: 0.0007 lr: 0.02\n",
            "iteration: 205640 loss: 0.0010 lr: 0.02\n",
            "iteration: 205650 loss: 0.0009 lr: 0.02\n",
            "iteration: 205660 loss: 0.0008 lr: 0.02\n",
            "iteration: 205670 loss: 0.0008 lr: 0.02\n",
            "iteration: 205680 loss: 0.0008 lr: 0.02\n",
            "iteration: 205690 loss: 0.0007 lr: 0.02\n",
            "iteration: 205700 loss: 0.0009 lr: 0.02\n",
            "iteration: 205710 loss: 0.0007 lr: 0.02\n",
            "iteration: 205720 loss: 0.0006 lr: 0.02\n",
            "iteration: 205730 loss: 0.0012 lr: 0.02\n",
            "iteration: 205740 loss: 0.0007 lr: 0.02\n",
            "iteration: 205750 loss: 0.0010 lr: 0.02\n",
            "iteration: 205760 loss: 0.0007 lr: 0.02\n",
            "iteration: 205770 loss: 0.0006 lr: 0.02\n",
            "iteration: 205780 loss: 0.0012 lr: 0.02\n",
            "iteration: 205790 loss: 0.0008 lr: 0.02\n",
            "iteration: 205800 loss: 0.0008 lr: 0.02\n",
            "iteration: 205810 loss: 0.0008 lr: 0.02\n",
            "iteration: 205820 loss: 0.0009 lr: 0.02\n",
            "iteration: 205830 loss: 0.0010 lr: 0.02\n",
            "iteration: 205840 loss: 0.0008 lr: 0.02\n",
            "iteration: 205850 loss: 0.0008 lr: 0.02\n",
            "iteration: 205860 loss: 0.0010 lr: 0.02\n",
            "iteration: 205870 loss: 0.0008 lr: 0.02\n",
            "iteration: 205880 loss: 0.0008 lr: 0.02\n",
            "iteration: 205890 loss: 0.0009 lr: 0.02\n",
            "iteration: 205900 loss: 0.0007 lr: 0.02\n",
            "iteration: 205910 loss: 0.0006 lr: 0.02\n",
            "iteration: 205920 loss: 0.0009 lr: 0.02\n",
            "iteration: 205930 loss: 0.0007 lr: 0.02\n",
            "iteration: 205940 loss: 0.0007 lr: 0.02\n",
            "iteration: 205950 loss: 0.0005 lr: 0.02\n",
            "iteration: 205960 loss: 0.0006 lr: 0.02\n",
            "iteration: 205970 loss: 0.0012 lr: 0.02\n",
            "iteration: 205980 loss: 0.0007 lr: 0.02\n",
            "iteration: 205990 loss: 0.0011 lr: 0.02\n",
            "iteration: 206000 loss: 0.0006 lr: 0.02\n",
            "iteration: 206010 loss: 0.0010 lr: 0.02\n",
            "iteration: 206020 loss: 0.0008 lr: 0.02\n",
            "iteration: 206030 loss: 0.0007 lr: 0.02\n",
            "iteration: 206040 loss: 0.0008 lr: 0.02\n",
            "iteration: 206050 loss: 0.0008 lr: 0.02\n",
            "iteration: 206060 loss: 0.0007 lr: 0.02\n",
            "iteration: 206070 loss: 0.0011 lr: 0.02\n",
            "iteration: 206080 loss: 0.0007 lr: 0.02\n",
            "iteration: 206090 loss: 0.0007 lr: 0.02\n",
            "iteration: 206100 loss: 0.0006 lr: 0.02\n",
            "iteration: 206110 loss: 0.0006 lr: 0.02\n",
            "iteration: 206120 loss: 0.0009 lr: 0.02\n",
            "iteration: 206130 loss: 0.0007 lr: 0.02\n",
            "iteration: 206140 loss: 0.0011 lr: 0.02\n",
            "iteration: 206150 loss: 0.0005 lr: 0.02\n",
            "iteration: 206160 loss: 0.0008 lr: 0.02\n",
            "iteration: 206170 loss: 0.0008 lr: 0.02\n",
            "iteration: 206180 loss: 0.0010 lr: 0.02\n",
            "iteration: 206190 loss: 0.0009 lr: 0.02\n",
            "iteration: 206200 loss: 0.0008 lr: 0.02\n",
            "iteration: 206210 loss: 0.0008 lr: 0.02\n",
            "iteration: 206220 loss: 0.0008 lr: 0.02\n",
            "iteration: 206230 loss: 0.0006 lr: 0.02\n",
            "iteration: 206240 loss: 0.0008 lr: 0.02\n",
            "iteration: 206250 loss: 0.0009 lr: 0.02\n",
            "iteration: 206260 loss: 0.0008 lr: 0.02\n",
            "iteration: 206270 loss: 0.0007 lr: 0.02\n",
            "iteration: 206280 loss: 0.0010 lr: 0.02\n",
            "iteration: 206290 loss: 0.0009 lr: 0.02\n",
            "iteration: 206300 loss: 0.0006 lr: 0.02\n",
            "iteration: 206310 loss: 0.0009 lr: 0.02\n",
            "iteration: 206320 loss: 0.0008 lr: 0.02\n",
            "iteration: 206330 loss: 0.0010 lr: 0.02\n",
            "iteration: 206340 loss: 0.0009 lr: 0.02\n",
            "iteration: 206350 loss: 0.0008 lr: 0.02\n",
            "iteration: 206360 loss: 0.0009 lr: 0.02\n",
            "iteration: 206370 loss: 0.0008 lr: 0.02\n",
            "iteration: 206380 loss: 0.0005 lr: 0.02\n",
            "iteration: 206390 loss: 0.0010 lr: 0.02\n",
            "iteration: 206400 loss: 0.0007 lr: 0.02\n",
            "iteration: 206410 loss: 0.0006 lr: 0.02\n",
            "iteration: 206420 loss: 0.0008 lr: 0.02\n",
            "iteration: 206430 loss: 0.0007 lr: 0.02\n",
            "iteration: 206440 loss: 0.0006 lr: 0.02\n",
            "iteration: 206450 loss: 0.0009 lr: 0.02\n",
            "iteration: 206460 loss: 0.0010 lr: 0.02\n",
            "iteration: 206470 loss: 0.0008 lr: 0.02\n",
            "iteration: 206480 loss: 0.0007 lr: 0.02\n",
            "iteration: 206490 loss: 0.0006 lr: 0.02\n",
            "iteration: 206500 loss: 0.0009 lr: 0.02\n",
            "iteration: 206510 loss: 0.0008 lr: 0.02\n",
            "iteration: 206520 loss: 0.0007 lr: 0.02\n",
            "iteration: 206530 loss: 0.0007 lr: 0.02\n",
            "iteration: 206540 loss: 0.0010 lr: 0.02\n",
            "iteration: 206550 loss: 0.0008 lr: 0.02\n",
            "iteration: 206560 loss: 0.0008 lr: 0.02\n",
            "iteration: 206570 loss: 0.0009 lr: 0.02\n",
            "iteration: 206580 loss: 0.0005 lr: 0.02\n",
            "iteration: 206590 loss: 0.0011 lr: 0.02\n",
            "iteration: 206600 loss: 0.0006 lr: 0.02\n",
            "iteration: 206610 loss: 0.0007 lr: 0.02\n",
            "iteration: 206620 loss: 0.0009 lr: 0.02\n",
            "iteration: 206630 loss: 0.0005 lr: 0.02\n",
            "iteration: 206640 loss: 0.0008 lr: 0.02\n",
            "iteration: 206650 loss: 0.0007 lr: 0.02\n",
            "iteration: 206660 loss: 0.0006 lr: 0.02\n",
            "iteration: 206670 loss: 0.0007 lr: 0.02\n",
            "iteration: 206680 loss: 0.0009 lr: 0.02\n",
            "iteration: 206690 loss: 0.0008 lr: 0.02\n",
            "iteration: 206700 loss: 0.0008 lr: 0.02\n",
            "iteration: 206710 loss: 0.0007 lr: 0.02\n",
            "iteration: 206720 loss: 0.0008 lr: 0.02\n",
            "iteration: 206730 loss: 0.0008 lr: 0.02\n",
            "iteration: 206740 loss: 0.0007 lr: 0.02\n",
            "iteration: 206750 loss: 0.0005 lr: 0.02\n",
            "iteration: 206760 loss: 0.0007 lr: 0.02\n",
            "iteration: 206770 loss: 0.0009 lr: 0.02\n",
            "iteration: 206780 loss: 0.0007 lr: 0.02\n",
            "iteration: 206790 loss: 0.0006 lr: 0.02\n",
            "iteration: 206800 loss: 0.0008 lr: 0.02\n",
            "iteration: 206810 loss: 0.0006 lr: 0.02\n",
            "iteration: 206820 loss: 0.0007 lr: 0.02\n",
            "iteration: 206830 loss: 0.0015 lr: 0.02\n",
            "iteration: 206840 loss: 0.0008 lr: 0.02\n",
            "iteration: 206850 loss: 0.0007 lr: 0.02\n",
            "iteration: 206860 loss: 0.0009 lr: 0.02\n",
            "iteration: 206870 loss: 0.0010 lr: 0.02\n",
            "iteration: 206880 loss: 0.0009 lr: 0.02\n",
            "iteration: 206890 loss: 0.0008 lr: 0.02\n",
            "iteration: 206900 loss: 0.0010 lr: 0.02\n",
            "iteration: 206910 loss: 0.0008 lr: 0.02\n",
            "iteration: 206920 loss: 0.0008 lr: 0.02\n",
            "iteration: 206930 loss: 0.0010 lr: 0.02\n",
            "iteration: 206940 loss: 0.0011 lr: 0.02\n",
            "iteration: 206950 loss: 0.0011 lr: 0.02\n",
            "iteration: 206960 loss: 0.0007 lr: 0.02\n",
            "iteration: 206970 loss: 0.0008 lr: 0.02\n",
            "iteration: 206980 loss: 0.0008 lr: 0.02\n",
            "iteration: 206990 loss: 0.0009 lr: 0.02\n",
            "iteration: 207000 loss: 0.0010 lr: 0.02\n",
            "iteration: 207010 loss: 0.0010 lr: 0.02\n",
            "iteration: 207020 loss: 0.0008 lr: 0.02\n",
            "iteration: 207030 loss: 0.0008 lr: 0.02\n",
            "iteration: 207040 loss: 0.0010 lr: 0.02\n",
            "iteration: 207050 loss: 0.0009 lr: 0.02\n",
            "iteration: 207060 loss: 0.0008 lr: 0.02\n",
            "iteration: 207070 loss: 0.0007 lr: 0.02\n",
            "iteration: 207080 loss: 0.0012 lr: 0.02\n",
            "iteration: 207090 loss: 0.0016 lr: 0.02\n",
            "iteration: 207100 loss: 0.0008 lr: 0.02\n",
            "iteration: 207110 loss: 0.0008 lr: 0.02\n",
            "iteration: 207120 loss: 0.0007 lr: 0.02\n",
            "iteration: 207130 loss: 0.0009 lr: 0.02\n",
            "iteration: 207140 loss: 0.0007 lr: 0.02\n",
            "iteration: 207150 loss: 0.0009 lr: 0.02\n",
            "iteration: 207160 loss: 0.0009 lr: 0.02\n",
            "iteration: 207170 loss: 0.0009 lr: 0.02\n",
            "iteration: 207180 loss: 0.0009 lr: 0.02\n",
            "iteration: 207190 loss: 0.0008 lr: 0.02\n",
            "iteration: 207200 loss: 0.0008 lr: 0.02\n",
            "iteration: 207210 loss: 0.0007 lr: 0.02\n",
            "iteration: 207220 loss: 0.0011 lr: 0.02\n",
            "iteration: 207230 loss: 0.0009 lr: 0.02\n",
            "iteration: 207240 loss: 0.0007 lr: 0.02\n",
            "iteration: 207250 loss: 0.0010 lr: 0.02\n",
            "iteration: 207260 loss: 0.0011 lr: 0.02\n",
            "iteration: 207270 loss: 0.0008 lr: 0.02\n",
            "iteration: 207280 loss: 0.0009 lr: 0.02\n",
            "iteration: 207290 loss: 0.0008 lr: 0.02\n",
            "iteration: 207300 loss: 0.0008 lr: 0.02\n",
            "iteration: 207310 loss: 0.0007 lr: 0.02\n",
            "iteration: 207320 loss: 0.0007 lr: 0.02\n",
            "iteration: 207330 loss: 0.0008 lr: 0.02\n",
            "iteration: 207340 loss: 0.0008 lr: 0.02\n",
            "iteration: 207350 loss: 0.0005 lr: 0.02\n",
            "iteration: 207360 loss: 0.0008 lr: 0.02\n",
            "iteration: 207370 loss: 0.0009 lr: 0.02\n",
            "iteration: 207380 loss: 0.0007 lr: 0.02\n",
            "iteration: 207390 loss: 0.0008 lr: 0.02\n",
            "iteration: 207400 loss: 0.0006 lr: 0.02\n",
            "iteration: 207410 loss: 0.0008 lr: 0.02\n",
            "iteration: 207420 loss: 0.0009 lr: 0.02\n",
            "iteration: 207430 loss: 0.0008 lr: 0.02\n",
            "iteration: 207440 loss: 0.0009 lr: 0.02\n",
            "iteration: 207450 loss: 0.0007 lr: 0.02\n",
            "iteration: 207460 loss: 0.0006 lr: 0.02\n",
            "iteration: 207470 loss: 0.0011 lr: 0.02\n",
            "iteration: 207480 loss: 0.0007 lr: 0.02\n",
            "iteration: 207490 loss: 0.0008 lr: 0.02\n",
            "iteration: 207500 loss: 0.0009 lr: 0.02\n",
            "iteration: 207510 loss: 0.0008 lr: 0.02\n",
            "iteration: 207520 loss: 0.0007 lr: 0.02\n",
            "iteration: 207530 loss: 0.0005 lr: 0.02\n",
            "iteration: 207540 loss: 0.0006 lr: 0.02\n",
            "iteration: 207550 loss: 0.0009 lr: 0.02\n",
            "iteration: 207560 loss: 0.0005 lr: 0.02\n",
            "iteration: 207570 loss: 0.0008 lr: 0.02\n",
            "iteration: 207580 loss: 0.0009 lr: 0.02\n",
            "iteration: 207590 loss: 0.0009 lr: 0.02\n",
            "iteration: 207600 loss: 0.0009 lr: 0.02\n",
            "iteration: 207610 loss: 0.0007 lr: 0.02\n",
            "iteration: 207620 loss: 0.0006 lr: 0.02\n",
            "iteration: 207630 loss: 0.0011 lr: 0.02\n",
            "iteration: 207640 loss: 0.0007 lr: 0.02\n",
            "iteration: 207650 loss: 0.0007 lr: 0.02\n",
            "iteration: 207660 loss: 0.0009 lr: 0.02\n",
            "iteration: 207670 loss: 0.0010 lr: 0.02\n",
            "iteration: 207680 loss: 0.0008 lr: 0.02\n",
            "iteration: 207690 loss: 0.0007 lr: 0.02\n",
            "iteration: 207700 loss: 0.0005 lr: 0.02\n",
            "iteration: 207710 loss: 0.0009 lr: 0.02\n",
            "iteration: 207720 loss: 0.0008 lr: 0.02\n",
            "iteration: 207730 loss: 0.0007 lr: 0.02\n",
            "iteration: 207740 loss: 0.0007 lr: 0.02\n",
            "iteration: 207750 loss: 0.0007 lr: 0.02\n",
            "iteration: 207760 loss: 0.0005 lr: 0.02\n",
            "iteration: 207770 loss: 0.0010 lr: 0.02\n",
            "iteration: 207780 loss: 0.0008 lr: 0.02\n",
            "iteration: 207790 loss: 0.0005 lr: 0.02\n",
            "iteration: 207800 loss: 0.0010 lr: 0.02\n",
            "iteration: 207810 loss: 0.0010 lr: 0.02\n",
            "iteration: 207820 loss: 0.0008 lr: 0.02\n",
            "iteration: 207830 loss: 0.0006 lr: 0.02\n",
            "iteration: 207840 loss: 0.0005 lr: 0.02\n",
            "iteration: 207850 loss: 0.0006 lr: 0.02\n",
            "iteration: 207860 loss: 0.0009 lr: 0.02\n",
            "iteration: 207870 loss: 0.0008 lr: 0.02\n",
            "iteration: 207880 loss: 0.0007 lr: 0.02\n",
            "iteration: 207890 loss: 0.0008 lr: 0.02\n",
            "iteration: 207900 loss: 0.0008 lr: 0.02\n",
            "iteration: 207910 loss: 0.0008 lr: 0.02\n",
            "iteration: 207920 loss: 0.0010 lr: 0.02\n",
            "iteration: 207930 loss: 0.0008 lr: 0.02\n",
            "iteration: 207940 loss: 0.0008 lr: 0.02\n",
            "iteration: 207950 loss: 0.0008 lr: 0.02\n",
            "iteration: 207960 loss: 0.0007 lr: 0.02\n",
            "iteration: 207970 loss: 0.0010 lr: 0.02\n",
            "iteration: 207980 loss: 0.0007 lr: 0.02\n",
            "iteration: 207990 loss: 0.0009 lr: 0.02\n",
            "iteration: 208000 loss: 0.0008 lr: 0.02\n",
            "iteration: 208010 loss: 0.0009 lr: 0.02\n",
            "iteration: 208020 loss: 0.0007 lr: 0.02\n",
            "iteration: 208030 loss: 0.0008 lr: 0.02\n",
            "iteration: 208040 loss: 0.0011 lr: 0.02\n",
            "iteration: 208050 loss: 0.0009 lr: 0.02\n",
            "iteration: 208060 loss: 0.0007 lr: 0.02\n",
            "iteration: 208070 loss: 0.0007 lr: 0.02\n",
            "iteration: 208080 loss: 0.0006 lr: 0.02\n",
            "iteration: 208090 loss: 0.0008 lr: 0.02\n",
            "iteration: 208100 loss: 0.0005 lr: 0.02\n",
            "iteration: 208110 loss: 0.0008 lr: 0.02\n",
            "iteration: 208120 loss: 0.0009 lr: 0.02\n",
            "iteration: 208130 loss: 0.0007 lr: 0.02\n",
            "iteration: 208140 loss: 0.0010 lr: 0.02\n",
            "iteration: 208150 loss: 0.0011 lr: 0.02\n",
            "iteration: 208160 loss: 0.0008 lr: 0.02\n",
            "iteration: 208170 loss: 0.0008 lr: 0.02\n",
            "iteration: 208180 loss: 0.0009 lr: 0.02\n",
            "iteration: 208190 loss: 0.0008 lr: 0.02\n",
            "iteration: 208200 loss: 0.0006 lr: 0.02\n",
            "iteration: 208210 loss: 0.0005 lr: 0.02\n",
            "iteration: 208220 loss: 0.0008 lr: 0.02\n",
            "iteration: 208230 loss: 0.0008 lr: 0.02\n",
            "iteration: 208240 loss: 0.0007 lr: 0.02\n",
            "iteration: 208250 loss: 0.0008 lr: 0.02\n",
            "iteration: 208260 loss: 0.0012 lr: 0.02\n",
            "iteration: 208270 loss: 0.0007 lr: 0.02\n",
            "iteration: 208280 loss: 0.0009 lr: 0.02\n",
            "iteration: 208290 loss: 0.0006 lr: 0.02\n",
            "iteration: 208300 loss: 0.0007 lr: 0.02\n",
            "iteration: 208310 loss: 0.0007 lr: 0.02\n",
            "iteration: 208320 loss: 0.0010 lr: 0.02\n",
            "iteration: 208330 loss: 0.0008 lr: 0.02\n",
            "iteration: 208340 loss: 0.0010 lr: 0.02\n",
            "iteration: 208350 loss: 0.0011 lr: 0.02\n",
            "iteration: 208360 loss: 0.0007 lr: 0.02\n",
            "iteration: 208370 loss: 0.0007 lr: 0.02\n",
            "iteration: 208380 loss: 0.0006 lr: 0.02\n",
            "iteration: 208390 loss: 0.0012 lr: 0.02\n",
            "iteration: 208400 loss: 0.0007 lr: 0.02\n",
            "iteration: 208410 loss: 0.0008 lr: 0.02\n",
            "iteration: 208420 loss: 0.0008 lr: 0.02\n",
            "iteration: 208430 loss: 0.0006 lr: 0.02\n",
            "iteration: 208440 loss: 0.0007 lr: 0.02\n",
            "iteration: 208450 loss: 0.0006 lr: 0.02\n",
            "iteration: 208460 loss: 0.0008 lr: 0.02\n",
            "iteration: 208470 loss: 0.0008 lr: 0.02\n",
            "iteration: 208480 loss: 0.0007 lr: 0.02\n",
            "iteration: 208490 loss: 0.0009 lr: 0.02\n",
            "iteration: 208500 loss: 0.0011 lr: 0.02\n",
            "iteration: 208510 loss: 0.0008 lr: 0.02\n",
            "iteration: 208520 loss: 0.0012 lr: 0.02\n",
            "iteration: 208530 loss: 0.0009 lr: 0.02\n",
            "iteration: 208540 loss: 0.0009 lr: 0.02\n",
            "iteration: 208550 loss: 0.0008 lr: 0.02\n",
            "iteration: 208560 loss: 0.0009 lr: 0.02\n",
            "iteration: 208570 loss: 0.0007 lr: 0.02\n",
            "iteration: 208580 loss: 0.0006 lr: 0.02\n",
            "iteration: 208590 loss: 0.0009 lr: 0.02\n",
            "iteration: 208600 loss: 0.0006 lr: 0.02\n",
            "iteration: 208610 loss: 0.0009 lr: 0.02\n",
            "iteration: 208620 loss: 0.0009 lr: 0.02\n",
            "iteration: 208630 loss: 0.0007 lr: 0.02\n",
            "iteration: 208640 loss: 0.0009 lr: 0.02\n",
            "iteration: 208650 loss: 0.0010 lr: 0.02\n",
            "iteration: 208660 loss: 0.0009 lr: 0.02\n",
            "iteration: 208670 loss: 0.0009 lr: 0.02\n",
            "iteration: 208680 loss: 0.0009 lr: 0.02\n",
            "iteration: 208690 loss: 0.0006 lr: 0.02\n",
            "iteration: 208700 loss: 0.0007 lr: 0.02\n",
            "iteration: 208710 loss: 0.0008 lr: 0.02\n",
            "iteration: 208720 loss: 0.0007 lr: 0.02\n",
            "iteration: 208730 loss: 0.0006 lr: 0.02\n",
            "iteration: 208740 loss: 0.0008 lr: 0.02\n",
            "iteration: 208750 loss: 0.0007 lr: 0.02\n",
            "iteration: 208760 loss: 0.0006 lr: 0.02\n",
            "iteration: 208770 loss: 0.0010 lr: 0.02\n",
            "iteration: 208780 loss: 0.0012 lr: 0.02\n",
            "iteration: 208790 loss: 0.0008 lr: 0.02\n",
            "iteration: 208800 loss: 0.0006 lr: 0.02\n",
            "iteration: 208810 loss: 0.0009 lr: 0.02\n",
            "iteration: 208820 loss: 0.0010 lr: 0.02\n",
            "iteration: 208830 loss: 0.0009 lr: 0.02\n",
            "iteration: 208840 loss: 0.0007 lr: 0.02\n",
            "iteration: 208850 loss: 0.0009 lr: 0.02\n",
            "iteration: 208860 loss: 0.0008 lr: 0.02\n",
            "iteration: 208870 loss: 0.0008 lr: 0.02\n",
            "iteration: 208880 loss: 0.0008 lr: 0.02\n",
            "iteration: 208890 loss: 0.0008 lr: 0.02\n",
            "iteration: 208900 loss: 0.0008 lr: 0.02\n",
            "iteration: 208910 loss: 0.0008 lr: 0.02\n",
            "iteration: 208920 loss: 0.0007 lr: 0.02\n",
            "iteration: 208930 loss: 0.0007 lr: 0.02\n",
            "iteration: 208940 loss: 0.0009 lr: 0.02\n",
            "iteration: 208950 loss: 0.0008 lr: 0.02\n",
            "iteration: 208960 loss: 0.0006 lr: 0.02\n",
            "iteration: 208970 loss: 0.0008 lr: 0.02\n",
            "iteration: 208980 loss: 0.0007 lr: 0.02\n",
            "iteration: 208990 loss: 0.0007 lr: 0.02\n",
            "iteration: 209000 loss: 0.0006 lr: 0.02\n",
            "iteration: 209010 loss: 0.0008 lr: 0.02\n",
            "iteration: 209020 loss: 0.0006 lr: 0.02\n",
            "iteration: 209030 loss: 0.0009 lr: 0.02\n",
            "iteration: 209040 loss: 0.0007 lr: 0.02\n",
            "iteration: 209050 loss: 0.0005 lr: 0.02\n",
            "iteration: 209060 loss: 0.0008 lr: 0.02\n",
            "iteration: 209070 loss: 0.0007 lr: 0.02\n",
            "iteration: 209080 loss: 0.0011 lr: 0.02\n",
            "iteration: 209090 loss: 0.0008 lr: 0.02\n",
            "iteration: 209100 loss: 0.0008 lr: 0.02\n",
            "iteration: 209110 loss: 0.0010 lr: 0.02\n",
            "iteration: 209120 loss: 0.0008 lr: 0.02\n",
            "iteration: 209130 loss: 0.0010 lr: 0.02\n",
            "iteration: 209140 loss: 0.0009 lr: 0.02\n",
            "iteration: 209150 loss: 0.0008 lr: 0.02\n",
            "iteration: 209160 loss: 0.0006 lr: 0.02\n",
            "iteration: 209170 loss: 0.0011 lr: 0.02\n",
            "iteration: 209180 loss: 0.0008 lr: 0.02\n",
            "iteration: 209190 loss: 0.0006 lr: 0.02\n",
            "iteration: 209200 loss: 0.0007 lr: 0.02\n",
            "iteration: 209210 loss: 0.0006 lr: 0.02\n",
            "iteration: 209220 loss: 0.0007 lr: 0.02\n",
            "iteration: 209230 loss: 0.0010 lr: 0.02\n",
            "iteration: 209240 loss: 0.0009 lr: 0.02\n",
            "iteration: 209250 loss: 0.0009 lr: 0.02\n",
            "iteration: 209260 loss: 0.0011 lr: 0.02\n",
            "iteration: 209270 loss: 0.0008 lr: 0.02\n",
            "iteration: 209280 loss: 0.0011 lr: 0.02\n",
            "iteration: 209290 loss: 0.0010 lr: 0.02\n",
            "iteration: 209300 loss: 0.0011 lr: 0.02\n",
            "iteration: 209310 loss: 0.0008 lr: 0.02\n",
            "iteration: 209320 loss: 0.0007 lr: 0.02\n",
            "iteration: 209330 loss: 0.0008 lr: 0.02\n",
            "iteration: 209340 loss: 0.0008 lr: 0.02\n",
            "iteration: 209350 loss: 0.0005 lr: 0.02\n",
            "iteration: 209360 loss: 0.0005 lr: 0.02\n",
            "iteration: 209370 loss: 0.0006 lr: 0.02\n",
            "iteration: 209380 loss: 0.0013 lr: 0.02\n",
            "iteration: 209390 loss: 0.0010 lr: 0.02\n",
            "iteration: 209400 loss: 0.0005 lr: 0.02\n",
            "iteration: 209410 loss: 0.0009 lr: 0.02\n",
            "iteration: 209420 loss: 0.0008 lr: 0.02\n",
            "iteration: 209430 loss: 0.0008 lr: 0.02\n",
            "iteration: 209440 loss: 0.0013 lr: 0.02\n",
            "iteration: 209450 loss: 0.0010 lr: 0.02\n",
            "iteration: 209460 loss: 0.0013 lr: 0.02\n",
            "iteration: 209470 loss: 0.0010 lr: 0.02\n",
            "iteration: 209480 loss: 0.0007 lr: 0.02\n",
            "iteration: 209490 loss: 0.0009 lr: 0.02\n",
            "iteration: 209500 loss: 0.0016 lr: 0.02\n",
            "iteration: 209510 loss: 0.0006 lr: 0.02\n",
            "iteration: 209520 loss: 0.0010 lr: 0.02\n",
            "iteration: 209530 loss: 0.0007 lr: 0.02\n",
            "iteration: 209540 loss: 0.0007 lr: 0.02\n",
            "iteration: 209550 loss: 0.0008 lr: 0.02\n",
            "iteration: 209560 loss: 0.0009 lr: 0.02\n",
            "iteration: 209570 loss: 0.0006 lr: 0.02\n",
            "iteration: 209580 loss: 0.0008 lr: 0.02\n",
            "iteration: 209590 loss: 0.0009 lr: 0.02\n",
            "iteration: 209600 loss: 0.0008 lr: 0.02\n",
            "iteration: 209610 loss: 0.0009 lr: 0.02\n",
            "iteration: 209620 loss: 0.0011 lr: 0.02\n",
            "iteration: 209630 loss: 0.0007 lr: 0.02\n",
            "iteration: 209640 loss: 0.0009 lr: 0.02\n",
            "iteration: 209650 loss: 0.0005 lr: 0.02\n",
            "iteration: 209660 loss: 0.0008 lr: 0.02\n",
            "iteration: 209670 loss: 0.0009 lr: 0.02\n",
            "iteration: 209680 loss: 0.0007 lr: 0.02\n",
            "iteration: 209690 loss: 0.0011 lr: 0.02\n",
            "iteration: 209700 loss: 0.0008 lr: 0.02\n",
            "iteration: 209710 loss: 0.0010 lr: 0.02\n",
            "iteration: 209720 loss: 0.0007 lr: 0.02\n",
            "iteration: 209730 loss: 0.0009 lr: 0.02\n",
            "iteration: 209740 loss: 0.0005 lr: 0.02\n",
            "iteration: 209750 loss: 0.0005 lr: 0.02\n",
            "iteration: 209760 loss: 0.0009 lr: 0.02\n",
            "iteration: 209770 loss: 0.0007 lr: 0.02\n",
            "iteration: 209780 loss: 0.0008 lr: 0.02\n",
            "iteration: 209790 loss: 0.0012 lr: 0.02\n",
            "iteration: 209800 loss: 0.0010 lr: 0.02\n",
            "iteration: 209810 loss: 0.0009 lr: 0.02\n",
            "iteration: 209820 loss: 0.0007 lr: 0.02\n",
            "iteration: 209830 loss: 0.0009 lr: 0.02\n",
            "iteration: 209840 loss: 0.0005 lr: 0.02\n",
            "iteration: 209850 loss: 0.0010 lr: 0.02\n",
            "iteration: 209860 loss: 0.0007 lr: 0.02\n",
            "iteration: 209870 loss: 0.0008 lr: 0.02\n",
            "iteration: 209880 loss: 0.0011 lr: 0.02\n",
            "iteration: 209890 loss: 0.0007 lr: 0.02\n",
            "iteration: 209900 loss: 0.0008 lr: 0.02\n",
            "iteration: 209910 loss: 0.0008 lr: 0.02\n",
            "iteration: 209920 loss: 0.0008 lr: 0.02\n",
            "iteration: 209930 loss: 0.0006 lr: 0.02\n",
            "iteration: 209940 loss: 0.0010 lr: 0.02\n",
            "iteration: 209950 loss: 0.0009 lr: 0.02\n",
            "iteration: 209960 loss: 0.0005 lr: 0.02\n",
            "iteration: 209970 loss: 0.0006 lr: 0.02\n",
            "iteration: 209980 loss: 0.0008 lr: 0.02\n",
            "iteration: 209990 loss: 0.0011 lr: 0.02\n",
            "iteration: 210000 loss: 0.0007 lr: 0.02\n",
            "iteration: 210010 loss: 0.0008 lr: 0.02\n",
            "iteration: 210020 loss: 0.0006 lr: 0.02\n",
            "iteration: 210030 loss: 0.0007 lr: 0.02\n",
            "iteration: 210040 loss: 0.0010 lr: 0.02\n",
            "iteration: 210050 loss: 0.0006 lr: 0.02\n",
            "iteration: 210060 loss: 0.0006 lr: 0.02\n",
            "iteration: 210070 loss: 0.0007 lr: 0.02\n",
            "iteration: 210080 loss: 0.0009 lr: 0.02\n",
            "iteration: 210090 loss: 0.0007 lr: 0.02\n",
            "iteration: 210100 loss: 0.0012 lr: 0.02\n",
            "iteration: 210110 loss: 0.0009 lr: 0.02\n",
            "iteration: 210120 loss: 0.0009 lr: 0.02\n",
            "iteration: 210130 loss: 0.0019 lr: 0.02\n",
            "iteration: 210140 loss: 0.0009 lr: 0.02\n",
            "iteration: 210150 loss: 0.0010 lr: 0.02\n",
            "iteration: 210160 loss: 0.0009 lr: 0.02\n",
            "iteration: 210170 loss: 0.0007 lr: 0.02\n",
            "iteration: 210180 loss: 0.0008 lr: 0.02\n",
            "iteration: 210190 loss: 0.0007 lr: 0.02\n",
            "iteration: 210200 loss: 0.0011 lr: 0.02\n",
            "iteration: 210210 loss: 0.0011 lr: 0.02\n",
            "iteration: 210220 loss: 0.0007 lr: 0.02\n",
            "iteration: 210230 loss: 0.0008 lr: 0.02\n",
            "iteration: 210240 loss: 0.0007 lr: 0.02\n",
            "iteration: 210250 loss: 0.0011 lr: 0.02\n",
            "iteration: 210260 loss: 0.0011 lr: 0.02\n",
            "iteration: 210270 loss: 0.0007 lr: 0.02\n",
            "iteration: 210280 loss: 0.0008 lr: 0.02\n",
            "iteration: 210290 loss: 0.0007 lr: 0.02\n",
            "iteration: 210300 loss: 0.0006 lr: 0.02\n",
            "iteration: 210310 loss: 0.0013 lr: 0.02\n",
            "iteration: 210320 loss: 0.0008 lr: 0.02\n",
            "iteration: 210330 loss: 0.0008 lr: 0.02\n",
            "iteration: 210340 loss: 0.0008 lr: 0.02\n",
            "iteration: 210350 loss: 0.0007 lr: 0.02\n",
            "iteration: 210360 loss: 0.0006 lr: 0.02\n",
            "iteration: 210370 loss: 0.0008 lr: 0.02\n",
            "iteration: 210380 loss: 0.0010 lr: 0.02\n",
            "iteration: 210390 loss: 0.0009 lr: 0.02\n",
            "iteration: 210400 loss: 0.0008 lr: 0.02\n",
            "iteration: 210410 loss: 0.0012 lr: 0.02\n",
            "iteration: 210420 loss: 0.0014 lr: 0.02\n",
            "iteration: 210430 loss: 0.0011 lr: 0.02\n",
            "iteration: 210440 loss: 0.0007 lr: 0.02\n",
            "iteration: 210450 loss: 0.0009 lr: 0.02\n",
            "iteration: 210460 loss: 0.0006 lr: 0.02\n",
            "iteration: 210470 loss: 0.0009 lr: 0.02\n",
            "iteration: 210480 loss: 0.0006 lr: 0.02\n",
            "iteration: 210490 loss: 0.0005 lr: 0.02\n",
            "iteration: 210500 loss: 0.0009 lr: 0.02\n",
            "iteration: 210510 loss: 0.0009 lr: 0.02\n",
            "iteration: 210520 loss: 0.0008 lr: 0.02\n",
            "iteration: 210530 loss: 0.0007 lr: 0.02\n",
            "iteration: 210540 loss: 0.0006 lr: 0.02\n",
            "iteration: 210550 loss: 0.0007 lr: 0.02\n",
            "iteration: 210560 loss: 0.0007 lr: 0.02\n",
            "iteration: 210570 loss: 0.0009 lr: 0.02\n",
            "iteration: 210580 loss: 0.0008 lr: 0.02\n",
            "iteration: 210590 loss: 0.0007 lr: 0.02\n",
            "iteration: 210600 loss: 0.0006 lr: 0.02\n",
            "iteration: 210610 loss: 0.0007 lr: 0.02\n",
            "iteration: 210620 loss: 0.0007 lr: 0.02\n",
            "iteration: 210630 loss: 0.0009 lr: 0.02\n",
            "iteration: 210640 loss: 0.0007 lr: 0.02\n",
            "iteration: 210650 loss: 0.0012 lr: 0.02\n",
            "iteration: 210660 loss: 0.0007 lr: 0.02\n",
            "iteration: 210670 loss: 0.0010 lr: 0.02\n",
            "iteration: 210680 loss: 0.0006 lr: 0.02\n",
            "iteration: 210690 loss: 0.0011 lr: 0.02\n",
            "iteration: 210700 loss: 0.0007 lr: 0.02\n",
            "iteration: 210710 loss: 0.0007 lr: 0.02\n",
            "iteration: 210720 loss: 0.0010 lr: 0.02\n",
            "iteration: 210730 loss: 0.0010 lr: 0.02\n",
            "iteration: 210740 loss: 0.0009 lr: 0.02\n",
            "iteration: 210750 loss: 0.0011 lr: 0.02\n",
            "iteration: 210760 loss: 0.0009 lr: 0.02\n",
            "iteration: 210770 loss: 0.0009 lr: 0.02\n",
            "iteration: 210780 loss: 0.0010 lr: 0.02\n",
            "iteration: 210790 loss: 0.0010 lr: 0.02\n",
            "iteration: 210800 loss: 0.0008 lr: 0.02\n",
            "iteration: 210810 loss: 0.0006 lr: 0.02\n",
            "iteration: 210820 loss: 0.0008 lr: 0.02\n",
            "iteration: 210830 loss: 0.0010 lr: 0.02\n",
            "iteration: 210840 loss: 0.0007 lr: 0.02\n",
            "iteration: 210850 loss: 0.0011 lr: 0.02\n",
            "iteration: 210860 loss: 0.0008 lr: 0.02\n",
            "iteration: 210870 loss: 0.0014 lr: 0.02\n",
            "iteration: 210880 loss: 0.0007 lr: 0.02\n",
            "iteration: 210890 loss: 0.0006 lr: 0.02\n",
            "iteration: 210900 loss: 0.0008 lr: 0.02\n",
            "iteration: 210910 loss: 0.0008 lr: 0.02\n",
            "iteration: 210920 loss: 0.0007 lr: 0.02\n",
            "iteration: 210930 loss: 0.0008 lr: 0.02\n",
            "iteration: 210940 loss: 0.0008 lr: 0.02\n",
            "iteration: 210950 loss: 0.0010 lr: 0.02\n",
            "iteration: 210960 loss: 0.0011 lr: 0.02\n",
            "iteration: 210970 loss: 0.0008 lr: 0.02\n",
            "iteration: 210980 loss: 0.0006 lr: 0.02\n",
            "iteration: 210990 loss: 0.0006 lr: 0.02\n",
            "iteration: 211000 loss: 0.0006 lr: 0.02\n",
            "iteration: 211010 loss: 0.0007 lr: 0.02\n",
            "iteration: 211020 loss: 0.0006 lr: 0.02\n",
            "iteration: 211030 loss: 0.0007 lr: 0.02\n",
            "iteration: 211040 loss: 0.0007 lr: 0.02\n",
            "iteration: 211050 loss: 0.0005 lr: 0.02\n",
            "iteration: 211060 loss: 0.0008 lr: 0.02\n",
            "iteration: 211070 loss: 0.0009 lr: 0.02\n",
            "iteration: 211080 loss: 0.0009 lr: 0.02\n",
            "iteration: 211090 loss: 0.0008 lr: 0.02\n",
            "iteration: 211100 loss: 0.0007 lr: 0.02\n",
            "iteration: 211110 loss: 0.0007 lr: 0.02\n",
            "iteration: 211120 loss: 0.0006 lr: 0.02\n",
            "iteration: 211130 loss: 0.0007 lr: 0.02\n",
            "iteration: 211140 loss: 0.0007 lr: 0.02\n",
            "iteration: 211150 loss: 0.0009 lr: 0.02\n",
            "iteration: 211160 loss: 0.0008 lr: 0.02\n",
            "iteration: 211170 loss: 0.0008 lr: 0.02\n",
            "iteration: 211180 loss: 0.0007 lr: 0.02\n",
            "iteration: 211190 loss: 0.0009 lr: 0.02\n",
            "iteration: 211200 loss: 0.0012 lr: 0.02\n",
            "iteration: 211210 loss: 0.0012 lr: 0.02\n",
            "iteration: 211220 loss: 0.0008 lr: 0.02\n",
            "iteration: 211230 loss: 0.0009 lr: 0.02\n",
            "iteration: 211240 loss: 0.0007 lr: 0.02\n",
            "iteration: 211250 loss: 0.0007 lr: 0.02\n",
            "iteration: 211260 loss: 0.0008 lr: 0.02\n",
            "iteration: 211270 loss: 0.0007 lr: 0.02\n",
            "iteration: 211280 loss: 0.0007 lr: 0.02\n",
            "iteration: 211290 loss: 0.0008 lr: 0.02\n",
            "iteration: 211300 loss: 0.0008 lr: 0.02\n",
            "iteration: 211310 loss: 0.0006 lr: 0.02\n",
            "iteration: 211320 loss: 0.0009 lr: 0.02\n",
            "iteration: 211330 loss: 0.0009 lr: 0.02\n",
            "iteration: 211340 loss: 0.0008 lr: 0.02\n",
            "iteration: 211350 loss: 0.0007 lr: 0.02\n",
            "iteration: 211360 loss: 0.0005 lr: 0.02\n",
            "iteration: 211370 loss: 0.0007 lr: 0.02\n",
            "iteration: 211380 loss: 0.0010 lr: 0.02\n",
            "iteration: 211390 loss: 0.0008 lr: 0.02\n",
            "iteration: 211400 loss: 0.0011 lr: 0.02\n",
            "iteration: 211410 loss: 0.0006 lr: 0.02\n",
            "iteration: 211420 loss: 0.0008 lr: 0.02\n",
            "iteration: 211430 loss: 0.0010 lr: 0.02\n",
            "iteration: 211440 loss: 0.0008 lr: 0.02\n",
            "iteration: 211450 loss: 0.0010 lr: 0.02\n",
            "iteration: 211460 loss: 0.0011 lr: 0.02\n",
            "iteration: 211470 loss: 0.0008 lr: 0.02\n",
            "iteration: 211480 loss: 0.0009 lr: 0.02\n",
            "iteration: 211490 loss: 0.0008 lr: 0.02\n",
            "iteration: 211500 loss: 0.0005 lr: 0.02\n",
            "iteration: 211510 loss: 0.0008 lr: 0.02\n",
            "iteration: 211520 loss: 0.0012 lr: 0.02\n",
            "iteration: 211530 loss: 0.0008 lr: 0.02\n",
            "iteration: 211540 loss: 0.0006 lr: 0.02\n",
            "iteration: 211550 loss: 0.0007 lr: 0.02\n",
            "iteration: 211560 loss: 0.0007 lr: 0.02\n",
            "iteration: 211570 loss: 0.0010 lr: 0.02\n",
            "iteration: 211580 loss: 0.0009 lr: 0.02\n",
            "iteration: 211590 loss: 0.0010 lr: 0.02\n",
            "iteration: 211600 loss: 0.0007 lr: 0.02\n",
            "iteration: 211610 loss: 0.0009 lr: 0.02\n",
            "iteration: 211620 loss: 0.0014 lr: 0.02\n",
            "iteration: 211630 loss: 0.0008 lr: 0.02\n",
            "iteration: 211640 loss: 0.0009 lr: 0.02\n",
            "iteration: 211650 loss: 0.0009 lr: 0.02\n",
            "iteration: 211660 loss: 0.0010 lr: 0.02\n",
            "iteration: 211670 loss: 0.0011 lr: 0.02\n",
            "iteration: 211680 loss: 0.0009 lr: 0.02\n",
            "iteration: 211690 loss: 0.0008 lr: 0.02\n",
            "iteration: 211700 loss: 0.0010 lr: 0.02\n",
            "iteration: 211710 loss: 0.0011 lr: 0.02\n",
            "iteration: 211720 loss: 0.0009 lr: 0.02\n",
            "iteration: 211730 loss: 0.0009 lr: 0.02\n",
            "iteration: 211740 loss: 0.0008 lr: 0.02\n",
            "iteration: 211750 loss: 0.0009 lr: 0.02\n",
            "iteration: 211760 loss: 0.0013 lr: 0.02\n",
            "iteration: 211770 loss: 0.0008 lr: 0.02\n",
            "iteration: 211780 loss: 0.0008 lr: 0.02\n",
            "iteration: 211790 loss: 0.0008 lr: 0.02\n",
            "iteration: 211800 loss: 0.0006 lr: 0.02\n",
            "iteration: 211810 loss: 0.0008 lr: 0.02\n",
            "iteration: 211820 loss: 0.0009 lr: 0.02\n",
            "iteration: 211830 loss: 0.0009 lr: 0.02\n",
            "iteration: 211840 loss: 0.0008 lr: 0.02\n",
            "iteration: 211850 loss: 0.0009 lr: 0.02\n",
            "iteration: 211860 loss: 0.0009 lr: 0.02\n",
            "iteration: 211870 loss: 0.0009 lr: 0.02\n",
            "iteration: 211880 loss: 0.0006 lr: 0.02\n",
            "iteration: 211890 loss: 0.0009 lr: 0.02\n",
            "iteration: 211900 loss: 0.0009 lr: 0.02\n",
            "iteration: 211910 loss: 0.0008 lr: 0.02\n",
            "iteration: 211920 loss: 0.0009 lr: 0.02\n",
            "iteration: 211930 loss: 0.0011 lr: 0.02\n",
            "iteration: 211940 loss: 0.0010 lr: 0.02\n",
            "iteration: 211950 loss: 0.0007 lr: 0.02\n",
            "iteration: 211960 loss: 0.0008 lr: 0.02\n",
            "iteration: 211970 loss: 0.0009 lr: 0.02\n",
            "iteration: 211980 loss: 0.0010 lr: 0.02\n",
            "iteration: 211990 loss: 0.0009 lr: 0.02\n",
            "iteration: 212000 loss: 0.0005 lr: 0.02\n",
            "iteration: 212010 loss: 0.0008 lr: 0.02\n",
            "iteration: 212020 loss: 0.0007 lr: 0.02\n",
            "iteration: 212030 loss: 0.0009 lr: 0.02\n",
            "iteration: 212040 loss: 0.0007 lr: 0.02\n",
            "iteration: 212050 loss: 0.0006 lr: 0.02\n",
            "iteration: 212060 loss: 0.0008 lr: 0.02\n",
            "iteration: 212070 loss: 0.0009 lr: 0.02\n",
            "iteration: 212080 loss: 0.0006 lr: 0.02\n",
            "iteration: 212090 loss: 0.0007 lr: 0.02\n",
            "iteration: 212100 loss: 0.0008 lr: 0.02\n",
            "iteration: 212110 loss: 0.0007 lr: 0.02\n",
            "iteration: 212120 loss: 0.0005 lr: 0.02\n",
            "iteration: 212130 loss: 0.0007 lr: 0.02\n",
            "iteration: 212140 loss: 0.0008 lr: 0.02\n",
            "iteration: 212150 loss: 0.0007 lr: 0.02\n",
            "iteration: 212160 loss: 0.0010 lr: 0.02\n",
            "iteration: 212170 loss: 0.0008 lr: 0.02\n",
            "iteration: 212180 loss: 0.0006 lr: 0.02\n",
            "iteration: 212190 loss: 0.0008 lr: 0.02\n",
            "iteration: 212200 loss: 0.0008 lr: 0.02\n",
            "iteration: 212210 loss: 0.0007 lr: 0.02\n",
            "iteration: 212220 loss: 0.0007 lr: 0.02\n",
            "iteration: 212230 loss: 0.0008 lr: 0.02\n",
            "iteration: 212240 loss: 0.0008 lr: 0.02\n",
            "iteration: 212250 loss: 0.0008 lr: 0.02\n",
            "iteration: 212260 loss: 0.0007 lr: 0.02\n",
            "iteration: 212270 loss: 0.0008 lr: 0.02\n",
            "iteration: 212280 loss: 0.0006 lr: 0.02\n",
            "iteration: 212290 loss: 0.0008 lr: 0.02\n",
            "iteration: 212300 loss: 0.0014 lr: 0.02\n",
            "iteration: 212310 loss: 0.0006 lr: 0.02\n",
            "iteration: 212320 loss: 0.0008 lr: 0.02\n",
            "iteration: 212330 loss: 0.0009 lr: 0.02\n",
            "iteration: 212340 loss: 0.0009 lr: 0.02\n",
            "iteration: 212350 loss: 0.0007 lr: 0.02\n",
            "iteration: 212360 loss: 0.0010 lr: 0.02\n",
            "iteration: 212370 loss: 0.0011 lr: 0.02\n",
            "iteration: 212380 loss: 0.0005 lr: 0.02\n",
            "iteration: 212390 loss: 0.0008 lr: 0.02\n",
            "iteration: 212400 loss: 0.0005 lr: 0.02\n",
            "iteration: 212410 loss: 0.0008 lr: 0.02\n",
            "iteration: 212420 loss: 0.0007 lr: 0.02\n",
            "iteration: 212430 loss: 0.0006 lr: 0.02\n",
            "iteration: 212440 loss: 0.0006 lr: 0.02\n",
            "iteration: 212450 loss: 0.0008 lr: 0.02\n",
            "iteration: 212460 loss: 0.0010 lr: 0.02\n",
            "iteration: 212470 loss: 0.0005 lr: 0.02\n",
            "iteration: 212480 loss: 0.0011 lr: 0.02\n",
            "iteration: 212490 loss: 0.0011 lr: 0.02\n",
            "iteration: 212500 loss: 0.0006 lr: 0.02\n",
            "iteration: 212510 loss: 0.0007 lr: 0.02\n",
            "iteration: 212520 loss: 0.0008 lr: 0.02\n",
            "iteration: 212530 loss: 0.0008 lr: 0.02\n",
            "iteration: 212540 loss: 0.0008 lr: 0.02\n",
            "iteration: 212550 loss: 0.0006 lr: 0.02\n",
            "iteration: 212560 loss: 0.0006 lr: 0.02\n",
            "iteration: 212570 loss: 0.0010 lr: 0.02\n",
            "iteration: 212580 loss: 0.0009 lr: 0.02\n",
            "iteration: 212590 loss: 0.0006 lr: 0.02\n",
            "iteration: 212600 loss: 0.0007 lr: 0.02\n",
            "iteration: 212610 loss: 0.0008 lr: 0.02\n",
            "iteration: 212620 loss: 0.0008 lr: 0.02\n",
            "iteration: 212630 loss: 0.0005 lr: 0.02\n",
            "iteration: 212640 loss: 0.0006 lr: 0.02\n",
            "iteration: 212650 loss: 0.0006 lr: 0.02\n",
            "iteration: 212660 loss: 0.0007 lr: 0.02\n",
            "iteration: 212670 loss: 0.0007 lr: 0.02\n",
            "iteration: 212680 loss: 0.0008 lr: 0.02\n",
            "iteration: 212690 loss: 0.0006 lr: 0.02\n",
            "iteration: 212700 loss: 0.0012 lr: 0.02\n",
            "iteration: 212710 loss: 0.0008 lr: 0.02\n",
            "iteration: 212720 loss: 0.0010 lr: 0.02\n",
            "iteration: 212730 loss: 0.0007 lr: 0.02\n",
            "iteration: 212740 loss: 0.0009 lr: 0.02\n",
            "iteration: 212750 loss: 0.0007 lr: 0.02\n",
            "iteration: 212760 loss: 0.0008 lr: 0.02\n",
            "iteration: 212770 loss: 0.0010 lr: 0.02\n",
            "iteration: 212780 loss: 0.0012 lr: 0.02\n",
            "iteration: 212790 loss: 0.0007 lr: 0.02\n",
            "iteration: 212800 loss: 0.0008 lr: 0.02\n",
            "iteration: 212810 loss: 0.0007 lr: 0.02\n",
            "iteration: 212820 loss: 0.0009 lr: 0.02\n",
            "iteration: 212830 loss: 0.0008 lr: 0.02\n",
            "iteration: 212840 loss: 0.0007 lr: 0.02\n",
            "iteration: 212850 loss: 0.0006 lr: 0.02\n",
            "iteration: 212860 loss: 0.0009 lr: 0.02\n",
            "iteration: 212870 loss: 0.0007 lr: 0.02\n",
            "iteration: 212880 loss: 0.0006 lr: 0.02\n",
            "iteration: 212890 loss: 0.0008 lr: 0.02\n",
            "iteration: 212900 loss: 0.0010 lr: 0.02\n",
            "iteration: 212910 loss: 0.0009 lr: 0.02\n",
            "iteration: 212920 loss: 0.0012 lr: 0.02\n",
            "iteration: 212930 loss: 0.0009 lr: 0.02\n",
            "iteration: 212940 loss: 0.0010 lr: 0.02\n",
            "iteration: 212950 loss: 0.0009 lr: 0.02\n",
            "iteration: 212960 loss: 0.0006 lr: 0.02\n",
            "iteration: 212970 loss: 0.0006 lr: 0.02\n",
            "iteration: 212980 loss: 0.0011 lr: 0.02\n",
            "iteration: 212990 loss: 0.0009 lr: 0.02\n",
            "iteration: 213000 loss: 0.0008 lr: 0.02\n",
            "iteration: 213010 loss: 0.0008 lr: 0.02\n",
            "iteration: 213020 loss: 0.0009 lr: 0.02\n",
            "iteration: 213030 loss: 0.0006 lr: 0.02\n",
            "iteration: 213040 loss: 0.0006 lr: 0.02\n",
            "iteration: 213050 loss: 0.0011 lr: 0.02\n",
            "iteration: 213060 loss: 0.0005 lr: 0.02\n",
            "iteration: 213070 loss: 0.0014 lr: 0.02\n",
            "iteration: 213080 loss: 0.0008 lr: 0.02\n",
            "iteration: 213090 loss: 0.0006 lr: 0.02\n",
            "iteration: 213100 loss: 0.0006 lr: 0.02\n",
            "iteration: 213110 loss: 0.0007 lr: 0.02\n",
            "iteration: 213120 loss: 0.0006 lr: 0.02\n",
            "iteration: 213130 loss: 0.0008 lr: 0.02\n",
            "iteration: 213140 loss: 0.0008 lr: 0.02\n",
            "iteration: 213150 loss: 0.0009 lr: 0.02\n",
            "iteration: 213160 loss: 0.0009 lr: 0.02\n",
            "iteration: 213170 loss: 0.0009 lr: 0.02\n",
            "iteration: 213180 loss: 0.0008 lr: 0.02\n",
            "iteration: 213190 loss: 0.0010 lr: 0.02\n",
            "iteration: 213200 loss: 0.0007 lr: 0.02\n",
            "iteration: 213210 loss: 0.0009 lr: 0.02\n",
            "iteration: 213220 loss: 0.0008 lr: 0.02\n",
            "iteration: 213230 loss: 0.0009 lr: 0.02\n",
            "iteration: 213240 loss: 0.0007 lr: 0.02\n",
            "iteration: 213250 loss: 0.0008 lr: 0.02\n",
            "iteration: 213260 loss: 0.0010 lr: 0.02\n",
            "iteration: 213270 loss: 0.0006 lr: 0.02\n",
            "iteration: 213280 loss: 0.0007 lr: 0.02\n",
            "iteration: 213290 loss: 0.0006 lr: 0.02\n",
            "iteration: 213300 loss: 0.0008 lr: 0.02\n",
            "iteration: 213310 loss: 0.0007 lr: 0.02\n",
            "iteration: 213320 loss: 0.0006 lr: 0.02\n",
            "iteration: 213330 loss: 0.0007 lr: 0.02\n",
            "iteration: 213340 loss: 0.0007 lr: 0.02\n",
            "iteration: 213350 loss: 0.0006 lr: 0.02\n",
            "iteration: 213360 loss: 0.0005 lr: 0.02\n",
            "iteration: 213370 loss: 0.0007 lr: 0.02\n",
            "iteration: 213380 loss: 0.0009 lr: 0.02\n",
            "iteration: 213390 loss: 0.0011 lr: 0.02\n",
            "iteration: 213400 loss: 0.0006 lr: 0.02\n",
            "iteration: 213410 loss: 0.0009 lr: 0.02\n",
            "iteration: 213420 loss: 0.0010 lr: 0.02\n",
            "iteration: 213430 loss: 0.0006 lr: 0.02\n",
            "iteration: 213440 loss: 0.0008 lr: 0.02\n",
            "iteration: 213450 loss: 0.0009 lr: 0.02\n",
            "iteration: 213460 loss: 0.0007 lr: 0.02\n",
            "iteration: 213470 loss: 0.0008 lr: 0.02\n",
            "iteration: 213480 loss: 0.0009 lr: 0.02\n",
            "iteration: 213490 loss: 0.0005 lr: 0.02\n",
            "iteration: 213500 loss: 0.0012 lr: 0.02\n",
            "iteration: 213510 loss: 0.0007 lr: 0.02\n",
            "iteration: 213520 loss: 0.0005 lr: 0.02\n",
            "iteration: 213530 loss: 0.0005 lr: 0.02\n",
            "iteration: 213540 loss: 0.0009 lr: 0.02\n",
            "iteration: 213550 loss: 0.0008 lr: 0.02\n",
            "iteration: 213560 loss: 0.0007 lr: 0.02\n",
            "iteration: 213570 loss: 0.0006 lr: 0.02\n",
            "iteration: 213580 loss: 0.0008 lr: 0.02\n",
            "iteration: 213590 loss: 0.0008 lr: 0.02\n",
            "iteration: 213600 loss: 0.0009 lr: 0.02\n",
            "iteration: 213610 loss: 0.0008 lr: 0.02\n",
            "iteration: 213620 loss: 0.0008 lr: 0.02\n",
            "iteration: 213630 loss: 0.0008 lr: 0.02\n",
            "iteration: 213640 loss: 0.0009 lr: 0.02\n",
            "iteration: 213650 loss: 0.0007 lr: 0.02\n",
            "iteration: 213660 loss: 0.0008 lr: 0.02\n",
            "iteration: 213670 loss: 0.0007 lr: 0.02\n",
            "iteration: 213680 loss: 0.0008 lr: 0.02\n",
            "iteration: 213690 loss: 0.0006 lr: 0.02\n",
            "iteration: 213700 loss: 0.0009 lr: 0.02\n",
            "iteration: 213710 loss: 0.0007 lr: 0.02\n",
            "iteration: 213720 loss: 0.0011 lr: 0.02\n",
            "iteration: 213730 loss: 0.0012 lr: 0.02\n",
            "iteration: 213740 loss: 0.0008 lr: 0.02\n",
            "iteration: 213750 loss: 0.0009 lr: 0.02\n",
            "iteration: 213760 loss: 0.0007 lr: 0.02\n",
            "iteration: 213770 loss: 0.0008 lr: 0.02\n",
            "iteration: 213780 loss: 0.0007 lr: 0.02\n",
            "iteration: 213790 loss: 0.0011 lr: 0.02\n",
            "iteration: 213800 loss: 0.0007 lr: 0.02\n",
            "iteration: 213810 loss: 0.0010 lr: 0.02\n",
            "iteration: 213820 loss: 0.0006 lr: 0.02\n",
            "iteration: 213830 loss: 0.0009 lr: 0.02\n",
            "iteration: 213840 loss: 0.0010 lr: 0.02\n",
            "iteration: 213850 loss: 0.0006 lr: 0.02\n",
            "iteration: 213860 loss: 0.0008 lr: 0.02\n",
            "iteration: 213870 loss: 0.0011 lr: 0.02\n",
            "iteration: 213880 loss: 0.0009 lr: 0.02\n",
            "iteration: 213890 loss: 0.0008 lr: 0.02\n",
            "iteration: 213900 loss: 0.0007 lr: 0.02\n",
            "iteration: 213910 loss: 0.0009 lr: 0.02\n",
            "iteration: 213920 loss: 0.0006 lr: 0.02\n",
            "iteration: 213930 loss: 0.0007 lr: 0.02\n",
            "iteration: 213940 loss: 0.0012 lr: 0.02\n",
            "iteration: 213950 loss: 0.0009 lr: 0.02\n",
            "iteration: 213960 loss: 0.0008 lr: 0.02\n",
            "iteration: 213970 loss: 0.0006 lr: 0.02\n",
            "iteration: 213980 loss: 0.0009 lr: 0.02\n",
            "iteration: 213990 loss: 0.0007 lr: 0.02\n",
            "iteration: 214000 loss: 0.0008 lr: 0.02\n",
            "iteration: 214010 loss: 0.0008 lr: 0.02\n",
            "iteration: 214020 loss: 0.0009 lr: 0.02\n",
            "iteration: 214030 loss: 0.0007 lr: 0.02\n",
            "iteration: 214040 loss: 0.0011 lr: 0.02\n",
            "iteration: 214050 loss: 0.0009 lr: 0.02\n",
            "iteration: 214060 loss: 0.0007 lr: 0.02\n",
            "iteration: 214070 loss: 0.0008 lr: 0.02\n",
            "iteration: 214080 loss: 0.0005 lr: 0.02\n",
            "iteration: 214090 loss: 0.0009 lr: 0.02\n",
            "iteration: 214100 loss: 0.0006 lr: 0.02\n",
            "iteration: 214110 loss: 0.0007 lr: 0.02\n",
            "iteration: 214120 loss: 0.0009 lr: 0.02\n",
            "iteration: 214130 loss: 0.0007 lr: 0.02\n",
            "iteration: 214140 loss: 0.0006 lr: 0.02\n",
            "iteration: 214150 loss: 0.0006 lr: 0.02\n",
            "iteration: 214160 loss: 0.0006 lr: 0.02\n",
            "iteration: 214170 loss: 0.0006 lr: 0.02\n",
            "iteration: 214180 loss: 0.0008 lr: 0.02\n",
            "iteration: 214190 loss: 0.0007 lr: 0.02\n",
            "iteration: 214200 loss: 0.0009 lr: 0.02\n",
            "iteration: 214210 loss: 0.0007 lr: 0.02\n",
            "iteration: 214220 loss: 0.0010 lr: 0.02\n",
            "iteration: 214230 loss: 0.0006 lr: 0.02\n",
            "iteration: 214240 loss: 0.0008 lr: 0.02\n",
            "iteration: 214250 loss: 0.0008 lr: 0.02\n",
            "iteration: 214260 loss: 0.0008 lr: 0.02\n",
            "iteration: 214270 loss: 0.0007 lr: 0.02\n",
            "iteration: 214280 loss: 0.0006 lr: 0.02\n",
            "iteration: 214290 loss: 0.0012 lr: 0.02\n",
            "iteration: 214300 loss: 0.0006 lr: 0.02\n",
            "iteration: 214310 loss: 0.0007 lr: 0.02\n",
            "iteration: 214320 loss: 0.0007 lr: 0.02\n",
            "iteration: 214330 loss: 0.0009 lr: 0.02\n",
            "iteration: 214340 loss: 0.0013 lr: 0.02\n",
            "iteration: 214350 loss: 0.0006 lr: 0.02\n",
            "iteration: 214360 loss: 0.0006 lr: 0.02\n",
            "iteration: 214370 loss: 0.0008 lr: 0.02\n",
            "iteration: 214380 loss: 0.0008 lr: 0.02\n",
            "iteration: 214390 loss: 0.0005 lr: 0.02\n",
            "iteration: 214400 loss: 0.0006 lr: 0.02\n",
            "iteration: 214410 loss: 0.0010 lr: 0.02\n",
            "iteration: 214420 loss: 0.0008 lr: 0.02\n",
            "iteration: 214430 loss: 0.0008 lr: 0.02\n",
            "iteration: 214440 loss: 0.0008 lr: 0.02\n",
            "iteration: 214450 loss: 0.0009 lr: 0.02\n",
            "iteration: 214460 loss: 0.0007 lr: 0.02\n",
            "iteration: 214470 loss: 0.0008 lr: 0.02\n",
            "iteration: 214480 loss: 0.0007 lr: 0.02\n",
            "iteration: 214490 loss: 0.0009 lr: 0.02\n",
            "iteration: 214500 loss: 0.0007 lr: 0.02\n",
            "iteration: 214510 loss: 0.0009 lr: 0.02\n",
            "iteration: 214520 loss: 0.0006 lr: 0.02\n",
            "iteration: 214530 loss: 0.0008 lr: 0.02\n",
            "iteration: 214540 loss: 0.0015 lr: 0.02\n",
            "iteration: 214550 loss: 0.0013 lr: 0.02\n",
            "iteration: 214560 loss: 0.0010 lr: 0.02\n",
            "iteration: 214570 loss: 0.0008 lr: 0.02\n",
            "iteration: 214580 loss: 0.0014 lr: 0.02\n",
            "iteration: 214590 loss: 0.0009 lr: 0.02\n",
            "iteration: 214600 loss: 0.0008 lr: 0.02\n",
            "iteration: 214610 loss: 0.0008 lr: 0.02\n",
            "iteration: 214620 loss: 0.0014 lr: 0.02\n",
            "iteration: 214630 loss: 0.0007 lr: 0.02\n",
            "iteration: 214640 loss: 0.0007 lr: 0.02\n",
            "iteration: 214650 loss: 0.0010 lr: 0.02\n",
            "iteration: 214660 loss: 0.0009 lr: 0.02\n",
            "iteration: 214670 loss: 0.0008 lr: 0.02\n",
            "iteration: 214680 loss: 0.0009 lr: 0.02\n",
            "iteration: 214690 loss: 0.0008 lr: 0.02\n",
            "iteration: 214700 loss: 0.0006 lr: 0.02\n",
            "iteration: 214710 loss: 0.0009 lr: 0.02\n",
            "iteration: 214720 loss: 0.0009 lr: 0.02\n",
            "iteration: 214730 loss: 0.0009 lr: 0.02\n",
            "iteration: 214740 loss: 0.0010 lr: 0.02\n",
            "iteration: 214750 loss: 0.0007 lr: 0.02\n",
            "iteration: 214760 loss: 0.0005 lr: 0.02\n",
            "iteration: 214770 loss: 0.0006 lr: 0.02\n",
            "iteration: 214780 loss: 0.0010 lr: 0.02\n",
            "iteration: 214790 loss: 0.0006 lr: 0.02\n",
            "iteration: 214800 loss: 0.0007 lr: 0.02\n",
            "iteration: 214810 loss: 0.0014 lr: 0.02\n",
            "iteration: 214820 loss: 0.0007 lr: 0.02\n",
            "iteration: 214830 loss: 0.0008 lr: 0.02\n",
            "iteration: 214840 loss: 0.0008 lr: 0.02\n",
            "iteration: 214850 loss: 0.0008 lr: 0.02\n",
            "iteration: 214860 loss: 0.0009 lr: 0.02\n",
            "iteration: 214870 loss: 0.0014 lr: 0.02\n",
            "iteration: 214880 loss: 0.0008 lr: 0.02\n",
            "iteration: 214890 loss: 0.0005 lr: 0.02\n",
            "iteration: 214900 loss: 0.0010 lr: 0.02\n",
            "iteration: 214910 loss: 0.0008 lr: 0.02\n",
            "iteration: 214920 loss: 0.0008 lr: 0.02\n",
            "iteration: 214930 loss: 0.0005 lr: 0.02\n",
            "iteration: 214940 loss: 0.0007 lr: 0.02\n",
            "iteration: 214950 loss: 0.0009 lr: 0.02\n",
            "iteration: 214960 loss: 0.0007 lr: 0.02\n",
            "iteration: 214970 loss: 0.0009 lr: 0.02\n",
            "iteration: 214980 loss: 0.0005 lr: 0.02\n",
            "iteration: 214990 loss: 0.0006 lr: 0.02\n",
            "iteration: 215000 loss: 0.0005 lr: 0.02\n",
            "iteration: 215010 loss: 0.0009 lr: 0.02\n",
            "iteration: 215020 loss: 0.0009 lr: 0.02\n",
            "iteration: 215030 loss: 0.0007 lr: 0.02\n",
            "iteration: 215040 loss: 0.0007 lr: 0.02\n",
            "iteration: 215050 loss: 0.0007 lr: 0.02\n",
            "iteration: 215060 loss: 0.0008 lr: 0.02\n",
            "iteration: 215070 loss: 0.0007 lr: 0.02\n",
            "iteration: 215080 loss: 0.0007 lr: 0.02\n",
            "iteration: 215090 loss: 0.0010 lr: 0.02\n",
            "iteration: 215100 loss: 0.0007 lr: 0.02\n",
            "iteration: 215110 loss: 0.0009 lr: 0.02\n",
            "iteration: 215120 loss: 0.0005 lr: 0.02\n",
            "iteration: 215130 loss: 0.0009 lr: 0.02\n",
            "iteration: 215140 loss: 0.0009 lr: 0.02\n",
            "iteration: 215150 loss: 0.0012 lr: 0.02\n",
            "iteration: 215160 loss: 0.0006 lr: 0.02\n",
            "iteration: 215170 loss: 0.0006 lr: 0.02\n",
            "iteration: 215180 loss: 0.0008 lr: 0.02\n",
            "iteration: 215190 loss: 0.0007 lr: 0.02\n",
            "iteration: 215200 loss: 0.0008 lr: 0.02\n",
            "iteration: 215210 loss: 0.0007 lr: 0.02\n",
            "iteration: 215220 loss: 0.0006 lr: 0.02\n",
            "iteration: 215230 loss: 0.0006 lr: 0.02\n",
            "iteration: 215240 loss: 0.0010 lr: 0.02\n",
            "iteration: 215250 loss: 0.0009 lr: 0.02\n",
            "iteration: 215260 loss: 0.0011 lr: 0.02\n",
            "iteration: 215270 loss: 0.0008 lr: 0.02\n",
            "iteration: 215280 loss: 0.0006 lr: 0.02\n",
            "iteration: 215290 loss: 0.0009 lr: 0.02\n",
            "iteration: 215300 loss: 0.0008 lr: 0.02\n",
            "iteration: 215310 loss: 0.0008 lr: 0.02\n",
            "iteration: 215320 loss: 0.0007 lr: 0.02\n",
            "iteration: 215330 loss: 0.0011 lr: 0.02\n",
            "iteration: 215340 loss: 0.0007 lr: 0.02\n",
            "iteration: 215350 loss: 0.0007 lr: 0.02\n",
            "iteration: 215360 loss: 0.0009 lr: 0.02\n",
            "iteration: 215370 loss: 0.0009 lr: 0.02\n",
            "iteration: 215380 loss: 0.0007 lr: 0.02\n",
            "iteration: 215390 loss: 0.0007 lr: 0.02\n",
            "iteration: 215400 loss: 0.0006 lr: 0.02\n",
            "iteration: 215410 loss: 0.0009 lr: 0.02\n",
            "iteration: 215420 loss: 0.0017 lr: 0.02\n",
            "iteration: 215430 loss: 0.0008 lr: 0.02\n",
            "iteration: 215440 loss: 0.0007 lr: 0.02\n",
            "iteration: 215450 loss: 0.0008 lr: 0.02\n",
            "iteration: 215460 loss: 0.0006 lr: 0.02\n",
            "iteration: 215470 loss: 0.0008 lr: 0.02\n",
            "iteration: 215480 loss: 0.0008 lr: 0.02\n",
            "iteration: 215490 loss: 0.0007 lr: 0.02\n",
            "iteration: 215500 loss: 0.0007 lr: 0.02\n",
            "iteration: 215510 loss: 0.0006 lr: 0.02\n",
            "iteration: 215520 loss: 0.0006 lr: 0.02\n",
            "iteration: 215530 loss: 0.0008 lr: 0.02\n",
            "iteration: 215540 loss: 0.0008 lr: 0.02\n",
            "iteration: 215550 loss: 0.0006 lr: 0.02\n",
            "iteration: 215560 loss: 0.0005 lr: 0.02\n",
            "iteration: 215570 loss: 0.0008 lr: 0.02\n",
            "iteration: 215580 loss: 0.0007 lr: 0.02\n",
            "iteration: 215590 loss: 0.0006 lr: 0.02\n",
            "iteration: 215600 loss: 0.0008 lr: 0.02\n",
            "iteration: 215610 loss: 0.0009 lr: 0.02\n",
            "iteration: 215620 loss: 0.0007 lr: 0.02\n",
            "iteration: 215630 loss: 0.0006 lr: 0.02\n",
            "iteration: 215640 loss: 0.0012 lr: 0.02\n",
            "iteration: 215650 loss: 0.0008 lr: 0.02\n",
            "iteration: 215660 loss: 0.0007 lr: 0.02\n",
            "iteration: 215670 loss: 0.0010 lr: 0.02\n",
            "iteration: 215680 loss: 0.0008 lr: 0.02\n",
            "iteration: 215690 loss: 0.0008 lr: 0.02\n",
            "iteration: 215700 loss: 0.0009 lr: 0.02\n",
            "iteration: 215710 loss: 0.0011 lr: 0.02\n",
            "iteration: 215720 loss: 0.0008 lr: 0.02\n",
            "iteration: 215730 loss: 0.0008 lr: 0.02\n",
            "iteration: 215740 loss: 0.0006 lr: 0.02\n",
            "iteration: 215750 loss: 0.0013 lr: 0.02\n",
            "iteration: 215760 loss: 0.0006 lr: 0.02\n",
            "iteration: 215770 loss: 0.0009 lr: 0.02\n",
            "iteration: 215780 loss: 0.0007 lr: 0.02\n",
            "iteration: 215790 loss: 0.0008 lr: 0.02\n",
            "iteration: 215800 loss: 0.0010 lr: 0.02\n",
            "iteration: 215810 loss: 0.0005 lr: 0.02\n",
            "iteration: 215820 loss: 0.0008 lr: 0.02\n",
            "iteration: 215830 loss: 0.0010 lr: 0.02\n",
            "iteration: 215840 loss: 0.0009 lr: 0.02\n",
            "iteration: 215850 loss: 0.0007 lr: 0.02\n",
            "iteration: 215860 loss: 0.0008 lr: 0.02\n",
            "iteration: 215870 loss: 0.0009 lr: 0.02\n",
            "iteration: 215880 loss: 0.0010 lr: 0.02\n",
            "iteration: 215890 loss: 0.0008 lr: 0.02\n",
            "iteration: 215900 loss: 0.0010 lr: 0.02\n",
            "iteration: 215910 loss: 0.0009 lr: 0.02\n",
            "iteration: 215920 loss: 0.0007 lr: 0.02\n",
            "iteration: 215930 loss: 0.0006 lr: 0.02\n",
            "iteration: 215940 loss: 0.0005 lr: 0.02\n",
            "iteration: 215950 loss: 0.0005 lr: 0.02\n",
            "iteration: 215960 loss: 0.0007 lr: 0.02\n",
            "iteration: 215970 loss: 0.0007 lr: 0.02\n",
            "iteration: 215980 loss: 0.0009 lr: 0.02\n",
            "iteration: 215990 loss: 0.0006 lr: 0.02\n",
            "iteration: 216000 loss: 0.0009 lr: 0.02\n",
            "iteration: 216010 loss: 0.0010 lr: 0.02\n",
            "iteration: 216020 loss: 0.0008 lr: 0.02\n",
            "iteration: 216030 loss: 0.0006 lr: 0.02\n",
            "iteration: 216040 loss: 0.0010 lr: 0.02\n",
            "iteration: 216050 loss: 0.0012 lr: 0.02\n",
            "iteration: 216060 loss: 0.0013 lr: 0.02\n",
            "iteration: 216070 loss: 0.0008 lr: 0.02\n",
            "iteration: 216080 loss: 0.0007 lr: 0.02\n",
            "iteration: 216090 loss: 0.0009 lr: 0.02\n",
            "iteration: 216100 loss: 0.0009 lr: 0.02\n",
            "iteration: 216110 loss: 0.0008 lr: 0.02\n",
            "iteration: 216120 loss: 0.0007 lr: 0.02\n",
            "iteration: 216130 loss: 0.0007 lr: 0.02\n",
            "iteration: 216140 loss: 0.0006 lr: 0.02\n",
            "iteration: 216150 loss: 0.0008 lr: 0.02\n",
            "iteration: 216160 loss: 0.0007 lr: 0.02\n",
            "iteration: 216170 loss: 0.0011 lr: 0.02\n",
            "iteration: 216180 loss: 0.0008 lr: 0.02\n",
            "iteration: 216190 loss: 0.0007 lr: 0.02\n",
            "iteration: 216200 loss: 0.0007 lr: 0.02\n",
            "iteration: 216210 loss: 0.0005 lr: 0.02\n",
            "iteration: 216220 loss: 0.0007 lr: 0.02\n",
            "iteration: 216230 loss: 0.0008 lr: 0.02\n",
            "iteration: 216240 loss: 0.0007 lr: 0.02\n",
            "iteration: 216250 loss: 0.0008 lr: 0.02\n",
            "iteration: 216260 loss: 0.0010 lr: 0.02\n",
            "iteration: 216270 loss: 0.0009 lr: 0.02\n",
            "iteration: 216280 loss: 0.0010 lr: 0.02\n",
            "iteration: 216290 loss: 0.0007 lr: 0.02\n",
            "iteration: 216300 loss: 0.0009 lr: 0.02\n",
            "iteration: 216310 loss: 0.0008 lr: 0.02\n",
            "iteration: 216320 loss: 0.0008 lr: 0.02\n",
            "iteration: 216330 loss: 0.0006 lr: 0.02\n",
            "iteration: 216340 loss: 0.0008 lr: 0.02\n",
            "iteration: 216350 loss: 0.0007 lr: 0.02\n",
            "iteration: 216360 loss: 0.0008 lr: 0.02\n",
            "iteration: 216370 loss: 0.0007 lr: 0.02\n",
            "iteration: 216380 loss: 0.0009 lr: 0.02\n",
            "iteration: 216390 loss: 0.0008 lr: 0.02\n",
            "iteration: 216400 loss: 0.0007 lr: 0.02\n",
            "iteration: 216410 loss: 0.0011 lr: 0.02\n",
            "iteration: 216420 loss: 0.0010 lr: 0.02\n",
            "iteration: 216430 loss: 0.0008 lr: 0.02\n",
            "iteration: 216440 loss: 0.0008 lr: 0.02\n",
            "iteration: 216450 loss: 0.0007 lr: 0.02\n",
            "iteration: 216460 loss: 0.0008 lr: 0.02\n",
            "iteration: 216470 loss: 0.0010 lr: 0.02\n",
            "iteration: 216480 loss: 0.0011 lr: 0.02\n",
            "iteration: 216490 loss: 0.0008 lr: 0.02\n",
            "iteration: 216500 loss: 0.0011 lr: 0.02\n",
            "iteration: 216510 loss: 0.0008 lr: 0.02\n",
            "iteration: 216520 loss: 0.0006 lr: 0.02\n",
            "iteration: 216530 loss: 0.0006 lr: 0.02\n",
            "iteration: 216540 loss: 0.0007 lr: 0.02\n",
            "iteration: 216550 loss: 0.0007 lr: 0.02\n",
            "iteration: 216560 loss: 0.0005 lr: 0.02\n",
            "iteration: 216570 loss: 0.0009 lr: 0.02\n",
            "iteration: 216580 loss: 0.0005 lr: 0.02\n",
            "iteration: 216590 loss: 0.0008 lr: 0.02\n",
            "iteration: 216600 loss: 0.0008 lr: 0.02\n",
            "iteration: 216610 loss: 0.0008 lr: 0.02\n",
            "iteration: 216620 loss: 0.0007 lr: 0.02\n",
            "iteration: 216630 loss: 0.0006 lr: 0.02\n",
            "iteration: 216640 loss: 0.0005 lr: 0.02\n",
            "iteration: 216650 loss: 0.0009 lr: 0.02\n",
            "iteration: 216660 loss: 0.0007 lr: 0.02\n",
            "iteration: 216670 loss: 0.0008 lr: 0.02\n",
            "iteration: 216680 loss: 0.0008 lr: 0.02\n",
            "iteration: 216690 loss: 0.0007 lr: 0.02\n",
            "iteration: 216700 loss: 0.0005 lr: 0.02\n",
            "iteration: 216710 loss: 0.0007 lr: 0.02\n",
            "iteration: 216720 loss: 0.0008 lr: 0.02\n",
            "iteration: 216730 loss: 0.0009 lr: 0.02\n",
            "iteration: 216740 loss: 0.0009 lr: 0.02\n",
            "iteration: 216750 loss: 0.0008 lr: 0.02\n",
            "iteration: 216760 loss: 0.0008 lr: 0.02\n",
            "iteration: 216770 loss: 0.0009 lr: 0.02\n",
            "iteration: 216780 loss: 0.0008 lr: 0.02\n",
            "iteration: 216790 loss: 0.0009 lr: 0.02\n",
            "iteration: 216800 loss: 0.0013 lr: 0.02\n",
            "iteration: 216810 loss: 0.0006 lr: 0.02\n",
            "iteration: 216820 loss: 0.0009 lr: 0.02\n",
            "iteration: 216830 loss: 0.0006 lr: 0.02\n",
            "iteration: 216840 loss: 0.0007 lr: 0.02\n",
            "iteration: 216850 loss: 0.0010 lr: 0.02\n",
            "iteration: 216860 loss: 0.0007 lr: 0.02\n",
            "iteration: 216870 loss: 0.0007 lr: 0.02\n",
            "iteration: 216880 loss: 0.0007 lr: 0.02\n",
            "iteration: 216890 loss: 0.0009 lr: 0.02\n",
            "iteration: 216900 loss: 0.0012 lr: 0.02\n",
            "iteration: 216910 loss: 0.0009 lr: 0.02\n",
            "iteration: 216920 loss: 0.0012 lr: 0.02\n",
            "iteration: 216930 loss: 0.0007 lr: 0.02\n",
            "iteration: 216940 loss: 0.0006 lr: 0.02\n",
            "iteration: 216950 loss: 0.0011 lr: 0.02\n",
            "iteration: 216960 loss: 0.0007 lr: 0.02\n",
            "iteration: 216970 loss: 0.0010 lr: 0.02\n",
            "iteration: 216980 loss: 0.0008 lr: 0.02\n",
            "iteration: 216990 loss: 0.0007 lr: 0.02\n",
            "iteration: 217000 loss: 0.0005 lr: 0.02\n",
            "iteration: 217010 loss: 0.0011 lr: 0.02\n",
            "iteration: 217020 loss: 0.0008 lr: 0.02\n",
            "iteration: 217030 loss: 0.0011 lr: 0.02\n",
            "iteration: 217040 loss: 0.0012 lr: 0.02\n",
            "iteration: 217050 loss: 0.0011 lr: 0.02\n",
            "iteration: 217060 loss: 0.0008 lr: 0.02\n",
            "iteration: 217070 loss: 0.0008 lr: 0.02\n",
            "iteration: 217080 loss: 0.0007 lr: 0.02\n",
            "iteration: 217090 loss: 0.0007 lr: 0.02\n",
            "iteration: 217100 loss: 0.0008 lr: 0.02\n",
            "iteration: 217110 loss: 0.0009 lr: 0.02\n",
            "iteration: 217120 loss: 0.0008 lr: 0.02\n",
            "iteration: 217130 loss: 0.0011 lr: 0.02\n",
            "iteration: 217140 loss: 0.0009 lr: 0.02\n",
            "iteration: 217150 loss: 0.0007 lr: 0.02\n",
            "iteration: 217160 loss: 0.0009 lr: 0.02\n",
            "iteration: 217170 loss: 0.0006 lr: 0.02\n",
            "iteration: 217180 loss: 0.0009 lr: 0.02\n",
            "iteration: 217190 loss: 0.0009 lr: 0.02\n",
            "iteration: 217200 loss: 0.0007 lr: 0.02\n",
            "iteration: 217210 loss: 0.0008 lr: 0.02\n",
            "iteration: 217220 loss: 0.0009 lr: 0.02\n",
            "iteration: 217230 loss: 0.0007 lr: 0.02\n",
            "iteration: 217240 loss: 0.0008 lr: 0.02\n",
            "iteration: 217250 loss: 0.0007 lr: 0.02\n",
            "iteration: 217260 loss: 0.0008 lr: 0.02\n",
            "iteration: 217270 loss: 0.0007 lr: 0.02\n",
            "iteration: 217280 loss: 0.0008 lr: 0.02\n",
            "iteration: 217290 loss: 0.0008 lr: 0.02\n",
            "iteration: 217300 loss: 0.0007 lr: 0.02\n",
            "iteration: 217310 loss: 0.0007 lr: 0.02\n",
            "iteration: 217320 loss: 0.0011 lr: 0.02\n",
            "iteration: 217330 loss: 0.0009 lr: 0.02\n",
            "iteration: 217340 loss: 0.0006 lr: 0.02\n",
            "iteration: 217350 loss: 0.0008 lr: 0.02\n",
            "iteration: 217360 loss: 0.0006 lr: 0.02\n",
            "iteration: 217370 loss: 0.0009 lr: 0.02\n",
            "iteration: 217380 loss: 0.0008 lr: 0.02\n",
            "iteration: 217390 loss: 0.0008 lr: 0.02\n",
            "iteration: 217400 loss: 0.0007 lr: 0.02\n",
            "iteration: 217410 loss: 0.0006 lr: 0.02\n",
            "iteration: 217420 loss: 0.0007 lr: 0.02\n",
            "iteration: 217430 loss: 0.0009 lr: 0.02\n",
            "iteration: 217440 loss: 0.0009 lr: 0.02\n",
            "iteration: 217450 loss: 0.0006 lr: 0.02\n",
            "iteration: 217460 loss: 0.0006 lr: 0.02\n",
            "iteration: 217470 loss: 0.0007 lr: 0.02\n",
            "iteration: 217480 loss: 0.0011 lr: 0.02\n",
            "iteration: 217490 loss: 0.0007 lr: 0.02\n",
            "iteration: 217500 loss: 0.0008 lr: 0.02\n",
            "iteration: 217510 loss: 0.0007 lr: 0.02\n",
            "iteration: 217520 loss: 0.0008 lr: 0.02\n",
            "iteration: 217530 loss: 0.0007 lr: 0.02\n",
            "iteration: 217540 loss: 0.0009 lr: 0.02\n",
            "iteration: 217550 loss: 0.0010 lr: 0.02\n",
            "iteration: 217560 loss: 0.0008 lr: 0.02\n",
            "iteration: 217570 loss: 0.0013 lr: 0.02\n",
            "iteration: 217580 loss: 0.0007 lr: 0.02\n",
            "iteration: 217590 loss: 0.0006 lr: 0.02\n",
            "iteration: 217600 loss: 0.0011 lr: 0.02\n",
            "iteration: 217610 loss: 0.0008 lr: 0.02\n",
            "iteration: 217620 loss: 0.0005 lr: 0.02\n",
            "iteration: 217630 loss: 0.0007 lr: 0.02\n",
            "iteration: 217640 loss: 0.0008 lr: 0.02\n",
            "iteration: 217650 loss: 0.0008 lr: 0.02\n",
            "iteration: 217660 loss: 0.0006 lr: 0.02\n",
            "iteration: 217670 loss: 0.0008 lr: 0.02\n",
            "iteration: 217680 loss: 0.0007 lr: 0.02\n",
            "iteration: 217690 loss: 0.0011 lr: 0.02\n",
            "iteration: 217700 loss: 0.0010 lr: 0.02\n",
            "iteration: 217710 loss: 0.0010 lr: 0.02\n",
            "iteration: 217720 loss: 0.0010 lr: 0.02\n",
            "iteration: 217730 loss: 0.0007 lr: 0.02\n",
            "iteration: 217740 loss: 0.0008 lr: 0.02\n",
            "iteration: 217750 loss: 0.0008 lr: 0.02\n",
            "iteration: 217760 loss: 0.0010 lr: 0.02\n",
            "iteration: 217770 loss: 0.0008 lr: 0.02\n",
            "iteration: 217780 loss: 0.0008 lr: 0.02\n",
            "iteration: 217790 loss: 0.0008 lr: 0.02\n",
            "iteration: 217800 loss: 0.0007 lr: 0.02\n",
            "iteration: 217810 loss: 0.0007 lr: 0.02\n",
            "iteration: 217820 loss: 0.0007 lr: 0.02\n",
            "iteration: 217830 loss: 0.0006 lr: 0.02\n",
            "iteration: 217840 loss: 0.0006 lr: 0.02\n",
            "iteration: 217850 loss: 0.0009 lr: 0.02\n",
            "iteration: 217860 loss: 0.0008 lr: 0.02\n",
            "iteration: 217870 loss: 0.0010 lr: 0.02\n",
            "iteration: 217880 loss: 0.0006 lr: 0.02\n",
            "iteration: 217890 loss: 0.0007 lr: 0.02\n",
            "iteration: 217900 loss: 0.0008 lr: 0.02\n",
            "iteration: 217910 loss: 0.0006 lr: 0.02\n",
            "iteration: 217920 loss: 0.0009 lr: 0.02\n",
            "iteration: 217930 loss: 0.0006 lr: 0.02\n",
            "iteration: 217940 loss: 0.0006 lr: 0.02\n",
            "iteration: 217950 loss: 0.0008 lr: 0.02\n",
            "iteration: 217960 loss: 0.0010 lr: 0.02\n",
            "iteration: 217970 loss: 0.0008 lr: 0.02\n",
            "iteration: 217980 loss: 0.0007 lr: 0.02\n",
            "iteration: 217990 loss: 0.0006 lr: 0.02\n",
            "iteration: 218000 loss: 0.0008 lr: 0.02\n",
            "iteration: 218010 loss: 0.0008 lr: 0.02\n",
            "iteration: 218020 loss: 0.0011 lr: 0.02\n",
            "iteration: 218030 loss: 0.0007 lr: 0.02\n",
            "iteration: 218040 loss: 0.0008 lr: 0.02\n",
            "iteration: 218050 loss: 0.0010 lr: 0.02\n",
            "iteration: 218060 loss: 0.0008 lr: 0.02\n",
            "iteration: 218070 loss: 0.0007 lr: 0.02\n",
            "iteration: 218080 loss: 0.0009 lr: 0.02\n",
            "iteration: 218090 loss: 0.0006 lr: 0.02\n",
            "iteration: 218100 loss: 0.0007 lr: 0.02\n",
            "iteration: 218110 loss: 0.0009 lr: 0.02\n",
            "iteration: 218120 loss: 0.0006 lr: 0.02\n",
            "iteration: 218130 loss: 0.0005 lr: 0.02\n",
            "iteration: 218140 loss: 0.0007 lr: 0.02\n",
            "iteration: 218150 loss: 0.0007 lr: 0.02\n",
            "iteration: 218160 loss: 0.0006 lr: 0.02\n",
            "iteration: 218170 loss: 0.0008 lr: 0.02\n",
            "iteration: 218180 loss: 0.0008 lr: 0.02\n",
            "iteration: 218190 loss: 0.0007 lr: 0.02\n",
            "iteration: 218200 loss: 0.0007 lr: 0.02\n",
            "iteration: 218210 loss: 0.0007 lr: 0.02\n",
            "iteration: 218220 loss: 0.0006 lr: 0.02\n",
            "iteration: 218230 loss: 0.0006 lr: 0.02\n",
            "iteration: 218240 loss: 0.0007 lr: 0.02\n",
            "iteration: 218250 loss: 0.0006 lr: 0.02\n",
            "iteration: 218260 loss: 0.0008 lr: 0.02\n",
            "iteration: 218270 loss: 0.0012 lr: 0.02\n",
            "iteration: 218280 loss: 0.0010 lr: 0.02\n",
            "iteration: 218290 loss: 0.0009 lr: 0.02\n",
            "iteration: 218300 loss: 0.0008 lr: 0.02\n",
            "iteration: 218310 loss: 0.0008 lr: 0.02\n",
            "iteration: 218320 loss: 0.0008 lr: 0.02\n",
            "iteration: 218330 loss: 0.0007 lr: 0.02\n",
            "iteration: 218340 loss: 0.0008 lr: 0.02\n",
            "iteration: 218350 loss: 0.0009 lr: 0.02\n",
            "iteration: 218360 loss: 0.0008 lr: 0.02\n",
            "iteration: 218370 loss: 0.0006 lr: 0.02\n",
            "iteration: 218380 loss: 0.0007 lr: 0.02\n",
            "iteration: 218390 loss: 0.0007 lr: 0.02\n",
            "iteration: 218400 loss: 0.0008 lr: 0.02\n",
            "iteration: 218410 loss: 0.0010 lr: 0.02\n",
            "iteration: 218420 loss: 0.0007 lr: 0.02\n",
            "iteration: 218430 loss: 0.0007 lr: 0.02\n",
            "iteration: 218440 loss: 0.0005 lr: 0.02\n",
            "iteration: 218450 loss: 0.0009 lr: 0.02\n",
            "iteration: 218460 loss: 0.0009 lr: 0.02\n",
            "iteration: 218470 loss: 0.0008 lr: 0.02\n",
            "iteration: 218480 loss: 0.0009 lr: 0.02\n",
            "iteration: 218490 loss: 0.0009 lr: 0.02\n",
            "iteration: 218500 loss: 0.0007 lr: 0.02\n",
            "iteration: 218510 loss: 0.0008 lr: 0.02\n",
            "iteration: 218520 loss: 0.0008 lr: 0.02\n",
            "iteration: 218530 loss: 0.0010 lr: 0.02\n",
            "iteration: 218540 loss: 0.0009 lr: 0.02\n",
            "iteration: 218550 loss: 0.0008 lr: 0.02\n",
            "iteration: 218560 loss: 0.0006 lr: 0.02\n",
            "iteration: 218570 loss: 0.0007 lr: 0.02\n",
            "iteration: 218580 loss: 0.0008 lr: 0.02\n",
            "iteration: 218590 loss: 0.0008 lr: 0.02\n",
            "iteration: 218600 loss: 0.0008 lr: 0.02\n",
            "iteration: 218610 loss: 0.0008 lr: 0.02\n",
            "iteration: 218620 loss: 0.0009 lr: 0.02\n",
            "iteration: 218630 loss: 0.0007 lr: 0.02\n",
            "iteration: 218640 loss: 0.0006 lr: 0.02\n",
            "iteration: 218650 loss: 0.0007 lr: 0.02\n",
            "iteration: 218660 loss: 0.0011 lr: 0.02\n",
            "iteration: 218670 loss: 0.0007 lr: 0.02\n",
            "iteration: 218680 loss: 0.0006 lr: 0.02\n",
            "iteration: 218690 loss: 0.0008 lr: 0.02\n",
            "iteration: 218700 loss: 0.0007 lr: 0.02\n",
            "iteration: 218710 loss: 0.0007 lr: 0.02\n",
            "iteration: 218720 loss: 0.0007 lr: 0.02\n",
            "iteration: 218730 loss: 0.0008 lr: 0.02\n",
            "iteration: 218740 loss: 0.0009 lr: 0.02\n",
            "iteration: 218750 loss: 0.0009 lr: 0.02\n",
            "iteration: 218760 loss: 0.0005 lr: 0.02\n",
            "iteration: 218770 loss: 0.0006 lr: 0.02\n",
            "iteration: 218780 loss: 0.0008 lr: 0.02\n",
            "iteration: 218790 loss: 0.0007 lr: 0.02\n",
            "iteration: 218800 loss: 0.0007 lr: 0.02\n",
            "iteration: 218810 loss: 0.0006 lr: 0.02\n",
            "iteration: 218820 loss: 0.0007 lr: 0.02\n",
            "iteration: 218830 loss: 0.0010 lr: 0.02\n",
            "iteration: 218840 loss: 0.0007 lr: 0.02\n",
            "iteration: 218850 loss: 0.0007 lr: 0.02\n",
            "iteration: 218860 loss: 0.0006 lr: 0.02\n",
            "iteration: 218870 loss: 0.0007 lr: 0.02\n",
            "iteration: 218880 loss: 0.0009 lr: 0.02\n",
            "iteration: 218890 loss: 0.0007 lr: 0.02\n",
            "iteration: 218900 loss: 0.0011 lr: 0.02\n",
            "iteration: 218910 loss: 0.0008 lr: 0.02\n",
            "iteration: 218920 loss: 0.0010 lr: 0.02\n",
            "iteration: 218930 loss: 0.0006 lr: 0.02\n",
            "iteration: 218940 loss: 0.0010 lr: 0.02\n",
            "iteration: 218950 loss: 0.0006 lr: 0.02\n",
            "iteration: 218960 loss: 0.0008 lr: 0.02\n",
            "iteration: 218970 loss: 0.0008 lr: 0.02\n",
            "iteration: 218980 loss: 0.0010 lr: 0.02\n",
            "iteration: 218990 loss: 0.0010 lr: 0.02\n",
            "iteration: 219000 loss: 0.0009 lr: 0.02\n",
            "iteration: 219010 loss: 0.0006 lr: 0.02\n",
            "iteration: 219020 loss: 0.0009 lr: 0.02\n",
            "iteration: 219030 loss: 0.0010 lr: 0.02\n",
            "iteration: 219040 loss: 0.0008 lr: 0.02\n",
            "iteration: 219050 loss: 0.0011 lr: 0.02\n",
            "iteration: 219060 loss: 0.0007 lr: 0.02\n",
            "iteration: 219070 loss: 0.0010 lr: 0.02\n",
            "iteration: 219080 loss: 0.0011 lr: 0.02\n",
            "iteration: 219090 loss: 0.0009 lr: 0.02\n",
            "iteration: 219100 loss: 0.0010 lr: 0.02\n",
            "iteration: 219110 loss: 0.0010 lr: 0.02\n",
            "iteration: 219120 loss: 0.0009 lr: 0.02\n",
            "iteration: 219130 loss: 0.0009 lr: 0.02\n",
            "iteration: 219140 loss: 0.0011 lr: 0.02\n",
            "iteration: 219150 loss: 0.0009 lr: 0.02\n",
            "iteration: 219160 loss: 0.0011 lr: 0.02\n",
            "iteration: 219170 loss: 0.0010 lr: 0.02\n",
            "iteration: 219180 loss: 0.0007 lr: 0.02\n",
            "iteration: 219190 loss: 0.0005 lr: 0.02\n",
            "iteration: 219200 loss: 0.0008 lr: 0.02\n",
            "iteration: 219210 loss: 0.0007 lr: 0.02\n",
            "iteration: 219220 loss: 0.0008 lr: 0.02\n",
            "iteration: 219230 loss: 0.0007 lr: 0.02\n",
            "iteration: 219240 loss: 0.0007 lr: 0.02\n",
            "iteration: 219250 loss: 0.0008 lr: 0.02\n",
            "iteration: 219260 loss: 0.0007 lr: 0.02\n",
            "iteration: 219270 loss: 0.0007 lr: 0.02\n",
            "iteration: 219280 loss: 0.0006 lr: 0.02\n",
            "iteration: 219290 loss: 0.0009 lr: 0.02\n",
            "iteration: 219300 loss: 0.0008 lr: 0.02\n",
            "iteration: 219310 loss: 0.0007 lr: 0.02\n",
            "iteration: 219320 loss: 0.0008 lr: 0.02\n",
            "iteration: 219330 loss: 0.0008 lr: 0.02\n",
            "iteration: 219340 loss: 0.0006 lr: 0.02\n",
            "iteration: 219350 loss: 0.0007 lr: 0.02\n",
            "iteration: 219360 loss: 0.0006 lr: 0.02\n",
            "iteration: 219370 loss: 0.0006 lr: 0.02\n",
            "iteration: 219380 loss: 0.0007 lr: 0.02\n",
            "iteration: 219390 loss: 0.0009 lr: 0.02\n",
            "iteration: 219400 loss: 0.0007 lr: 0.02\n",
            "iteration: 219410 loss: 0.0007 lr: 0.02\n",
            "iteration: 219420 loss: 0.0009 lr: 0.02\n",
            "iteration: 219430 loss: 0.0012 lr: 0.02\n",
            "iteration: 219440 loss: 0.0007 lr: 0.02\n",
            "iteration: 219450 loss: 0.0008 lr: 0.02\n",
            "iteration: 219460 loss: 0.0007 lr: 0.02\n",
            "iteration: 219470 loss: 0.0009 lr: 0.02\n",
            "iteration: 219480 loss: 0.0008 lr: 0.02\n",
            "iteration: 219490 loss: 0.0007 lr: 0.02\n",
            "iteration: 219500 loss: 0.0010 lr: 0.02\n",
            "iteration: 219510 loss: 0.0010 lr: 0.02\n",
            "iteration: 219520 loss: 0.0009 lr: 0.02\n",
            "iteration: 219530 loss: 0.0009 lr: 0.02\n",
            "iteration: 219540 loss: 0.0010 lr: 0.02\n",
            "iteration: 219550 loss: 0.0009 lr: 0.02\n",
            "iteration: 219560 loss: 0.0009 lr: 0.02\n",
            "iteration: 219570 loss: 0.0009 lr: 0.02\n",
            "iteration: 219580 loss: 0.0009 lr: 0.02\n",
            "iteration: 219590 loss: 0.0007 lr: 0.02\n",
            "iteration: 219600 loss: 0.0006 lr: 0.02\n",
            "iteration: 219610 loss: 0.0009 lr: 0.02\n",
            "iteration: 219620 loss: 0.0009 lr: 0.02\n",
            "iteration: 219630 loss: 0.0008 lr: 0.02\n",
            "iteration: 219640 loss: 0.0015 lr: 0.02\n",
            "iteration: 219650 loss: 0.0007 lr: 0.02\n",
            "iteration: 219660 loss: 0.0006 lr: 0.02\n",
            "iteration: 219670 loss: 0.0010 lr: 0.02\n",
            "iteration: 219680 loss: 0.0008 lr: 0.02\n",
            "iteration: 219690 loss: 0.0009 lr: 0.02\n",
            "iteration: 219700 loss: 0.0008 lr: 0.02\n",
            "iteration: 219710 loss: 0.0008 lr: 0.02\n",
            "iteration: 219720 loss: 0.0006 lr: 0.02\n",
            "iteration: 219730 loss: 0.0006 lr: 0.02\n",
            "iteration: 219740 loss: 0.0007 lr: 0.02\n",
            "iteration: 219750 loss: 0.0007 lr: 0.02\n",
            "iteration: 219760 loss: 0.0008 lr: 0.02\n",
            "iteration: 219770 loss: 0.0008 lr: 0.02\n",
            "iteration: 219780 loss: 0.0007 lr: 0.02\n",
            "iteration: 219790 loss: 0.0010 lr: 0.02\n",
            "iteration: 219800 loss: 0.0011 lr: 0.02\n",
            "iteration: 219810 loss: 0.0007 lr: 0.02\n",
            "iteration: 219820 loss: 0.0005 lr: 0.02\n",
            "iteration: 219830 loss: 0.0008 lr: 0.02\n",
            "iteration: 219840 loss: 0.0009 lr: 0.02\n",
            "iteration: 219850 loss: 0.0009 lr: 0.02\n",
            "iteration: 219860 loss: 0.0009 lr: 0.02\n",
            "iteration: 219870 loss: 0.0006 lr: 0.02\n",
            "iteration: 219880 loss: 0.0008 lr: 0.02\n",
            "iteration: 219890 loss: 0.0009 lr: 0.02\n",
            "iteration: 219900 loss: 0.0006 lr: 0.02\n",
            "iteration: 219910 loss: 0.0008 lr: 0.02\n",
            "iteration: 219920 loss: 0.0007 lr: 0.02\n",
            "iteration: 219930 loss: 0.0006 lr: 0.02\n",
            "iteration: 219940 loss: 0.0008 lr: 0.02\n",
            "iteration: 219950 loss: 0.0007 lr: 0.02\n",
            "iteration: 219960 loss: 0.0007 lr: 0.02\n",
            "iteration: 219970 loss: 0.0006 lr: 0.02\n",
            "iteration: 219980 loss: 0.0007 lr: 0.02\n",
            "iteration: 219990 loss: 0.0009 lr: 0.02\n",
            "iteration: 220000 loss: 0.0009 lr: 0.02\n",
            "iteration: 220010 loss: 0.0009 lr: 0.02\n",
            "iteration: 220020 loss: 0.0012 lr: 0.02\n",
            "iteration: 220030 loss: 0.0009 lr: 0.02\n",
            "iteration: 220040 loss: 0.0006 lr: 0.02\n",
            "iteration: 220050 loss: 0.0020 lr: 0.02\n",
            "iteration: 220060 loss: 0.0005 lr: 0.02\n",
            "iteration: 220070 loss: 0.0008 lr: 0.02\n",
            "iteration: 220080 loss: 0.0009 lr: 0.02\n",
            "iteration: 220090 loss: 0.0007 lr: 0.02\n",
            "iteration: 220100 loss: 0.0008 lr: 0.02\n",
            "iteration: 220110 loss: 0.0009 lr: 0.02\n",
            "iteration: 220120 loss: 0.0008 lr: 0.02\n",
            "iteration: 220130 loss: 0.0006 lr: 0.02\n",
            "iteration: 220140 loss: 0.0009 lr: 0.02\n",
            "iteration: 220150 loss: 0.0010 lr: 0.02\n",
            "iteration: 220160 loss: 0.0006 lr: 0.02\n",
            "iteration: 220170 loss: 0.0007 lr: 0.02\n",
            "iteration: 220180 loss: 0.0005 lr: 0.02\n",
            "iteration: 220190 loss: 0.0007 lr: 0.02\n",
            "iteration: 220200 loss: 0.0012 lr: 0.02\n",
            "iteration: 220210 loss: 0.0008 lr: 0.02\n",
            "iteration: 220220 loss: 0.0007 lr: 0.02\n",
            "iteration: 220230 loss: 0.0010 lr: 0.02\n",
            "iteration: 220240 loss: 0.0010 lr: 0.02\n",
            "iteration: 220250 loss: 0.0007 lr: 0.02\n",
            "iteration: 220260 loss: 0.0009 lr: 0.02\n",
            "iteration: 220270 loss: 0.0008 lr: 0.02\n",
            "iteration: 220280 loss: 0.0009 lr: 0.02\n",
            "iteration: 220290 loss: 0.0009 lr: 0.02\n",
            "iteration: 220300 loss: 0.0010 lr: 0.02\n",
            "iteration: 220310 loss: 0.0007 lr: 0.02\n",
            "iteration: 220320 loss: 0.0008 lr: 0.02\n",
            "iteration: 220330 loss: 0.0010 lr: 0.02\n",
            "iteration: 220340 loss: 0.0009 lr: 0.02\n",
            "iteration: 220350 loss: 0.0009 lr: 0.02\n",
            "iteration: 220360 loss: 0.0007 lr: 0.02\n",
            "iteration: 220370 loss: 0.0010 lr: 0.02\n",
            "iteration: 220380 loss: 0.0009 lr: 0.02\n",
            "iteration: 220390 loss: 0.0008 lr: 0.02\n",
            "iteration: 220400 loss: 0.0008 lr: 0.02\n",
            "iteration: 220410 loss: 0.0007 lr: 0.02\n",
            "iteration: 220420 loss: 0.0008 lr: 0.02\n",
            "iteration: 220430 loss: 0.0008 lr: 0.02\n",
            "iteration: 220440 loss: 0.0009 lr: 0.02\n",
            "iteration: 220450 loss: 0.0010 lr: 0.02\n",
            "iteration: 220460 loss: 0.0012 lr: 0.02\n",
            "iteration: 220470 loss: 0.0009 lr: 0.02\n",
            "iteration: 220480 loss: 0.0006 lr: 0.02\n",
            "iteration: 220490 loss: 0.0006 lr: 0.02\n",
            "iteration: 220500 loss: 0.0008 lr: 0.02\n",
            "iteration: 220510 loss: 0.0007 lr: 0.02\n",
            "iteration: 220520 loss: 0.0009 lr: 0.02\n",
            "iteration: 220530 loss: 0.0007 lr: 0.02\n",
            "iteration: 220540 loss: 0.0009 lr: 0.02\n",
            "iteration: 220550 loss: 0.0010 lr: 0.02\n",
            "iteration: 220560 loss: 0.0008 lr: 0.02\n",
            "iteration: 220570 loss: 0.0006 lr: 0.02\n",
            "iteration: 220580 loss: 0.0008 lr: 0.02\n",
            "iteration: 220590 loss: 0.0008 lr: 0.02\n",
            "iteration: 220600 loss: 0.0006 lr: 0.02\n",
            "iteration: 220610 loss: 0.0007 lr: 0.02\n",
            "iteration: 220620 loss: 0.0008 lr: 0.02\n",
            "iteration: 220630 loss: 0.0005 lr: 0.02\n",
            "iteration: 220640 loss: 0.0008 lr: 0.02\n",
            "iteration: 220650 loss: 0.0010 lr: 0.02\n",
            "iteration: 220660 loss: 0.0005 lr: 0.02\n",
            "iteration: 220670 loss: 0.0009 lr: 0.02\n",
            "iteration: 220680 loss: 0.0008 lr: 0.02\n",
            "iteration: 220690 loss: 0.0010 lr: 0.02\n",
            "iteration: 220700 loss: 0.0008 lr: 0.02\n",
            "iteration: 220710 loss: 0.0008 lr: 0.02\n",
            "iteration: 220720 loss: 0.0009 lr: 0.02\n",
            "iteration: 220730 loss: 0.0007 lr: 0.02\n",
            "iteration: 220740 loss: 0.0007 lr: 0.02\n",
            "iteration: 220750 loss: 0.0010 lr: 0.02\n",
            "iteration: 220760 loss: 0.0006 lr: 0.02\n",
            "iteration: 220770 loss: 0.0013 lr: 0.02\n",
            "iteration: 220780 loss: 0.0010 lr: 0.02\n",
            "iteration: 220790 loss: 0.0009 lr: 0.02\n",
            "iteration: 220800 loss: 0.0004 lr: 0.02\n",
            "iteration: 220810 loss: 0.0008 lr: 0.02\n",
            "iteration: 220820 loss: 0.0006 lr: 0.02\n",
            "iteration: 220830 loss: 0.0007 lr: 0.02\n",
            "iteration: 220840 loss: 0.0008 lr: 0.02\n",
            "iteration: 220850 loss: 0.0008 lr: 0.02\n",
            "iteration: 220860 loss: 0.0010 lr: 0.02\n",
            "iteration: 220870 loss: 0.0007 lr: 0.02\n",
            "iteration: 220880 loss: 0.0008 lr: 0.02\n",
            "iteration: 220890 loss: 0.0008 lr: 0.02\n",
            "iteration: 220900 loss: 0.0006 lr: 0.02\n",
            "iteration: 220910 loss: 0.0009 lr: 0.02\n",
            "iteration: 220920 loss: 0.0010 lr: 0.02\n",
            "iteration: 220930 loss: 0.0008 lr: 0.02\n",
            "iteration: 220940 loss: 0.0006 lr: 0.02\n",
            "iteration: 220950 loss: 0.0009 lr: 0.02\n",
            "iteration: 220960 loss: 0.0009 lr: 0.02\n",
            "iteration: 220970 loss: 0.0006 lr: 0.02\n",
            "iteration: 220980 loss: 0.0008 lr: 0.02\n",
            "iteration: 220990 loss: 0.0006 lr: 0.02\n",
            "iteration: 221000 loss: 0.0007 lr: 0.02\n",
            "iteration: 221010 loss: 0.0008 lr: 0.02\n",
            "iteration: 221020 loss: 0.0009 lr: 0.02\n",
            "iteration: 221030 loss: 0.0010 lr: 0.02\n",
            "iteration: 221040 loss: 0.0010 lr: 0.02\n",
            "iteration: 221050 loss: 0.0008 lr: 0.02\n",
            "iteration: 221060 loss: 0.0007 lr: 0.02\n",
            "iteration: 221070 loss: 0.0008 lr: 0.02\n",
            "iteration: 221080 loss: 0.0007 lr: 0.02\n",
            "iteration: 221090 loss: 0.0007 lr: 0.02\n",
            "iteration: 221100 loss: 0.0007 lr: 0.02\n",
            "iteration: 221110 loss: 0.0008 lr: 0.02\n",
            "iteration: 221120 loss: 0.0008 lr: 0.02\n",
            "iteration: 221130 loss: 0.0006 lr: 0.02\n",
            "iteration: 221140 loss: 0.0007 lr: 0.02\n",
            "iteration: 221150 loss: 0.0008 lr: 0.02\n",
            "iteration: 221160 loss: 0.0008 lr: 0.02\n",
            "iteration: 221170 loss: 0.0008 lr: 0.02\n",
            "iteration: 221180 loss: 0.0010 lr: 0.02\n",
            "iteration: 221190 loss: 0.0008 lr: 0.02\n",
            "iteration: 221200 loss: 0.0008 lr: 0.02\n",
            "iteration: 221210 loss: 0.0008 lr: 0.02\n",
            "iteration: 221220 loss: 0.0008 lr: 0.02\n",
            "iteration: 221230 loss: 0.0007 lr: 0.02\n",
            "iteration: 221240 loss: 0.0008 lr: 0.02\n",
            "iteration: 221250 loss: 0.0010 lr: 0.02\n",
            "iteration: 221260 loss: 0.0008 lr: 0.02\n",
            "iteration: 221270 loss: 0.0007 lr: 0.02\n",
            "iteration: 221280 loss: 0.0008 lr: 0.02\n",
            "iteration: 221290 loss: 0.0008 lr: 0.02\n",
            "iteration: 221300 loss: 0.0013 lr: 0.02\n",
            "iteration: 221310 loss: 0.0010 lr: 0.02\n",
            "iteration: 221320 loss: 0.0008 lr: 0.02\n",
            "iteration: 221330 loss: 0.0007 lr: 0.02\n",
            "iteration: 221340 loss: 0.0008 lr: 0.02\n",
            "iteration: 221350 loss: 0.0008 lr: 0.02\n",
            "iteration: 221360 loss: 0.0006 lr: 0.02\n",
            "iteration: 221370 loss: 0.0008 lr: 0.02\n",
            "iteration: 221380 loss: 0.0008 lr: 0.02\n",
            "iteration: 221390 loss: 0.0008 lr: 0.02\n",
            "iteration: 221400 loss: 0.0007 lr: 0.02\n",
            "iteration: 221410 loss: 0.0008 lr: 0.02\n",
            "iteration: 221420 loss: 0.0007 lr: 0.02\n",
            "iteration: 221430 loss: 0.0007 lr: 0.02\n",
            "iteration: 221440 loss: 0.0008 lr: 0.02\n",
            "iteration: 221450 loss: 0.0009 lr: 0.02\n",
            "iteration: 221460 loss: 0.0011 lr: 0.02\n",
            "iteration: 221470 loss: 0.0006 lr: 0.02\n",
            "iteration: 221480 loss: 0.0009 lr: 0.02\n",
            "iteration: 221490 loss: 0.0009 lr: 0.02\n",
            "iteration: 221500 loss: 0.0006 lr: 0.02\n",
            "iteration: 221510 loss: 0.0008 lr: 0.02\n",
            "iteration: 221520 loss: 0.0007 lr: 0.02\n",
            "iteration: 221530 loss: 0.0005 lr: 0.02\n",
            "iteration: 221540 loss: 0.0008 lr: 0.02\n",
            "iteration: 221550 loss: 0.0005 lr: 0.02\n",
            "iteration: 221560 loss: 0.0010 lr: 0.02\n",
            "iteration: 221570 loss: 0.0006 lr: 0.02\n",
            "iteration: 221580 loss: 0.0009 lr: 0.02\n",
            "iteration: 221590 loss: 0.0009 lr: 0.02\n",
            "iteration: 221600 loss: 0.0008 lr: 0.02\n",
            "iteration: 221610 loss: 0.0007 lr: 0.02\n",
            "iteration: 221620 loss: 0.0008 lr: 0.02\n",
            "iteration: 221630 loss: 0.0009 lr: 0.02\n",
            "iteration: 221640 loss: 0.0007 lr: 0.02\n",
            "iteration: 221650 loss: 0.0007 lr: 0.02\n",
            "iteration: 221660 loss: 0.0008 lr: 0.02\n",
            "iteration: 221670 loss: 0.0010 lr: 0.02\n",
            "iteration: 221680 loss: 0.0007 lr: 0.02\n",
            "iteration: 221690 loss: 0.0008 lr: 0.02\n",
            "iteration: 221700 loss: 0.0010 lr: 0.02\n",
            "iteration: 221710 loss: 0.0008 lr: 0.02\n",
            "iteration: 221720 loss: 0.0007 lr: 0.02\n",
            "iteration: 221730 loss: 0.0010 lr: 0.02\n",
            "iteration: 221740 loss: 0.0010 lr: 0.02\n",
            "iteration: 221750 loss: 0.0006 lr: 0.02\n",
            "iteration: 221760 loss: 0.0006 lr: 0.02\n",
            "iteration: 221770 loss: 0.0009 lr: 0.02\n",
            "iteration: 221780 loss: 0.0007 lr: 0.02\n",
            "iteration: 221790 loss: 0.0009 lr: 0.02\n",
            "iteration: 221800 loss: 0.0008 lr: 0.02\n",
            "iteration: 221810 loss: 0.0009 lr: 0.02\n",
            "iteration: 221820 loss: 0.0008 lr: 0.02\n",
            "iteration: 221830 loss: 0.0007 lr: 0.02\n",
            "iteration: 221840 loss: 0.0007 lr: 0.02\n",
            "iteration: 221850 loss: 0.0011 lr: 0.02\n",
            "iteration: 221860 loss: 0.0007 lr: 0.02\n",
            "iteration: 221870 loss: 0.0011 lr: 0.02\n",
            "iteration: 221880 loss: 0.0007 lr: 0.02\n",
            "iteration: 221890 loss: 0.0007 lr: 0.02\n",
            "iteration: 221900 loss: 0.0012 lr: 0.02\n",
            "iteration: 221910 loss: 0.0009 lr: 0.02\n",
            "iteration: 221920 loss: 0.0008 lr: 0.02\n",
            "iteration: 221930 loss: 0.0006 lr: 0.02\n",
            "iteration: 221940 loss: 0.0008 lr: 0.02\n",
            "iteration: 221950 loss: 0.0008 lr: 0.02\n",
            "iteration: 221960 loss: 0.0008 lr: 0.02\n",
            "iteration: 221970 loss: 0.0007 lr: 0.02\n",
            "iteration: 221980 loss: 0.0008 lr: 0.02\n",
            "iteration: 221990 loss: 0.0005 lr: 0.02\n",
            "iteration: 222000 loss: 0.0008 lr: 0.02\n",
            "iteration: 222010 loss: 0.0009 lr: 0.02\n",
            "iteration: 222020 loss: 0.0006 lr: 0.02\n",
            "iteration: 222030 loss: 0.0006 lr: 0.02\n",
            "iteration: 222040 loss: 0.0005 lr: 0.02\n",
            "iteration: 222050 loss: 0.0009 lr: 0.02\n",
            "iteration: 222060 loss: 0.0008 lr: 0.02\n",
            "iteration: 222070 loss: 0.0007 lr: 0.02\n",
            "iteration: 222080 loss: 0.0012 lr: 0.02\n",
            "iteration: 222090 loss: 0.0009 lr: 0.02\n",
            "iteration: 222100 loss: 0.0012 lr: 0.02\n",
            "iteration: 222110 loss: 0.0006 lr: 0.02\n",
            "iteration: 222120 loss: 0.0007 lr: 0.02\n",
            "iteration: 222130 loss: 0.0011 lr: 0.02\n",
            "iteration: 222140 loss: 0.0008 lr: 0.02\n",
            "iteration: 222150 loss: 0.0011 lr: 0.02\n",
            "iteration: 222160 loss: 0.0010 lr: 0.02\n",
            "iteration: 222170 loss: 0.0007 lr: 0.02\n",
            "iteration: 222180 loss: 0.0009 lr: 0.02\n",
            "iteration: 222190 loss: 0.0010 lr: 0.02\n",
            "iteration: 222200 loss: 0.0009 lr: 0.02\n",
            "iteration: 222210 loss: 0.0006 lr: 0.02\n",
            "iteration: 222220 loss: 0.0006 lr: 0.02\n",
            "iteration: 222230 loss: 0.0006 lr: 0.02\n",
            "iteration: 222240 loss: 0.0009 lr: 0.02\n",
            "iteration: 222250 loss: 0.0006 lr: 0.02\n",
            "iteration: 222260 loss: 0.0010 lr: 0.02\n",
            "iteration: 222270 loss: 0.0008 lr: 0.02\n",
            "iteration: 222280 loss: 0.0005 lr: 0.02\n",
            "iteration: 222290 loss: 0.0007 lr: 0.02\n",
            "iteration: 222300 loss: 0.0008 lr: 0.02\n",
            "iteration: 222310 loss: 0.0006 lr: 0.02\n",
            "iteration: 222320 loss: 0.0006 lr: 0.02\n",
            "iteration: 222330 loss: 0.0008 lr: 0.02\n",
            "iteration: 222340 loss: 0.0009 lr: 0.02\n",
            "iteration: 222350 loss: 0.0007 lr: 0.02\n",
            "iteration: 222360 loss: 0.0008 lr: 0.02\n",
            "iteration: 222370 loss: 0.0012 lr: 0.02\n",
            "iteration: 222380 loss: 0.0007 lr: 0.02\n",
            "iteration: 222390 loss: 0.0006 lr: 0.02\n",
            "iteration: 222400 loss: 0.0010 lr: 0.02\n",
            "iteration: 222410 loss: 0.0011 lr: 0.02\n",
            "iteration: 222420 loss: 0.0006 lr: 0.02\n",
            "iteration: 222430 loss: 0.0008 lr: 0.02\n",
            "iteration: 222440 loss: 0.0010 lr: 0.02\n",
            "iteration: 222450 loss: 0.0007 lr: 0.02\n",
            "iteration: 222460 loss: 0.0007 lr: 0.02\n",
            "iteration: 222470 loss: 0.0007 lr: 0.02\n",
            "iteration: 222480 loss: 0.0008 lr: 0.02\n",
            "iteration: 222490 loss: 0.0006 lr: 0.02\n",
            "iteration: 222500 loss: 0.0009 lr: 0.02\n",
            "iteration: 222510 loss: 0.0009 lr: 0.02\n",
            "iteration: 222520 loss: 0.0008 lr: 0.02\n",
            "iteration: 222530 loss: 0.0008 lr: 0.02\n",
            "iteration: 222540 loss: 0.0007 lr: 0.02\n",
            "iteration: 222550 loss: 0.0005 lr: 0.02\n",
            "iteration: 222560 loss: 0.0008 lr: 0.02\n",
            "iteration: 222570 loss: 0.0009 lr: 0.02\n",
            "iteration: 222580 loss: 0.0010 lr: 0.02\n",
            "iteration: 222590 loss: 0.0005 lr: 0.02\n",
            "iteration: 222600 loss: 0.0007 lr: 0.02\n",
            "iteration: 222610 loss: 0.0007 lr: 0.02\n",
            "iteration: 222620 loss: 0.0012 lr: 0.02\n",
            "iteration: 222630 loss: 0.0008 lr: 0.02\n",
            "iteration: 222640 loss: 0.0008 lr: 0.02\n",
            "iteration: 222650 loss: 0.0008 lr: 0.02\n",
            "iteration: 222660 loss: 0.0011 lr: 0.02\n",
            "iteration: 222670 loss: 0.0008 lr: 0.02\n",
            "iteration: 222680 loss: 0.0010 lr: 0.02\n",
            "iteration: 222690 loss: 0.0007 lr: 0.02\n",
            "iteration: 222700 loss: 0.0007 lr: 0.02\n",
            "iteration: 222710 loss: 0.0011 lr: 0.02\n",
            "iteration: 222720 loss: 0.0008 lr: 0.02\n",
            "iteration: 222730 loss: 0.0007 lr: 0.02\n",
            "iteration: 222740 loss: 0.0009 lr: 0.02\n",
            "iteration: 222750 loss: 0.0007 lr: 0.02\n",
            "iteration: 222760 loss: 0.0010 lr: 0.02\n",
            "iteration: 222770 loss: 0.0007 lr: 0.02\n",
            "iteration: 222780 loss: 0.0006 lr: 0.02\n",
            "iteration: 222790 loss: 0.0010 lr: 0.02\n",
            "iteration: 222800 loss: 0.0009 lr: 0.02\n",
            "iteration: 222810 loss: 0.0010 lr: 0.02\n",
            "iteration: 222820 loss: 0.0012 lr: 0.02\n",
            "iteration: 222830 loss: 0.0008 lr: 0.02\n",
            "iteration: 222840 loss: 0.0006 lr: 0.02\n",
            "iteration: 222850 loss: 0.0007 lr: 0.02\n",
            "iteration: 222860 loss: 0.0006 lr: 0.02\n",
            "iteration: 222870 loss: 0.0008 lr: 0.02\n",
            "iteration: 222880 loss: 0.0008 lr: 0.02\n",
            "iteration: 222890 loss: 0.0009 lr: 0.02\n",
            "iteration: 222900 loss: 0.0007 lr: 0.02\n",
            "iteration: 222910 loss: 0.0009 lr: 0.02\n",
            "iteration: 222920 loss: 0.0010 lr: 0.02\n",
            "iteration: 222930 loss: 0.0008 lr: 0.02\n",
            "iteration: 222940 loss: 0.0008 lr: 0.02\n",
            "iteration: 222950 loss: 0.0008 lr: 0.02\n",
            "iteration: 222960 loss: 0.0007 lr: 0.02\n",
            "iteration: 222970 loss: 0.0010 lr: 0.02\n",
            "iteration: 222980 loss: 0.0010 lr: 0.02\n",
            "iteration: 222990 loss: 0.0010 lr: 0.02\n",
            "iteration: 223000 loss: 0.0008 lr: 0.02\n",
            "iteration: 223010 loss: 0.0009 lr: 0.02\n",
            "iteration: 223020 loss: 0.0009 lr: 0.02\n",
            "iteration: 223030 loss: 0.0009 lr: 0.02\n",
            "iteration: 223040 loss: 0.0009 lr: 0.02\n",
            "iteration: 223050 loss: 0.0008 lr: 0.02\n",
            "iteration: 223060 loss: 0.0006 lr: 0.02\n",
            "iteration: 223070 loss: 0.0006 lr: 0.02\n",
            "iteration: 223080 loss: 0.0008 lr: 0.02\n",
            "iteration: 223090 loss: 0.0006 lr: 0.02\n",
            "iteration: 223100 loss: 0.0007 lr: 0.02\n",
            "iteration: 223110 loss: 0.0008 lr: 0.02\n",
            "iteration: 223120 loss: 0.0007 lr: 0.02\n",
            "iteration: 223130 loss: 0.0011 lr: 0.02\n",
            "iteration: 223140 loss: 0.0008 lr: 0.02\n",
            "iteration: 223150 loss: 0.0009 lr: 0.02\n",
            "iteration: 223160 loss: 0.0009 lr: 0.02\n",
            "iteration: 223170 loss: 0.0007 lr: 0.02\n",
            "iteration: 223180 loss: 0.0005 lr: 0.02\n",
            "iteration: 223190 loss: 0.0011 lr: 0.02\n",
            "iteration: 223200 loss: 0.0008 lr: 0.02\n",
            "iteration: 223210 loss: 0.0011 lr: 0.02\n",
            "iteration: 223220 loss: 0.0011 lr: 0.02\n",
            "iteration: 223230 loss: 0.0008 lr: 0.02\n",
            "iteration: 223240 loss: 0.0007 lr: 0.02\n",
            "iteration: 223250 loss: 0.0011 lr: 0.02\n",
            "iteration: 223260 loss: 0.0007 lr: 0.02\n",
            "iteration: 223270 loss: 0.0007 lr: 0.02\n",
            "iteration: 223280 loss: 0.0006 lr: 0.02\n",
            "iteration: 223290 loss: 0.0008 lr: 0.02\n",
            "iteration: 223300 loss: 0.0008 lr: 0.02\n",
            "iteration: 223310 loss: 0.0006 lr: 0.02\n",
            "iteration: 223320 loss: 0.0007 lr: 0.02\n",
            "iteration: 223330 loss: 0.0010 lr: 0.02\n",
            "iteration: 223340 loss: 0.0008 lr: 0.02\n",
            "iteration: 223350 loss: 0.0004 lr: 0.02\n",
            "iteration: 223360 loss: 0.0009 lr: 0.02\n",
            "iteration: 223370 loss: 0.0005 lr: 0.02\n",
            "iteration: 223380 loss: 0.0007 lr: 0.02\n",
            "iteration: 223390 loss: 0.0011 lr: 0.02\n",
            "iteration: 223400 loss: 0.0011 lr: 0.02\n",
            "iteration: 223410 loss: 0.0007 lr: 0.02\n",
            "iteration: 223420 loss: 0.0007 lr: 0.02\n",
            "iteration: 223430 loss: 0.0007 lr: 0.02\n",
            "iteration: 223440 loss: 0.0008 lr: 0.02\n",
            "iteration: 223450 loss: 0.0007 lr: 0.02\n",
            "iteration: 223460 loss: 0.0007 lr: 0.02\n",
            "iteration: 223470 loss: 0.0009 lr: 0.02\n",
            "iteration: 223480 loss: 0.0013 lr: 0.02\n",
            "iteration: 223490 loss: 0.0007 lr: 0.02\n",
            "iteration: 223500 loss: 0.0007 lr: 0.02\n",
            "iteration: 223510 loss: 0.0007 lr: 0.02\n",
            "iteration: 223520 loss: 0.0007 lr: 0.02\n",
            "iteration: 223530 loss: 0.0007 lr: 0.02\n",
            "iteration: 223540 loss: 0.0009 lr: 0.02\n",
            "iteration: 223550 loss: 0.0008 lr: 0.02\n",
            "iteration: 223560 loss: 0.0006 lr: 0.02\n",
            "iteration: 223570 loss: 0.0010 lr: 0.02\n",
            "iteration: 223580 loss: 0.0006 lr: 0.02\n",
            "iteration: 223590 loss: 0.0005 lr: 0.02\n",
            "iteration: 223600 loss: 0.0009 lr: 0.02\n",
            "iteration: 223610 loss: 0.0010 lr: 0.02\n",
            "iteration: 223620 loss: 0.0007 lr: 0.02\n",
            "iteration: 223630 loss: 0.0007 lr: 0.02\n",
            "iteration: 223640 loss: 0.0005 lr: 0.02\n",
            "iteration: 223650 loss: 0.0007 lr: 0.02\n",
            "iteration: 223660 loss: 0.0008 lr: 0.02\n",
            "iteration: 223670 loss: 0.0006 lr: 0.02\n",
            "iteration: 223680 loss: 0.0006 lr: 0.02\n",
            "iteration: 223690 loss: 0.0008 lr: 0.02\n",
            "iteration: 223700 loss: 0.0008 lr: 0.02\n",
            "iteration: 223710 loss: 0.0006 lr: 0.02\n",
            "iteration: 223720 loss: 0.0005 lr: 0.02\n",
            "iteration: 223730 loss: 0.0005 lr: 0.02\n",
            "iteration: 223740 loss: 0.0010 lr: 0.02\n",
            "iteration: 223750 loss: 0.0008 lr: 0.02\n",
            "iteration: 223760 loss: 0.0006 lr: 0.02\n",
            "iteration: 223770 loss: 0.0008 lr: 0.02\n",
            "iteration: 223780 loss: 0.0007 lr: 0.02\n",
            "iteration: 223790 loss: 0.0008 lr: 0.02\n",
            "iteration: 223800 loss: 0.0006 lr: 0.02\n",
            "iteration: 223810 loss: 0.0007 lr: 0.02\n",
            "iteration: 223820 loss: 0.0008 lr: 0.02\n",
            "iteration: 223830 loss: 0.0006 lr: 0.02\n",
            "iteration: 223840 loss: 0.0007 lr: 0.02\n",
            "iteration: 223850 loss: 0.0006 lr: 0.02\n",
            "iteration: 223860 loss: 0.0007 lr: 0.02\n",
            "iteration: 223870 loss: 0.0011 lr: 0.02\n",
            "iteration: 223880 loss: 0.0006 lr: 0.02\n",
            "iteration: 223890 loss: 0.0005 lr: 0.02\n",
            "iteration: 223900 loss: 0.0007 lr: 0.02\n",
            "iteration: 223910 loss: 0.0009 lr: 0.02\n",
            "iteration: 223920 loss: 0.0013 lr: 0.02\n",
            "iteration: 223930 loss: 0.0009 lr: 0.02\n",
            "iteration: 223940 loss: 0.0010 lr: 0.02\n",
            "iteration: 223950 loss: 0.0004 lr: 0.02\n",
            "iteration: 223960 loss: 0.0008 lr: 0.02\n",
            "iteration: 223970 loss: 0.0007 lr: 0.02\n",
            "iteration: 223980 loss: 0.0009 lr: 0.02\n",
            "iteration: 223990 loss: 0.0005 lr: 0.02\n",
            "iteration: 224000 loss: 0.0009 lr: 0.02\n",
            "iteration: 224010 loss: 0.0010 lr: 0.02\n",
            "iteration: 224020 loss: 0.0007 lr: 0.02\n",
            "iteration: 224030 loss: 0.0011 lr: 0.02\n",
            "iteration: 224040 loss: 0.0008 lr: 0.02\n",
            "iteration: 224050 loss: 0.0009 lr: 0.02\n",
            "iteration: 224060 loss: 0.0009 lr: 0.02\n",
            "iteration: 224070 loss: 0.0009 lr: 0.02\n",
            "iteration: 224080 loss: 0.0006 lr: 0.02\n",
            "iteration: 224090 loss: 0.0009 lr: 0.02\n",
            "iteration: 224100 loss: 0.0008 lr: 0.02\n",
            "iteration: 224110 loss: 0.0008 lr: 0.02\n",
            "iteration: 224120 loss: 0.0007 lr: 0.02\n",
            "iteration: 224130 loss: 0.0009 lr: 0.02\n",
            "iteration: 224140 loss: 0.0007 lr: 0.02\n",
            "iteration: 224150 loss: 0.0013 lr: 0.02\n",
            "iteration: 224160 loss: 0.0006 lr: 0.02\n",
            "iteration: 224170 loss: 0.0007 lr: 0.02\n",
            "iteration: 224180 loss: 0.0007 lr: 0.02\n",
            "iteration: 224190 loss: 0.0007 lr: 0.02\n",
            "iteration: 224200 loss: 0.0008 lr: 0.02\n",
            "iteration: 224210 loss: 0.0009 lr: 0.02\n",
            "iteration: 224220 loss: 0.0008 lr: 0.02\n",
            "iteration: 224230 loss: 0.0006 lr: 0.02\n",
            "iteration: 224240 loss: 0.0007 lr: 0.02\n",
            "iteration: 224250 loss: 0.0009 lr: 0.02\n",
            "iteration: 224260 loss: 0.0007 lr: 0.02\n",
            "iteration: 224270 loss: 0.0006 lr: 0.02\n",
            "iteration: 224280 loss: 0.0007 lr: 0.02\n",
            "iteration: 224290 loss: 0.0008 lr: 0.02\n",
            "iteration: 224300 loss: 0.0010 lr: 0.02\n",
            "iteration: 224310 loss: 0.0008 lr: 0.02\n",
            "iteration: 224320 loss: 0.0007 lr: 0.02\n",
            "iteration: 224330 loss: 0.0006 lr: 0.02\n",
            "iteration: 224340 loss: 0.0007 lr: 0.02\n",
            "iteration: 224350 loss: 0.0008 lr: 0.02\n",
            "iteration: 224360 loss: 0.0006 lr: 0.02\n",
            "iteration: 224370 loss: 0.0007 lr: 0.02\n",
            "iteration: 224380 loss: 0.0009 lr: 0.02\n",
            "iteration: 224390 loss: 0.0007 lr: 0.02\n",
            "iteration: 224400 loss: 0.0010 lr: 0.02\n",
            "iteration: 224410 loss: 0.0009 lr: 0.02\n",
            "iteration: 224420 loss: 0.0007 lr: 0.02\n",
            "iteration: 224430 loss: 0.0008 lr: 0.02\n",
            "iteration: 224440 loss: 0.0008 lr: 0.02\n",
            "iteration: 224450 loss: 0.0008 lr: 0.02\n",
            "iteration: 224460 loss: 0.0013 lr: 0.02\n",
            "iteration: 224470 loss: 0.0007 lr: 0.02\n",
            "iteration: 224480 loss: 0.0009 lr: 0.02\n",
            "iteration: 224490 loss: 0.0006 lr: 0.02\n",
            "iteration: 224500 loss: 0.0010 lr: 0.02\n",
            "iteration: 224510 loss: 0.0007 lr: 0.02\n",
            "iteration: 224520 loss: 0.0010 lr: 0.02\n",
            "iteration: 224530 loss: 0.0010 lr: 0.02\n",
            "iteration: 224540 loss: 0.0009 lr: 0.02\n",
            "iteration: 224550 loss: 0.0010 lr: 0.02\n",
            "iteration: 224560 loss: 0.0008 lr: 0.02\n",
            "iteration: 224570 loss: 0.0006 lr: 0.02\n",
            "iteration: 224580 loss: 0.0006 lr: 0.02\n",
            "iteration: 224590 loss: 0.0006 lr: 0.02\n",
            "iteration: 224600 loss: 0.0007 lr: 0.02\n",
            "iteration: 224610 loss: 0.0009 lr: 0.02\n",
            "iteration: 224620 loss: 0.0011 lr: 0.02\n",
            "iteration: 224630 loss: 0.0008 lr: 0.02\n",
            "iteration: 224640 loss: 0.0007 lr: 0.02\n",
            "iteration: 224650 loss: 0.0008 lr: 0.02\n",
            "iteration: 224660 loss: 0.0011 lr: 0.02\n",
            "iteration: 224670 loss: 0.0009 lr: 0.02\n",
            "iteration: 224680 loss: 0.0010 lr: 0.02\n",
            "iteration: 224690 loss: 0.0005 lr: 0.02\n",
            "iteration: 224700 loss: 0.0006 lr: 0.02\n",
            "iteration: 224710 loss: 0.0010 lr: 0.02\n",
            "iteration: 224720 loss: 0.0006 lr: 0.02\n",
            "iteration: 224730 loss: 0.0011 lr: 0.02\n",
            "iteration: 224740 loss: 0.0008 lr: 0.02\n",
            "iteration: 224750 loss: 0.0008 lr: 0.02\n",
            "iteration: 224760 loss: 0.0009 lr: 0.02\n",
            "iteration: 224770 loss: 0.0007 lr: 0.02\n",
            "iteration: 224780 loss: 0.0007 lr: 0.02\n",
            "iteration: 224790 loss: 0.0010 lr: 0.02\n",
            "iteration: 224800 loss: 0.0007 lr: 0.02\n",
            "iteration: 224810 loss: 0.0006 lr: 0.02\n",
            "iteration: 224820 loss: 0.0009 lr: 0.02\n",
            "iteration: 224830 loss: 0.0010 lr: 0.02\n",
            "iteration: 224840 loss: 0.0007 lr: 0.02\n",
            "iteration: 224850 loss: 0.0009 lr: 0.02\n",
            "iteration: 224860 loss: 0.0007 lr: 0.02\n",
            "iteration: 224870 loss: 0.0010 lr: 0.02\n",
            "iteration: 224880 loss: 0.0008 lr: 0.02\n",
            "iteration: 224890 loss: 0.0008 lr: 0.02\n",
            "iteration: 224900 loss: 0.0009 lr: 0.02\n",
            "iteration: 224910 loss: 0.0006 lr: 0.02\n",
            "iteration: 224920 loss: 0.0008 lr: 0.02\n",
            "iteration: 224930 loss: 0.0008 lr: 0.02\n",
            "iteration: 224940 loss: 0.0006 lr: 0.02\n",
            "iteration: 224950 loss: 0.0009 lr: 0.02\n",
            "iteration: 224960 loss: 0.0008 lr: 0.02\n",
            "iteration: 224970 loss: 0.0009 lr: 0.02\n",
            "iteration: 224980 loss: 0.0010 lr: 0.02\n",
            "iteration: 224990 loss: 0.0006 lr: 0.02\n",
            "iteration: 225000 loss: 0.0008 lr: 0.02\n",
            "iteration: 225010 loss: 0.0006 lr: 0.02\n",
            "iteration: 225020 loss: 0.0007 lr: 0.02\n",
            "iteration: 225030 loss: 0.0007 lr: 0.02\n",
            "iteration: 225040 loss: 0.0011 lr: 0.02\n",
            "iteration: 225050 loss: 0.0006 lr: 0.02\n",
            "iteration: 225060 loss: 0.0009 lr: 0.02\n",
            "iteration: 225070 loss: 0.0006 lr: 0.02\n",
            "iteration: 225080 loss: 0.0007 lr: 0.02\n",
            "iteration: 225090 loss: 0.0009 lr: 0.02\n",
            "iteration: 225100 loss: 0.0009 lr: 0.02\n",
            "iteration: 225110 loss: 0.0010 lr: 0.02\n",
            "iteration: 225120 loss: 0.0007 lr: 0.02\n",
            "iteration: 225130 loss: 0.0007 lr: 0.02\n",
            "iteration: 225140 loss: 0.0007 lr: 0.02\n",
            "iteration: 225150 loss: 0.0006 lr: 0.02\n",
            "iteration: 225160 loss: 0.0008 lr: 0.02\n",
            "iteration: 225170 loss: 0.0011 lr: 0.02\n",
            "iteration: 225180 loss: 0.0009 lr: 0.02\n",
            "iteration: 225190 loss: 0.0009 lr: 0.02\n",
            "iteration: 225200 loss: 0.0008 lr: 0.02\n",
            "iteration: 225210 loss: 0.0009 lr: 0.02\n",
            "iteration: 225220 loss: 0.0007 lr: 0.02\n",
            "iteration: 225230 loss: 0.0007 lr: 0.02\n",
            "iteration: 225240 loss: 0.0010 lr: 0.02\n",
            "iteration: 225250 loss: 0.0008 lr: 0.02\n",
            "iteration: 225260 loss: 0.0007 lr: 0.02\n",
            "iteration: 225270 loss: 0.0006 lr: 0.02\n",
            "iteration: 225280 loss: 0.0007 lr: 0.02\n",
            "iteration: 225290 loss: 0.0010 lr: 0.02\n",
            "iteration: 225300 loss: 0.0006 lr: 0.02\n",
            "iteration: 225310 loss: 0.0008 lr: 0.02\n",
            "iteration: 225320 loss: 0.0007 lr: 0.02\n",
            "iteration: 225330 loss: 0.0006 lr: 0.02\n",
            "iteration: 225340 loss: 0.0007 lr: 0.02\n",
            "iteration: 225350 loss: 0.0007 lr: 0.02\n",
            "iteration: 225360 loss: 0.0006 lr: 0.02\n",
            "iteration: 225370 loss: 0.0008 lr: 0.02\n",
            "iteration: 225380 loss: 0.0007 lr: 0.02\n",
            "iteration: 225390 loss: 0.0007 lr: 0.02\n",
            "iteration: 225400 loss: 0.0008 lr: 0.02\n",
            "iteration: 225410 loss: 0.0007 lr: 0.02\n",
            "iteration: 225420 loss: 0.0007 lr: 0.02\n",
            "iteration: 225430 loss: 0.0006 lr: 0.02\n",
            "iteration: 225440 loss: 0.0007 lr: 0.02\n",
            "iteration: 225450 loss: 0.0008 lr: 0.02\n",
            "iteration: 225460 loss: 0.0007 lr: 0.02\n",
            "iteration: 225470 loss: 0.0007 lr: 0.02\n",
            "iteration: 225480 loss: 0.0008 lr: 0.02\n",
            "iteration: 225490 loss: 0.0006 lr: 0.02\n",
            "iteration: 225500 loss: 0.0009 lr: 0.02\n",
            "iteration: 225510 loss: 0.0010 lr: 0.02\n",
            "iteration: 225520 loss: 0.0007 lr: 0.02\n",
            "iteration: 225530 loss: 0.0007 lr: 0.02\n",
            "iteration: 225540 loss: 0.0009 lr: 0.02\n",
            "iteration: 225550 loss: 0.0006 lr: 0.02\n",
            "iteration: 225560 loss: 0.0008 lr: 0.02\n",
            "iteration: 225570 loss: 0.0008 lr: 0.02\n",
            "iteration: 225580 loss: 0.0008 lr: 0.02\n",
            "iteration: 225590 loss: 0.0007 lr: 0.02\n",
            "iteration: 225600 loss: 0.0008 lr: 0.02\n",
            "iteration: 225610 loss: 0.0007 lr: 0.02\n",
            "iteration: 225620 loss: 0.0009 lr: 0.02\n",
            "iteration: 225630 loss: 0.0009 lr: 0.02\n",
            "iteration: 225640 loss: 0.0009 lr: 0.02\n",
            "iteration: 225650 loss: 0.0009 lr: 0.02\n",
            "iteration: 225660 loss: 0.0007 lr: 0.02\n",
            "iteration: 225670 loss: 0.0008 lr: 0.02\n",
            "iteration: 225680 loss: 0.0009 lr: 0.02\n",
            "iteration: 225690 loss: 0.0005 lr: 0.02\n",
            "iteration: 225700 loss: 0.0008 lr: 0.02\n",
            "iteration: 225710 loss: 0.0007 lr: 0.02\n",
            "iteration: 225720 loss: 0.0007 lr: 0.02\n",
            "iteration: 225730 loss: 0.0009 lr: 0.02\n",
            "iteration: 225740 loss: 0.0006 lr: 0.02\n",
            "iteration: 225750 loss: 0.0007 lr: 0.02\n",
            "iteration: 225760 loss: 0.0009 lr: 0.02\n",
            "iteration: 225770 loss: 0.0007 lr: 0.02\n",
            "iteration: 225780 loss: 0.0007 lr: 0.02\n",
            "iteration: 225790 loss: 0.0010 lr: 0.02\n",
            "iteration: 225800 loss: 0.0008 lr: 0.02\n",
            "iteration: 225810 loss: 0.0007 lr: 0.02\n",
            "iteration: 225820 loss: 0.0006 lr: 0.02\n",
            "iteration: 225830 loss: 0.0009 lr: 0.02\n",
            "iteration: 225840 loss: 0.0006 lr: 0.02\n",
            "iteration: 225850 loss: 0.0006 lr: 0.02\n",
            "iteration: 225860 loss: 0.0007 lr: 0.02\n",
            "iteration: 225870 loss: 0.0007 lr: 0.02\n",
            "iteration: 225880 loss: 0.0008 lr: 0.02\n",
            "iteration: 225890 loss: 0.0006 lr: 0.02\n",
            "iteration: 225900 loss: 0.0005 lr: 0.02\n",
            "iteration: 225910 loss: 0.0009 lr: 0.02\n",
            "iteration: 225920 loss: 0.0011 lr: 0.02\n",
            "iteration: 225930 loss: 0.0006 lr: 0.02\n",
            "iteration: 225940 loss: 0.0008 lr: 0.02\n",
            "iteration: 225950 loss: 0.0006 lr: 0.02\n",
            "iteration: 225960 loss: 0.0006 lr: 0.02\n",
            "iteration: 225970 loss: 0.0007 lr: 0.02\n",
            "iteration: 225980 loss: 0.0008 lr: 0.02\n",
            "iteration: 225990 loss: 0.0011 lr: 0.02\n",
            "iteration: 226000 loss: 0.0006 lr: 0.02\n",
            "iteration: 226010 loss: 0.0009 lr: 0.02\n",
            "iteration: 226020 loss: 0.0010 lr: 0.02\n",
            "iteration: 226030 loss: 0.0010 lr: 0.02\n",
            "iteration: 226040 loss: 0.0008 lr: 0.02\n",
            "iteration: 226050 loss: 0.0010 lr: 0.02\n",
            "iteration: 226060 loss: 0.0005 lr: 0.02\n",
            "iteration: 226070 loss: 0.0008 lr: 0.02\n",
            "iteration: 226080 loss: 0.0007 lr: 0.02\n",
            "iteration: 226090 loss: 0.0005 lr: 0.02\n",
            "iteration: 226100 loss: 0.0008 lr: 0.02\n",
            "iteration: 226110 loss: 0.0011 lr: 0.02\n",
            "iteration: 226120 loss: 0.0008 lr: 0.02\n",
            "iteration: 226130 loss: 0.0009 lr: 0.02\n",
            "iteration: 226140 loss: 0.0008 lr: 0.02\n",
            "iteration: 226150 loss: 0.0008 lr: 0.02\n",
            "iteration: 226160 loss: 0.0009 lr: 0.02\n",
            "iteration: 226170 loss: 0.0009 lr: 0.02\n",
            "iteration: 226180 loss: 0.0008 lr: 0.02\n",
            "iteration: 226190 loss: 0.0010 lr: 0.02\n",
            "iteration: 226200 loss: 0.0018 lr: 0.02\n",
            "iteration: 226210 loss: 0.0009 lr: 0.02\n",
            "iteration: 226220 loss: 0.0009 lr: 0.02\n",
            "iteration: 226230 loss: 0.0006 lr: 0.02\n",
            "iteration: 226240 loss: 0.0006 lr: 0.02\n",
            "iteration: 226250 loss: 0.0007 lr: 0.02\n",
            "iteration: 226260 loss: 0.0008 lr: 0.02\n",
            "iteration: 226270 loss: 0.0007 lr: 0.02\n",
            "iteration: 226280 loss: 0.0008 lr: 0.02\n",
            "iteration: 226290 loss: 0.0007 lr: 0.02\n",
            "iteration: 226300 loss: 0.0007 lr: 0.02\n",
            "iteration: 226310 loss: 0.0008 lr: 0.02\n",
            "iteration: 226320 loss: 0.0008 lr: 0.02\n",
            "iteration: 226330 loss: 0.0008 lr: 0.02\n",
            "iteration: 226340 loss: 0.0008 lr: 0.02\n",
            "iteration: 226350 loss: 0.0007 lr: 0.02\n",
            "iteration: 226360 loss: 0.0007 lr: 0.02\n",
            "iteration: 226370 loss: 0.0007 lr: 0.02\n",
            "iteration: 226380 loss: 0.0011 lr: 0.02\n",
            "iteration: 226390 loss: 0.0008 lr: 0.02\n",
            "iteration: 226400 loss: 0.0006 lr: 0.02\n",
            "iteration: 226410 loss: 0.0006 lr: 0.02\n",
            "iteration: 226420 loss: 0.0008 lr: 0.02\n",
            "iteration: 226430 loss: 0.0007 lr: 0.02\n",
            "iteration: 226440 loss: 0.0007 lr: 0.02\n",
            "iteration: 226450 loss: 0.0007 lr: 0.02\n",
            "iteration: 226460 loss: 0.0010 lr: 0.02\n",
            "iteration: 226470 loss: 0.0006 lr: 0.02\n",
            "iteration: 226480 loss: 0.0006 lr: 0.02\n",
            "iteration: 226490 loss: 0.0007 lr: 0.02\n",
            "iteration: 226500 loss: 0.0007 lr: 0.02\n",
            "iteration: 226510 loss: 0.0008 lr: 0.02\n",
            "iteration: 226520 loss: 0.0007 lr: 0.02\n",
            "iteration: 226530 loss: 0.0013 lr: 0.02\n",
            "iteration: 226540 loss: 0.0008 lr: 0.02\n",
            "iteration: 226550 loss: 0.0009 lr: 0.02\n",
            "iteration: 226560 loss: 0.0007 lr: 0.02\n",
            "iteration: 226570 loss: 0.0006 lr: 0.02\n",
            "iteration: 226580 loss: 0.0006 lr: 0.02\n",
            "iteration: 226590 loss: 0.0007 lr: 0.02\n",
            "iteration: 226600 loss: 0.0012 lr: 0.02\n",
            "iteration: 226610 loss: 0.0008 lr: 0.02\n",
            "iteration: 226620 loss: 0.0008 lr: 0.02\n",
            "iteration: 226630 loss: 0.0006 lr: 0.02\n",
            "iteration: 226640 loss: 0.0007 lr: 0.02\n",
            "iteration: 226650 loss: 0.0009 lr: 0.02\n",
            "iteration: 226660 loss: 0.0007 lr: 0.02\n",
            "iteration: 226670 loss: 0.0011 lr: 0.02\n",
            "iteration: 226680 loss: 0.0005 lr: 0.02\n",
            "iteration: 226690 loss: 0.0012 lr: 0.02\n",
            "iteration: 226700 loss: 0.0008 lr: 0.02\n",
            "iteration: 226710 loss: 0.0011 lr: 0.02\n",
            "iteration: 226720 loss: 0.0008 lr: 0.02\n",
            "iteration: 226730 loss: 0.0008 lr: 0.02\n",
            "iteration: 226740 loss: 0.0008 lr: 0.02\n",
            "iteration: 226750 loss: 0.0009 lr: 0.02\n",
            "iteration: 226760 loss: 0.0006 lr: 0.02\n",
            "iteration: 226770 loss: 0.0011 lr: 0.02\n",
            "iteration: 226780 loss: 0.0007 lr: 0.02\n",
            "iteration: 226790 loss: 0.0007 lr: 0.02\n",
            "iteration: 226800 loss: 0.0008 lr: 0.02\n",
            "iteration: 226810 loss: 0.0007 lr: 0.02\n",
            "iteration: 226820 loss: 0.0008 lr: 0.02\n",
            "iteration: 226830 loss: 0.0007 lr: 0.02\n",
            "iteration: 226840 loss: 0.0007 lr: 0.02\n",
            "iteration: 226850 loss: 0.0006 lr: 0.02\n",
            "iteration: 226860 loss: 0.0008 lr: 0.02\n",
            "iteration: 226870 loss: 0.0007 lr: 0.02\n",
            "iteration: 226880 loss: 0.0011 lr: 0.02\n",
            "iteration: 226890 loss: 0.0008 lr: 0.02\n",
            "iteration: 226900 loss: 0.0011 lr: 0.02\n",
            "iteration: 226910 loss: 0.0006 lr: 0.02\n",
            "iteration: 226920 loss: 0.0005 lr: 0.02\n",
            "iteration: 226930 loss: 0.0010 lr: 0.02\n",
            "iteration: 226940 loss: 0.0010 lr: 0.02\n",
            "iteration: 226950 loss: 0.0012 lr: 0.02\n",
            "iteration: 226960 loss: 0.0006 lr: 0.02\n",
            "iteration: 226970 loss: 0.0007 lr: 0.02\n",
            "iteration: 226980 loss: 0.0009 lr: 0.02\n",
            "iteration: 226990 loss: 0.0007 lr: 0.02\n",
            "iteration: 227000 loss: 0.0011 lr: 0.02\n",
            "iteration: 227010 loss: 0.0006 lr: 0.02\n",
            "iteration: 227020 loss: 0.0011 lr: 0.02\n",
            "iteration: 227030 loss: 0.0008 lr: 0.02\n",
            "iteration: 227040 loss: 0.0006 lr: 0.02\n",
            "iteration: 227050 loss: 0.0006 lr: 0.02\n",
            "iteration: 227060 loss: 0.0010 lr: 0.02\n",
            "iteration: 227070 loss: 0.0007 lr: 0.02\n",
            "iteration: 227080 loss: 0.0010 lr: 0.02\n",
            "iteration: 227090 loss: 0.0010 lr: 0.02\n",
            "iteration: 227100 loss: 0.0009 lr: 0.02\n",
            "iteration: 227110 loss: 0.0011 lr: 0.02\n",
            "iteration: 227120 loss: 0.0007 lr: 0.02\n",
            "iteration: 227130 loss: 0.0009 lr: 0.02\n",
            "iteration: 227140 loss: 0.0012 lr: 0.02\n",
            "iteration: 227150 loss: 0.0006 lr: 0.02\n",
            "iteration: 227160 loss: 0.0006 lr: 0.02\n",
            "iteration: 227170 loss: 0.0006 lr: 0.02\n",
            "iteration: 227180 loss: 0.0007 lr: 0.02\n",
            "iteration: 227190 loss: 0.0006 lr: 0.02\n",
            "iteration: 227200 loss: 0.0008 lr: 0.02\n",
            "iteration: 227210 loss: 0.0009 lr: 0.02\n",
            "iteration: 227220 loss: 0.0010 lr: 0.02\n",
            "iteration: 227230 loss: 0.0007 lr: 0.02\n",
            "iteration: 227240 loss: 0.0007 lr: 0.02\n",
            "iteration: 227250 loss: 0.0009 lr: 0.02\n",
            "iteration: 227260 loss: 0.0008 lr: 0.02\n",
            "iteration: 227270 loss: 0.0007 lr: 0.02\n",
            "iteration: 227280 loss: 0.0008 lr: 0.02\n",
            "iteration: 227290 loss: 0.0005 lr: 0.02\n",
            "iteration: 227300 loss: 0.0008 lr: 0.02\n",
            "iteration: 227310 loss: 0.0008 lr: 0.02\n",
            "iteration: 227320 loss: 0.0007 lr: 0.02\n",
            "iteration: 227330 loss: 0.0008 lr: 0.02\n",
            "iteration: 227340 loss: 0.0010 lr: 0.02\n",
            "iteration: 227350 loss: 0.0011 lr: 0.02\n",
            "iteration: 227360 loss: 0.0008 lr: 0.02\n",
            "iteration: 227370 loss: 0.0007 lr: 0.02\n",
            "iteration: 227380 loss: 0.0006 lr: 0.02\n",
            "iteration: 227390 loss: 0.0006 lr: 0.02\n",
            "iteration: 227400 loss: 0.0009 lr: 0.02\n",
            "iteration: 227410 loss: 0.0011 lr: 0.02\n",
            "iteration: 227420 loss: 0.0009 lr: 0.02\n",
            "iteration: 227430 loss: 0.0008 lr: 0.02\n",
            "iteration: 227440 loss: 0.0007 lr: 0.02\n",
            "iteration: 227450 loss: 0.0009 lr: 0.02\n",
            "iteration: 227460 loss: 0.0008 lr: 0.02\n",
            "iteration: 227470 loss: 0.0007 lr: 0.02\n",
            "iteration: 227480 loss: 0.0007 lr: 0.02\n",
            "iteration: 227490 loss: 0.0010 lr: 0.02\n",
            "iteration: 227500 loss: 0.0008 lr: 0.02\n",
            "iteration: 227510 loss: 0.0008 lr: 0.02\n",
            "iteration: 227520 loss: 0.0008 lr: 0.02\n",
            "iteration: 227530 loss: 0.0008 lr: 0.02\n",
            "iteration: 227540 loss: 0.0007 lr: 0.02\n",
            "iteration: 227550 loss: 0.0008 lr: 0.02\n",
            "iteration: 227560 loss: 0.0012 lr: 0.02\n",
            "iteration: 227570 loss: 0.0014 lr: 0.02\n",
            "iteration: 227580 loss: 0.0012 lr: 0.02\n",
            "iteration: 227590 loss: 0.0007 lr: 0.02\n",
            "iteration: 227600 loss: 0.0012 lr: 0.02\n",
            "iteration: 227610 loss: 0.0009 lr: 0.02\n",
            "iteration: 227620 loss: 0.0008 lr: 0.02\n",
            "iteration: 227630 loss: 0.0008 lr: 0.02\n",
            "iteration: 227640 loss: 0.0008 lr: 0.02\n",
            "iteration: 227650 loss: 0.0010 lr: 0.02\n",
            "iteration: 227660 loss: 0.0009 lr: 0.02\n",
            "iteration: 227670 loss: 0.0007 lr: 0.02\n",
            "iteration: 227680 loss: 0.0009 lr: 0.02\n",
            "iteration: 227690 loss: 0.0009 lr: 0.02\n",
            "iteration: 227700 loss: 0.0007 lr: 0.02\n",
            "iteration: 227710 loss: 0.0009 lr: 0.02\n",
            "iteration: 227720 loss: 0.0006 lr: 0.02\n",
            "iteration: 227730 loss: 0.0009 lr: 0.02\n",
            "iteration: 227740 loss: 0.0006 lr: 0.02\n",
            "iteration: 227750 loss: 0.0008 lr: 0.02\n",
            "iteration: 227760 loss: 0.0005 lr: 0.02\n",
            "iteration: 227770 loss: 0.0007 lr: 0.02\n",
            "iteration: 227780 loss: 0.0006 lr: 0.02\n",
            "iteration: 227790 loss: 0.0009 lr: 0.02\n",
            "iteration: 227800 loss: 0.0010 lr: 0.02\n",
            "iteration: 227810 loss: 0.0014 lr: 0.02\n",
            "iteration: 227820 loss: 0.0010 lr: 0.02\n",
            "iteration: 227830 loss: 0.0008 lr: 0.02\n",
            "iteration: 227840 loss: 0.0009 lr: 0.02\n",
            "iteration: 227850 loss: 0.0007 lr: 0.02\n",
            "iteration: 227860 loss: 0.0009 lr: 0.02\n",
            "iteration: 227870 loss: 0.0008 lr: 0.02\n",
            "iteration: 227880 loss: 0.0014 lr: 0.02\n",
            "iteration: 227890 loss: 0.0007 lr: 0.02\n",
            "iteration: 227900 loss: 0.0012 lr: 0.02\n",
            "iteration: 227910 loss: 0.0006 lr: 0.02\n",
            "iteration: 227920 loss: 0.0007 lr: 0.02\n",
            "iteration: 227930 loss: 0.0007 lr: 0.02\n",
            "iteration: 227940 loss: 0.0007 lr: 0.02\n",
            "iteration: 227950 loss: 0.0009 lr: 0.02\n",
            "iteration: 227960 loss: 0.0006 lr: 0.02\n",
            "iteration: 227970 loss: 0.0010 lr: 0.02\n",
            "iteration: 227980 loss: 0.0008 lr: 0.02\n",
            "iteration: 227990 loss: 0.0009 lr: 0.02\n",
            "iteration: 228000 loss: 0.0007 lr: 0.02\n",
            "iteration: 228010 loss: 0.0010 lr: 0.02\n",
            "iteration: 228020 loss: 0.0006 lr: 0.02\n",
            "iteration: 228030 loss: 0.0006 lr: 0.02\n",
            "iteration: 228040 loss: 0.0008 lr: 0.02\n",
            "iteration: 228050 loss: 0.0006 lr: 0.02\n",
            "iteration: 228060 loss: 0.0006 lr: 0.02\n",
            "iteration: 228070 loss: 0.0008 lr: 0.02\n",
            "iteration: 228080 loss: 0.0007 lr: 0.02\n",
            "iteration: 228090 loss: 0.0009 lr: 0.02\n",
            "iteration: 228100 loss: 0.0007 lr: 0.02\n",
            "iteration: 228110 loss: 0.0008 lr: 0.02\n",
            "iteration: 228120 loss: 0.0007 lr: 0.02\n",
            "iteration: 228130 loss: 0.0005 lr: 0.02\n",
            "iteration: 228140 loss: 0.0009 lr: 0.02\n",
            "iteration: 228150 loss: 0.0008 lr: 0.02\n",
            "iteration: 228160 loss: 0.0008 lr: 0.02\n",
            "iteration: 228170 loss: 0.0007 lr: 0.02\n",
            "iteration: 228180 loss: 0.0009 lr: 0.02\n",
            "iteration: 228190 loss: 0.0009 lr: 0.02\n",
            "iteration: 228200 loss: 0.0008 lr: 0.02\n",
            "iteration: 228210 loss: 0.0011 lr: 0.02\n",
            "iteration: 228220 loss: 0.0008 lr: 0.02\n",
            "iteration: 228230 loss: 0.0008 lr: 0.02\n",
            "iteration: 228240 loss: 0.0008 lr: 0.02\n",
            "iteration: 228250 loss: 0.0007 lr: 0.02\n",
            "iteration: 228260 loss: 0.0007 lr: 0.02\n",
            "iteration: 228270 loss: 0.0008 lr: 0.02\n",
            "iteration: 228280 loss: 0.0008 lr: 0.02\n",
            "iteration: 228290 loss: 0.0009 lr: 0.02\n",
            "iteration: 228300 loss: 0.0008 lr: 0.02\n",
            "iteration: 228310 loss: 0.0009 lr: 0.02\n",
            "iteration: 228320 loss: 0.0008 lr: 0.02\n",
            "iteration: 228330 loss: 0.0006 lr: 0.02\n",
            "iteration: 228340 loss: 0.0006 lr: 0.02\n",
            "iteration: 228350 loss: 0.0010 lr: 0.02\n",
            "iteration: 228360 loss: 0.0008 lr: 0.02\n",
            "iteration: 228370 loss: 0.0006 lr: 0.02\n",
            "iteration: 228380 loss: 0.0009 lr: 0.02\n",
            "iteration: 228390 loss: 0.0006 lr: 0.02\n",
            "iteration: 228400 loss: 0.0007 lr: 0.02\n",
            "iteration: 228410 loss: 0.0012 lr: 0.02\n",
            "iteration: 228420 loss: 0.0006 lr: 0.02\n",
            "iteration: 228430 loss: 0.0009 lr: 0.02\n",
            "iteration: 228440 loss: 0.0007 lr: 0.02\n",
            "iteration: 228450 loss: 0.0010 lr: 0.02\n",
            "iteration: 228460 loss: 0.0008 lr: 0.02\n",
            "iteration: 228470 loss: 0.0008 lr: 0.02\n",
            "iteration: 228480 loss: 0.0007 lr: 0.02\n",
            "iteration: 228490 loss: 0.0006 lr: 0.02\n",
            "iteration: 228500 loss: 0.0005 lr: 0.02\n",
            "iteration: 228510 loss: 0.0009 lr: 0.02\n",
            "iteration: 228520 loss: 0.0007 lr: 0.02\n",
            "iteration: 228530 loss: 0.0008 lr: 0.02\n",
            "iteration: 228540 loss: 0.0008 lr: 0.02\n",
            "iteration: 228550 loss: 0.0007 lr: 0.02\n",
            "iteration: 228560 loss: 0.0007 lr: 0.02\n",
            "iteration: 228570 loss: 0.0005 lr: 0.02\n",
            "iteration: 228580 loss: 0.0006 lr: 0.02\n",
            "iteration: 228590 loss: 0.0006 lr: 0.02\n",
            "iteration: 228600 loss: 0.0012 lr: 0.02\n",
            "iteration: 228610 loss: 0.0009 lr: 0.02\n",
            "iteration: 228620 loss: 0.0009 lr: 0.02\n",
            "iteration: 228630 loss: 0.0008 lr: 0.02\n",
            "iteration: 228640 loss: 0.0010 lr: 0.02\n",
            "iteration: 228650 loss: 0.0006 lr: 0.02\n",
            "iteration: 228660 loss: 0.0011 lr: 0.02\n",
            "iteration: 228670 loss: 0.0010 lr: 0.02\n",
            "iteration: 228680 loss: 0.0005 lr: 0.02\n",
            "iteration: 228690 loss: 0.0012 lr: 0.02\n",
            "iteration: 228700 loss: 0.0009 lr: 0.02\n",
            "iteration: 228710 loss: 0.0010 lr: 0.02\n",
            "iteration: 228720 loss: 0.0009 lr: 0.02\n",
            "iteration: 228730 loss: 0.0006 lr: 0.02\n",
            "iteration: 228740 loss: 0.0006 lr: 0.02\n",
            "iteration: 228750 loss: 0.0007 lr: 0.02\n",
            "iteration: 228760 loss: 0.0006 lr: 0.02\n",
            "iteration: 228770 loss: 0.0007 lr: 0.02\n",
            "iteration: 228780 loss: 0.0010 lr: 0.02\n",
            "iteration: 228790 loss: 0.0008 lr: 0.02\n",
            "iteration: 228800 loss: 0.0006 lr: 0.02\n",
            "iteration: 228810 loss: 0.0010 lr: 0.02\n",
            "iteration: 228820 loss: 0.0007 lr: 0.02\n",
            "iteration: 228830 loss: 0.0010 lr: 0.02\n",
            "iteration: 228840 loss: 0.0009 lr: 0.02\n",
            "iteration: 228850 loss: 0.0008 lr: 0.02\n",
            "iteration: 228860 loss: 0.0009 lr: 0.02\n",
            "iteration: 228870 loss: 0.0007 lr: 0.02\n",
            "iteration: 228880 loss: 0.0009 lr: 0.02\n",
            "iteration: 228890 loss: 0.0007 lr: 0.02\n",
            "iteration: 228900 loss: 0.0009 lr: 0.02\n",
            "iteration: 228910 loss: 0.0009 lr: 0.02\n",
            "iteration: 228920 loss: 0.0008 lr: 0.02\n",
            "iteration: 228930 loss: 0.0007 lr: 0.02\n",
            "iteration: 228940 loss: 0.0008 lr: 0.02\n",
            "iteration: 228950 loss: 0.0006 lr: 0.02\n",
            "iteration: 228960 loss: 0.0006 lr: 0.02\n",
            "iteration: 228970 loss: 0.0011 lr: 0.02\n",
            "iteration: 228980 loss: 0.0009 lr: 0.02\n",
            "iteration: 228990 loss: 0.0007 lr: 0.02\n",
            "iteration: 229000 loss: 0.0010 lr: 0.02\n",
            "iteration: 229010 loss: 0.0008 lr: 0.02\n",
            "iteration: 229020 loss: 0.0007 lr: 0.02\n",
            "iteration: 229030 loss: 0.0007 lr: 0.02\n",
            "iteration: 229040 loss: 0.0009 lr: 0.02\n",
            "iteration: 229050 loss: 0.0008 lr: 0.02\n",
            "iteration: 229060 loss: 0.0011 lr: 0.02\n",
            "iteration: 229070 loss: 0.0007 lr: 0.02\n",
            "iteration: 229080 loss: 0.0008 lr: 0.02\n",
            "iteration: 229090 loss: 0.0011 lr: 0.02\n",
            "iteration: 229100 loss: 0.0009 lr: 0.02\n",
            "iteration: 229110 loss: 0.0009 lr: 0.02\n",
            "iteration: 229120 loss: 0.0012 lr: 0.02\n",
            "iteration: 229130 loss: 0.0007 lr: 0.02\n",
            "iteration: 229140 loss: 0.0006 lr: 0.02\n",
            "iteration: 229150 loss: 0.0012 lr: 0.02\n",
            "iteration: 229160 loss: 0.0007 lr: 0.02\n",
            "iteration: 229170 loss: 0.0011 lr: 0.02\n",
            "iteration: 229180 loss: 0.0009 lr: 0.02\n",
            "iteration: 229190 loss: 0.0010 lr: 0.02\n",
            "iteration: 229200 loss: 0.0007 lr: 0.02\n",
            "iteration: 229210 loss: 0.0006 lr: 0.02\n",
            "iteration: 229220 loss: 0.0006 lr: 0.02\n",
            "iteration: 229230 loss: 0.0010 lr: 0.02\n",
            "iteration: 229240 loss: 0.0008 lr: 0.02\n",
            "iteration: 229250 loss: 0.0007 lr: 0.02\n",
            "iteration: 229260 loss: 0.0007 lr: 0.02\n",
            "iteration: 229270 loss: 0.0005 lr: 0.02\n",
            "iteration: 229280 loss: 0.0010 lr: 0.02\n",
            "iteration: 229290 loss: 0.0007 lr: 0.02\n",
            "iteration: 229300 loss: 0.0007 lr: 0.02\n",
            "iteration: 229310 loss: 0.0008 lr: 0.02\n",
            "iteration: 229320 loss: 0.0010 lr: 0.02\n",
            "iteration: 229330 loss: 0.0008 lr: 0.02\n",
            "iteration: 229340 loss: 0.0009 lr: 0.02\n",
            "iteration: 229350 loss: 0.0006 lr: 0.02\n",
            "iteration: 229360 loss: 0.0007 lr: 0.02\n",
            "iteration: 229370 loss: 0.0008 lr: 0.02\n",
            "iteration: 229380 loss: 0.0006 lr: 0.02\n",
            "iteration: 229390 loss: 0.0009 lr: 0.02\n",
            "iteration: 229400 loss: 0.0008 lr: 0.02\n",
            "iteration: 229410 loss: 0.0010 lr: 0.02\n",
            "iteration: 229420 loss: 0.0008 lr: 0.02\n",
            "iteration: 229430 loss: 0.0007 lr: 0.02\n",
            "iteration: 229440 loss: 0.0006 lr: 0.02\n",
            "iteration: 229450 loss: 0.0009 lr: 0.02\n",
            "iteration: 229460 loss: 0.0007 lr: 0.02\n",
            "iteration: 229470 loss: 0.0010 lr: 0.02\n",
            "iteration: 229480 loss: 0.0008 lr: 0.02\n",
            "iteration: 229490 loss: 0.0010 lr: 0.02\n",
            "iteration: 229500 loss: 0.0008 lr: 0.02\n",
            "iteration: 229510 loss: 0.0008 lr: 0.02\n",
            "iteration: 229520 loss: 0.0009 lr: 0.02\n",
            "iteration: 229530 loss: 0.0009 lr: 0.02\n",
            "iteration: 229540 loss: 0.0008 lr: 0.02\n",
            "iteration: 229550 loss: 0.0009 lr: 0.02\n",
            "iteration: 229560 loss: 0.0009 lr: 0.02\n",
            "iteration: 229570 loss: 0.0009 lr: 0.02\n",
            "iteration: 229580 loss: 0.0008 lr: 0.02\n",
            "iteration: 229590 loss: 0.0009 lr: 0.02\n",
            "iteration: 229600 loss: 0.0007 lr: 0.02\n",
            "iteration: 229610 loss: 0.0009 lr: 0.02\n",
            "iteration: 229620 loss: 0.0008 lr: 0.02\n",
            "iteration: 229630 loss: 0.0008 lr: 0.02\n",
            "iteration: 229640 loss: 0.0007 lr: 0.02\n",
            "iteration: 229650 loss: 0.0009 lr: 0.02\n",
            "iteration: 229660 loss: 0.0009 lr: 0.02\n",
            "iteration: 229670 loss: 0.0008 lr: 0.02\n",
            "iteration: 229680 loss: 0.0008 lr: 0.02\n",
            "iteration: 229690 loss: 0.0008 lr: 0.02\n",
            "iteration: 229700 loss: 0.0008 lr: 0.02\n",
            "iteration: 229710 loss: 0.0010 lr: 0.02\n",
            "iteration: 229720 loss: 0.0010 lr: 0.02\n",
            "iteration: 229730 loss: 0.0009 lr: 0.02\n",
            "iteration: 229740 loss: 0.0008 lr: 0.02\n",
            "iteration: 229750 loss: 0.0007 lr: 0.02\n",
            "iteration: 229760 loss: 0.0008 lr: 0.02\n",
            "iteration: 229770 loss: 0.0008 lr: 0.02\n",
            "iteration: 229780 loss: 0.0008 lr: 0.02\n",
            "iteration: 229790 loss: 0.0012 lr: 0.02\n",
            "iteration: 229800 loss: 0.0009 lr: 0.02\n",
            "iteration: 229810 loss: 0.0007 lr: 0.02\n",
            "iteration: 229820 loss: 0.0007 lr: 0.02\n",
            "iteration: 229830 loss: 0.0006 lr: 0.02\n",
            "iteration: 229840 loss: 0.0009 lr: 0.02\n",
            "iteration: 229850 loss: 0.0012 lr: 0.02\n",
            "iteration: 229860 loss: 0.0010 lr: 0.02\n",
            "iteration: 229870 loss: 0.0009 lr: 0.02\n",
            "iteration: 229880 loss: 0.0008 lr: 0.02\n",
            "iteration: 229890 loss: 0.0009 lr: 0.02\n",
            "iteration: 229900 loss: 0.0006 lr: 0.02\n",
            "iteration: 229910 loss: 0.0009 lr: 0.02\n",
            "iteration: 229920 loss: 0.0009 lr: 0.02\n",
            "iteration: 229930 loss: 0.0008 lr: 0.02\n",
            "iteration: 229940 loss: 0.0013 lr: 0.02\n",
            "iteration: 229950 loss: 0.0011 lr: 0.02\n",
            "iteration: 229960 loss: 0.0008 lr: 0.02\n",
            "iteration: 229970 loss: 0.0013 lr: 0.02\n",
            "iteration: 229980 loss: 0.0007 lr: 0.02\n",
            "iteration: 229990 loss: 0.0008 lr: 0.02\n",
            "iteration: 230000 loss: 0.0008 lr: 0.02\n",
            "iteration: 230010 loss: 0.0014 lr: 0.02\n",
            "iteration: 230020 loss: 0.0010 lr: 0.02\n",
            "iteration: 230030 loss: 0.0009 lr: 0.02\n",
            "iteration: 230040 loss: 0.0008 lr: 0.02\n",
            "iteration: 230050 loss: 0.0008 lr: 0.02\n",
            "iteration: 230060 loss: 0.0007 lr: 0.02\n",
            "iteration: 230070 loss: 0.0006 lr: 0.02\n",
            "iteration: 230080 loss: 0.0005 lr: 0.02\n",
            "iteration: 230090 loss: 0.0006 lr: 0.02\n",
            "iteration: 230100 loss: 0.0006 lr: 0.02\n",
            "iteration: 230110 loss: 0.0009 lr: 0.02\n",
            "iteration: 230120 loss: 0.0008 lr: 0.02\n",
            "iteration: 230130 loss: 0.0004 lr: 0.02\n",
            "iteration: 230140 loss: 0.0007 lr: 0.02\n",
            "iteration: 230150 loss: 0.0005 lr: 0.02\n",
            "iteration: 230160 loss: 0.0006 lr: 0.02\n",
            "iteration: 230170 loss: 0.0008 lr: 0.02\n",
            "iteration: 230180 loss: 0.0006 lr: 0.02\n",
            "iteration: 230190 loss: 0.0008 lr: 0.02\n",
            "iteration: 230200 loss: 0.0007 lr: 0.02\n",
            "iteration: 230210 loss: 0.0008 lr: 0.02\n",
            "iteration: 230220 loss: 0.0007 lr: 0.02\n",
            "iteration: 230230 loss: 0.0010 lr: 0.02\n",
            "iteration: 230240 loss: 0.0009 lr: 0.02\n",
            "iteration: 230250 loss: 0.0006 lr: 0.02\n",
            "iteration: 230260 loss: 0.0006 lr: 0.02\n",
            "iteration: 230270 loss: 0.0010 lr: 0.02\n",
            "iteration: 230280 loss: 0.0006 lr: 0.02\n",
            "iteration: 230290 loss: 0.0007 lr: 0.02\n",
            "iteration: 230300 loss: 0.0007 lr: 0.02\n",
            "iteration: 230310 loss: 0.0008 lr: 0.02\n",
            "iteration: 230320 loss: 0.0007 lr: 0.02\n",
            "iteration: 230330 loss: 0.0007 lr: 0.02\n",
            "iteration: 230340 loss: 0.0006 lr: 0.02\n",
            "iteration: 230350 loss: 0.0007 lr: 0.02\n",
            "iteration: 230360 loss: 0.0011 lr: 0.02\n",
            "iteration: 230370 loss: 0.0009 lr: 0.02\n",
            "iteration: 230380 loss: 0.0007 lr: 0.02\n",
            "iteration: 230390 loss: 0.0007 lr: 0.02\n",
            "iteration: 230400 loss: 0.0007 lr: 0.02\n",
            "iteration: 230410 loss: 0.0008 lr: 0.02\n",
            "iteration: 230420 loss: 0.0007 lr: 0.02\n",
            "iteration: 230430 loss: 0.0009 lr: 0.02\n",
            "iteration: 230440 loss: 0.0007 lr: 0.02\n",
            "iteration: 230450 loss: 0.0009 lr: 0.02\n",
            "iteration: 230460 loss: 0.0007 lr: 0.02\n",
            "iteration: 230470 loss: 0.0008 lr: 0.02\n",
            "iteration: 230480 loss: 0.0008 lr: 0.02\n",
            "iteration: 230490 loss: 0.0008 lr: 0.02\n",
            "iteration: 230500 loss: 0.0007 lr: 0.02\n",
            "iteration: 230510 loss: 0.0006 lr: 0.02\n",
            "iteration: 230520 loss: 0.0007 lr: 0.02\n",
            "iteration: 230530 loss: 0.0004 lr: 0.02\n",
            "iteration: 230540 loss: 0.0008 lr: 0.02\n",
            "iteration: 230550 loss: 0.0009 lr: 0.02\n",
            "iteration: 230560 loss: 0.0006 lr: 0.02\n",
            "iteration: 230570 loss: 0.0009 lr: 0.02\n",
            "iteration: 230580 loss: 0.0007 lr: 0.02\n",
            "iteration: 230590 loss: 0.0007 lr: 0.02\n",
            "iteration: 230600 loss: 0.0009 lr: 0.02\n",
            "iteration: 230610 loss: 0.0012 lr: 0.02\n",
            "iteration: 230620 loss: 0.0007 lr: 0.02\n",
            "iteration: 230630 loss: 0.0008 lr: 0.02\n",
            "iteration: 230640 loss: 0.0008 lr: 0.02\n",
            "iteration: 230650 loss: 0.0006 lr: 0.02\n",
            "iteration: 230660 loss: 0.0008 lr: 0.02\n",
            "iteration: 230670 loss: 0.0007 lr: 0.02\n",
            "iteration: 230680 loss: 0.0006 lr: 0.02\n",
            "iteration: 230690 loss: 0.0007 lr: 0.02\n",
            "iteration: 230700 loss: 0.0008 lr: 0.02\n",
            "iteration: 230710 loss: 0.0009 lr: 0.02\n",
            "iteration: 230720 loss: 0.0010 lr: 0.02\n",
            "iteration: 230730 loss: 0.0008 lr: 0.02\n",
            "iteration: 230740 loss: 0.0007 lr: 0.02\n",
            "iteration: 230750 loss: 0.0008 lr: 0.02\n",
            "iteration: 230760 loss: 0.0007 lr: 0.02\n",
            "iteration: 230770 loss: 0.0007 lr: 0.02\n",
            "iteration: 230780 loss: 0.0007 lr: 0.02\n",
            "iteration: 230790 loss: 0.0011 lr: 0.02\n",
            "iteration: 230800 loss: 0.0008 lr: 0.02\n",
            "iteration: 230810 loss: 0.0007 lr: 0.02\n",
            "iteration: 230820 loss: 0.0007 lr: 0.02\n",
            "iteration: 230830 loss: 0.0008 lr: 0.02\n",
            "iteration: 230840 loss: 0.0007 lr: 0.02\n",
            "iteration: 230850 loss: 0.0008 lr: 0.02\n",
            "iteration: 230860 loss: 0.0007 lr: 0.02\n",
            "iteration: 230870 loss: 0.0006 lr: 0.02\n",
            "iteration: 230880 loss: 0.0006 lr: 0.02\n",
            "iteration: 230890 loss: 0.0006 lr: 0.02\n",
            "iteration: 230900 loss: 0.0007 lr: 0.02\n",
            "iteration: 230910 loss: 0.0006 lr: 0.02\n",
            "iteration: 230920 loss: 0.0011 lr: 0.02\n",
            "iteration: 230930 loss: 0.0010 lr: 0.02\n",
            "iteration: 230940 loss: 0.0008 lr: 0.02\n",
            "iteration: 230950 loss: 0.0008 lr: 0.02\n",
            "iteration: 230960 loss: 0.0010 lr: 0.02\n",
            "iteration: 230970 loss: 0.0007 lr: 0.02\n",
            "iteration: 230980 loss: 0.0007 lr: 0.02\n",
            "iteration: 230990 loss: 0.0010 lr: 0.02\n",
            "iteration: 231000 loss: 0.0008 lr: 0.02\n",
            "iteration: 231010 loss: 0.0012 lr: 0.02\n",
            "iteration: 231020 loss: 0.0009 lr: 0.02\n",
            "iteration: 231030 loss: 0.0006 lr: 0.02\n",
            "iteration: 231040 loss: 0.0008 lr: 0.02\n",
            "iteration: 231050 loss: 0.0008 lr: 0.02\n",
            "iteration: 231060 loss: 0.0005 lr: 0.02\n",
            "iteration: 231070 loss: 0.0009 lr: 0.02\n",
            "iteration: 231080 loss: 0.0007 lr: 0.02\n",
            "iteration: 231090 loss: 0.0007 lr: 0.02\n",
            "iteration: 231100 loss: 0.0012 lr: 0.02\n",
            "iteration: 231110 loss: 0.0007 lr: 0.02\n",
            "iteration: 231120 loss: 0.0010 lr: 0.02\n",
            "iteration: 231130 loss: 0.0008 lr: 0.02\n",
            "iteration: 231140 loss: 0.0011 lr: 0.02\n",
            "iteration: 231150 loss: 0.0007 lr: 0.02\n",
            "iteration: 231160 loss: 0.0017 lr: 0.02\n",
            "iteration: 231170 loss: 0.0006 lr: 0.02\n",
            "iteration: 231180 loss: 0.0006 lr: 0.02\n",
            "iteration: 231190 loss: 0.0009 lr: 0.02\n",
            "iteration: 231200 loss: 0.0008 lr: 0.02\n",
            "iteration: 231210 loss: 0.0008 lr: 0.02\n",
            "iteration: 231220 loss: 0.0007 lr: 0.02\n",
            "iteration: 231230 loss: 0.0009 lr: 0.02\n",
            "iteration: 231240 loss: 0.0008 lr: 0.02\n",
            "iteration: 231250 loss: 0.0006 lr: 0.02\n",
            "iteration: 231260 loss: 0.0008 lr: 0.02\n",
            "iteration: 231270 loss: 0.0007 lr: 0.02\n",
            "iteration: 231280 loss: 0.0007 lr: 0.02\n",
            "iteration: 231290 loss: 0.0005 lr: 0.02\n",
            "iteration: 231300 loss: 0.0009 lr: 0.02\n",
            "iteration: 231310 loss: 0.0007 lr: 0.02\n",
            "iteration: 231320 loss: 0.0007 lr: 0.02\n",
            "iteration: 231330 loss: 0.0007 lr: 0.02\n",
            "iteration: 231340 loss: 0.0011 lr: 0.02\n",
            "iteration: 231350 loss: 0.0008 lr: 0.02\n",
            "iteration: 231360 loss: 0.0008 lr: 0.02\n",
            "iteration: 231370 loss: 0.0008 lr: 0.02\n",
            "iteration: 231380 loss: 0.0009 lr: 0.02\n",
            "iteration: 231390 loss: 0.0012 lr: 0.02\n",
            "iteration: 231400 loss: 0.0008 lr: 0.02\n",
            "iteration: 231410 loss: 0.0008 lr: 0.02\n",
            "iteration: 231420 loss: 0.0008 lr: 0.02\n",
            "iteration: 231430 loss: 0.0007 lr: 0.02\n",
            "iteration: 231440 loss: 0.0008 lr: 0.02\n",
            "iteration: 231450 loss: 0.0008 lr: 0.02\n",
            "iteration: 231460 loss: 0.0007 lr: 0.02\n",
            "iteration: 231470 loss: 0.0006 lr: 0.02\n",
            "iteration: 231480 loss: 0.0007 lr: 0.02\n",
            "iteration: 231490 loss: 0.0006 lr: 0.02\n",
            "iteration: 231500 loss: 0.0006 lr: 0.02\n",
            "iteration: 231510 loss: 0.0006 lr: 0.02\n",
            "iteration: 231520 loss: 0.0006 lr: 0.02\n",
            "iteration: 231530 loss: 0.0008 lr: 0.02\n",
            "iteration: 231540 loss: 0.0007 lr: 0.02\n",
            "iteration: 231550 loss: 0.0010 lr: 0.02\n",
            "iteration: 231560 loss: 0.0007 lr: 0.02\n",
            "iteration: 231570 loss: 0.0006 lr: 0.02\n",
            "iteration: 231580 loss: 0.0006 lr: 0.02\n",
            "iteration: 231590 loss: 0.0011 lr: 0.02\n",
            "iteration: 231600 loss: 0.0007 lr: 0.02\n",
            "iteration: 231610 loss: 0.0006 lr: 0.02\n",
            "iteration: 231620 loss: 0.0007 lr: 0.02\n",
            "iteration: 231630 loss: 0.0008 lr: 0.02\n",
            "iteration: 231640 loss: 0.0010 lr: 0.02\n",
            "iteration: 231650 loss: 0.0011 lr: 0.02\n",
            "iteration: 231660 loss: 0.0008 lr: 0.02\n",
            "iteration: 231670 loss: 0.0006 lr: 0.02\n",
            "iteration: 231680 loss: 0.0007 lr: 0.02\n",
            "iteration: 231690 loss: 0.0008 lr: 0.02\n",
            "iteration: 231700 loss: 0.0008 lr: 0.02\n",
            "iteration: 231710 loss: 0.0008 lr: 0.02\n",
            "iteration: 231720 loss: 0.0006 lr: 0.02\n",
            "iteration: 231730 loss: 0.0010 lr: 0.02\n",
            "iteration: 231740 loss: 0.0006 lr: 0.02\n",
            "iteration: 231750 loss: 0.0011 lr: 0.02\n",
            "iteration: 231760 loss: 0.0006 lr: 0.02\n",
            "iteration: 231770 loss: 0.0006 lr: 0.02\n",
            "iteration: 231780 loss: 0.0006 lr: 0.02\n",
            "iteration: 231790 loss: 0.0010 lr: 0.02\n",
            "iteration: 231800 loss: 0.0008 lr: 0.02\n",
            "iteration: 231810 loss: 0.0006 lr: 0.02\n",
            "iteration: 231820 loss: 0.0008 lr: 0.02\n",
            "iteration: 231830 loss: 0.0005 lr: 0.02\n",
            "iteration: 231840 loss: 0.0007 lr: 0.02\n",
            "iteration: 231850 loss: 0.0007 lr: 0.02\n",
            "iteration: 231860 loss: 0.0008 lr: 0.02\n",
            "iteration: 231870 loss: 0.0007 lr: 0.02\n",
            "iteration: 231880 loss: 0.0012 lr: 0.02\n",
            "iteration: 231890 loss: 0.0007 lr: 0.02\n",
            "iteration: 231900 loss: 0.0007 lr: 0.02\n",
            "iteration: 231910 loss: 0.0007 lr: 0.02\n",
            "iteration: 231920 loss: 0.0007 lr: 0.02\n",
            "iteration: 231930 loss: 0.0008 lr: 0.02\n",
            "iteration: 231940 loss: 0.0009 lr: 0.02\n",
            "iteration: 231950 loss: 0.0008 lr: 0.02\n",
            "iteration: 231960 loss: 0.0008 lr: 0.02\n",
            "iteration: 231970 loss: 0.0007 lr: 0.02\n",
            "iteration: 231980 loss: 0.0006 lr: 0.02\n",
            "iteration: 231990 loss: 0.0012 lr: 0.02\n",
            "iteration: 232000 loss: 0.0010 lr: 0.02\n",
            "iteration: 232010 loss: 0.0008 lr: 0.02\n",
            "iteration: 232020 loss: 0.0008 lr: 0.02\n",
            "iteration: 232030 loss: 0.0007 lr: 0.02\n",
            "iteration: 232040 loss: 0.0007 lr: 0.02\n",
            "iteration: 232050 loss: 0.0006 lr: 0.02\n",
            "iteration: 232060 loss: 0.0009 lr: 0.02\n",
            "iteration: 232070 loss: 0.0006 lr: 0.02\n",
            "iteration: 232080 loss: 0.0007 lr: 0.02\n",
            "iteration: 232090 loss: 0.0007 lr: 0.02\n",
            "iteration: 232100 loss: 0.0008 lr: 0.02\n",
            "iteration: 232110 loss: 0.0006 lr: 0.02\n",
            "iteration: 232120 loss: 0.0008 lr: 0.02\n",
            "iteration: 232130 loss: 0.0009 lr: 0.02\n",
            "iteration: 232140 loss: 0.0008 lr: 0.02\n",
            "iteration: 232150 loss: 0.0007 lr: 0.02\n",
            "iteration: 232160 loss: 0.0007 lr: 0.02\n",
            "iteration: 232170 loss: 0.0009 lr: 0.02\n",
            "iteration: 232180 loss: 0.0010 lr: 0.02\n",
            "iteration: 232190 loss: 0.0008 lr: 0.02\n",
            "iteration: 232200 loss: 0.0007 lr: 0.02\n",
            "iteration: 232210 loss: 0.0007 lr: 0.02\n",
            "iteration: 232220 loss: 0.0008 lr: 0.02\n",
            "iteration: 232230 loss: 0.0006 lr: 0.02\n",
            "iteration: 232240 loss: 0.0008 lr: 0.02\n",
            "iteration: 232250 loss: 0.0006 lr: 0.02\n",
            "iteration: 232260 loss: 0.0013 lr: 0.02\n",
            "iteration: 232270 loss: 0.0012 lr: 0.02\n",
            "iteration: 232280 loss: 0.0007 lr: 0.02\n",
            "iteration: 232290 loss: 0.0008 lr: 0.02\n",
            "iteration: 232300 loss: 0.0007 lr: 0.02\n",
            "iteration: 232310 loss: 0.0007 lr: 0.02\n",
            "iteration: 232320 loss: 0.0008 lr: 0.02\n",
            "iteration: 232330 loss: 0.0007 lr: 0.02\n",
            "iteration: 232340 loss: 0.0007 lr: 0.02\n",
            "iteration: 232350 loss: 0.0007 lr: 0.02\n",
            "iteration: 232360 loss: 0.0006 lr: 0.02\n",
            "iteration: 232370 loss: 0.0008 lr: 0.02\n",
            "iteration: 232380 loss: 0.0007 lr: 0.02\n",
            "iteration: 232390 loss: 0.0007 lr: 0.02\n",
            "iteration: 232400 loss: 0.0009 lr: 0.02\n",
            "iteration: 232410 loss: 0.0007 lr: 0.02\n",
            "iteration: 232420 loss: 0.0011 lr: 0.02\n",
            "iteration: 232430 loss: 0.0009 lr: 0.02\n",
            "iteration: 232440 loss: 0.0010 lr: 0.02\n",
            "iteration: 232450 loss: 0.0008 lr: 0.02\n",
            "iteration: 232460 loss: 0.0010 lr: 0.02\n",
            "iteration: 232470 loss: 0.0010 lr: 0.02\n",
            "iteration: 232480 loss: 0.0008 lr: 0.02\n",
            "iteration: 232490 loss: 0.0014 lr: 0.02\n",
            "iteration: 232500 loss: 0.0010 lr: 0.02\n",
            "iteration: 232510 loss: 0.0012 lr: 0.02\n",
            "iteration: 232520 loss: 0.0010 lr: 0.02\n",
            "iteration: 232530 loss: 0.0008 lr: 0.02\n",
            "iteration: 232540 loss: 0.0007 lr: 0.02\n",
            "iteration: 232550 loss: 0.0010 lr: 0.02\n",
            "iteration: 232560 loss: 0.0007 lr: 0.02\n",
            "iteration: 232570 loss: 0.0009 lr: 0.02\n",
            "iteration: 232580 loss: 0.0009 lr: 0.02\n",
            "iteration: 232590 loss: 0.0007 lr: 0.02\n",
            "iteration: 232600 loss: 0.0008 lr: 0.02\n",
            "iteration: 232610 loss: 0.0007 lr: 0.02\n",
            "iteration: 232620 loss: 0.0010 lr: 0.02\n",
            "iteration: 232630 loss: 0.0006 lr: 0.02\n",
            "iteration: 232640 loss: 0.0008 lr: 0.02\n",
            "iteration: 232650 loss: 0.0006 lr: 0.02\n",
            "iteration: 232660 loss: 0.0007 lr: 0.02\n",
            "iteration: 232670 loss: 0.0008 lr: 0.02\n",
            "iteration: 232680 loss: 0.0007 lr: 0.02\n",
            "iteration: 232690 loss: 0.0007 lr: 0.02\n",
            "iteration: 232700 loss: 0.0009 lr: 0.02\n",
            "iteration: 232710 loss: 0.0009 lr: 0.02\n",
            "iteration: 232720 loss: 0.0009 lr: 0.02\n",
            "iteration: 232730 loss: 0.0009 lr: 0.02\n",
            "iteration: 232740 loss: 0.0008 lr: 0.02\n",
            "iteration: 232750 loss: 0.0009 lr: 0.02\n",
            "iteration: 232760 loss: 0.0010 lr: 0.02\n",
            "iteration: 232770 loss: 0.0010 lr: 0.02\n",
            "iteration: 232780 loss: 0.0013 lr: 0.02\n",
            "iteration: 232790 loss: 0.0009 lr: 0.02\n",
            "iteration: 232800 loss: 0.0010 lr: 0.02\n",
            "iteration: 232810 loss: 0.0007 lr: 0.02\n",
            "iteration: 232820 loss: 0.0009 lr: 0.02\n",
            "iteration: 232830 loss: 0.0011 lr: 0.02\n",
            "iteration: 232840 loss: 0.0012 lr: 0.02\n",
            "iteration: 232850 loss: 0.0011 lr: 0.02\n",
            "iteration: 232860 loss: 0.0009 lr: 0.02\n",
            "iteration: 232870 loss: 0.0007 lr: 0.02\n",
            "iteration: 232880 loss: 0.0007 lr: 0.02\n",
            "iteration: 232890 loss: 0.0009 lr: 0.02\n",
            "iteration: 232900 loss: 0.0009 lr: 0.02\n",
            "iteration: 232910 loss: 0.0006 lr: 0.02\n",
            "iteration: 232920 loss: 0.0008 lr: 0.02\n",
            "iteration: 232930 loss: 0.0009 lr: 0.02\n",
            "iteration: 232940 loss: 0.0006 lr: 0.02\n",
            "iteration: 232950 loss: 0.0009 lr: 0.02\n",
            "iteration: 232960 loss: 0.0006 lr: 0.02\n",
            "iteration: 232970 loss: 0.0009 lr: 0.02\n",
            "iteration: 232980 loss: 0.0007 lr: 0.02\n",
            "iteration: 232990 loss: 0.0009 lr: 0.02\n",
            "iteration: 233000 loss: 0.0007 lr: 0.02\n",
            "iteration: 233010 loss: 0.0007 lr: 0.02\n",
            "iteration: 233020 loss: 0.0009 lr: 0.02\n",
            "iteration: 233030 loss: 0.0007 lr: 0.02\n",
            "iteration: 233040 loss: 0.0008 lr: 0.02\n",
            "iteration: 233050 loss: 0.0012 lr: 0.02\n",
            "iteration: 233060 loss: 0.0010 lr: 0.02\n",
            "iteration: 233070 loss: 0.0007 lr: 0.02\n",
            "iteration: 233080 loss: 0.0007 lr: 0.02\n",
            "iteration: 233090 loss: 0.0009 lr: 0.02\n",
            "iteration: 233100 loss: 0.0006 lr: 0.02\n",
            "iteration: 233110 loss: 0.0007 lr: 0.02\n",
            "iteration: 233120 loss: 0.0009 lr: 0.02\n",
            "iteration: 233130 loss: 0.0019 lr: 0.02\n",
            "iteration: 233140 loss: 0.0008 lr: 0.02\n",
            "iteration: 233150 loss: 0.0008 lr: 0.02\n",
            "iteration: 233160 loss: 0.0011 lr: 0.02\n",
            "iteration: 233170 loss: 0.0008 lr: 0.02\n",
            "iteration: 233180 loss: 0.0008 lr: 0.02\n",
            "iteration: 233190 loss: 0.0007 lr: 0.02\n",
            "iteration: 233200 loss: 0.0007 lr: 0.02\n",
            "iteration: 233210 loss: 0.0009 lr: 0.02\n",
            "iteration: 233220 loss: 0.0009 lr: 0.02\n",
            "iteration: 233230 loss: 0.0005 lr: 0.02\n",
            "iteration: 233240 loss: 0.0007 lr: 0.02\n",
            "iteration: 233250 loss: 0.0010 lr: 0.02\n",
            "iteration: 233260 loss: 0.0009 lr: 0.02\n",
            "iteration: 233270 loss: 0.0006 lr: 0.02\n",
            "iteration: 233280 loss: 0.0005 lr: 0.02\n",
            "iteration: 233290 loss: 0.0008 lr: 0.02\n",
            "iteration: 233300 loss: 0.0006 lr: 0.02\n",
            "iteration: 233310 loss: 0.0012 lr: 0.02\n",
            "iteration: 233320 loss: 0.0008 lr: 0.02\n",
            "iteration: 233330 loss: 0.0008 lr: 0.02\n",
            "iteration: 233340 loss: 0.0010 lr: 0.02\n",
            "iteration: 233350 loss: 0.0008 lr: 0.02\n",
            "iteration: 233360 loss: 0.0008 lr: 0.02\n",
            "iteration: 233370 loss: 0.0008 lr: 0.02\n",
            "iteration: 233380 loss: 0.0009 lr: 0.02\n",
            "iteration: 233390 loss: 0.0009 lr: 0.02\n",
            "iteration: 233400 loss: 0.0016 lr: 0.02\n",
            "iteration: 233410 loss: 0.0012 lr: 0.02\n",
            "iteration: 233420 loss: 0.0005 lr: 0.02\n",
            "iteration: 233430 loss: 0.0008 lr: 0.02\n",
            "iteration: 233440 loss: 0.0009 lr: 0.02\n",
            "iteration: 233450 loss: 0.0007 lr: 0.02\n",
            "iteration: 233460 loss: 0.0008 lr: 0.02\n",
            "iteration: 233470 loss: 0.0010 lr: 0.02\n",
            "iteration: 233480 loss: 0.0006 lr: 0.02\n",
            "iteration: 233490 loss: 0.0008 lr: 0.02\n",
            "iteration: 233500 loss: 0.0007 lr: 0.02\n",
            "iteration: 233510 loss: 0.0009 lr: 0.02\n",
            "iteration: 233520 loss: 0.0006 lr: 0.02\n",
            "iteration: 233530 loss: 0.0006 lr: 0.02\n",
            "iteration: 233540 loss: 0.0008 lr: 0.02\n",
            "iteration: 233550 loss: 0.0006 lr: 0.02\n",
            "iteration: 233560 loss: 0.0007 lr: 0.02\n",
            "iteration: 233570 loss: 0.0009 lr: 0.02\n",
            "iteration: 233580 loss: 0.0007 lr: 0.02\n",
            "iteration: 233590 loss: 0.0007 lr: 0.02\n",
            "iteration: 233600 loss: 0.0008 lr: 0.02\n",
            "iteration: 233610 loss: 0.0005 lr: 0.02\n",
            "iteration: 233620 loss: 0.0009 lr: 0.02\n",
            "iteration: 233630 loss: 0.0010 lr: 0.02\n",
            "iteration: 233640 loss: 0.0007 lr: 0.02\n",
            "iteration: 233650 loss: 0.0008 lr: 0.02\n",
            "iteration: 233660 loss: 0.0006 lr: 0.02\n",
            "iteration: 233670 loss: 0.0008 lr: 0.02\n",
            "iteration: 233680 loss: 0.0008 lr: 0.02\n",
            "iteration: 233690 loss: 0.0006 lr: 0.02\n",
            "iteration: 233700 loss: 0.0007 lr: 0.02\n",
            "iteration: 233710 loss: 0.0009 lr: 0.02\n",
            "iteration: 233720 loss: 0.0008 lr: 0.02\n",
            "iteration: 233730 loss: 0.0009 lr: 0.02\n",
            "iteration: 233740 loss: 0.0007 lr: 0.02\n",
            "iteration: 233750 loss: 0.0007 lr: 0.02\n",
            "iteration: 233760 loss: 0.0008 lr: 0.02\n",
            "iteration: 233770 loss: 0.0008 lr: 0.02\n",
            "iteration: 233780 loss: 0.0007 lr: 0.02\n",
            "iteration: 233790 loss: 0.0008 lr: 0.02\n",
            "iteration: 233800 loss: 0.0007 lr: 0.02\n",
            "iteration: 233810 loss: 0.0007 lr: 0.02\n",
            "iteration: 233820 loss: 0.0007 lr: 0.02\n",
            "iteration: 233830 loss: 0.0008 lr: 0.02\n",
            "iteration: 233840 loss: 0.0008 lr: 0.02\n",
            "iteration: 233850 loss: 0.0007 lr: 0.02\n",
            "iteration: 233860 loss: 0.0007 lr: 0.02\n",
            "iteration: 233870 loss: 0.0008 lr: 0.02\n",
            "iteration: 233880 loss: 0.0011 lr: 0.02\n",
            "iteration: 233890 loss: 0.0009 lr: 0.02\n",
            "iteration: 233900 loss: 0.0006 lr: 0.02\n",
            "iteration: 233910 loss: 0.0009 lr: 0.02\n",
            "iteration: 233920 loss: 0.0007 lr: 0.02\n",
            "iteration: 233930 loss: 0.0008 lr: 0.02\n",
            "iteration: 233940 loss: 0.0009 lr: 0.02\n",
            "iteration: 233950 loss: 0.0008 lr: 0.02\n",
            "iteration: 233960 loss: 0.0009 lr: 0.02\n",
            "iteration: 233970 loss: 0.0008 lr: 0.02\n",
            "iteration: 233980 loss: 0.0010 lr: 0.02\n",
            "iteration: 233990 loss: 0.0006 lr: 0.02\n",
            "iteration: 234000 loss: 0.0007 lr: 0.02\n",
            "iteration: 234010 loss: 0.0011 lr: 0.02\n",
            "iteration: 234020 loss: 0.0006 lr: 0.02\n",
            "iteration: 234030 loss: 0.0006 lr: 0.02\n",
            "iteration: 234040 loss: 0.0009 lr: 0.02\n",
            "iteration: 234050 loss: 0.0008 lr: 0.02\n",
            "iteration: 234060 loss: 0.0007 lr: 0.02\n",
            "iteration: 234070 loss: 0.0008 lr: 0.02\n",
            "iteration: 234080 loss: 0.0007 lr: 0.02\n",
            "iteration: 234090 loss: 0.0008 lr: 0.02\n",
            "iteration: 234100 loss: 0.0006 lr: 0.02\n",
            "iteration: 234110 loss: 0.0006 lr: 0.02\n",
            "iteration: 234120 loss: 0.0006 lr: 0.02\n",
            "iteration: 234130 loss: 0.0007 lr: 0.02\n",
            "iteration: 234140 loss: 0.0009 lr: 0.02\n",
            "iteration: 234150 loss: 0.0007 lr: 0.02\n",
            "iteration: 234160 loss: 0.0008 lr: 0.02\n",
            "iteration: 234170 loss: 0.0006 lr: 0.02\n",
            "iteration: 234180 loss: 0.0009 lr: 0.02\n",
            "iteration: 234190 loss: 0.0005 lr: 0.02\n",
            "iteration: 234200 loss: 0.0006 lr: 0.02\n",
            "iteration: 234210 loss: 0.0005 lr: 0.02\n",
            "iteration: 234220 loss: 0.0008 lr: 0.02\n",
            "iteration: 234230 loss: 0.0007 lr: 0.02\n",
            "iteration: 234240 loss: 0.0008 lr: 0.02\n",
            "iteration: 234250 loss: 0.0008 lr: 0.02\n",
            "iteration: 234260 loss: 0.0006 lr: 0.02\n",
            "iteration: 234270 loss: 0.0007 lr: 0.02\n",
            "iteration: 234280 loss: 0.0009 lr: 0.02\n",
            "iteration: 234290 loss: 0.0008 lr: 0.02\n",
            "iteration: 234300 loss: 0.0013 lr: 0.02\n",
            "iteration: 234310 loss: 0.0007 lr: 0.02\n",
            "iteration: 234320 loss: 0.0008 lr: 0.02\n",
            "iteration: 234330 loss: 0.0008 lr: 0.02\n",
            "iteration: 234340 loss: 0.0007 lr: 0.02\n",
            "iteration: 234350 loss: 0.0006 lr: 0.02\n",
            "iteration: 234360 loss: 0.0006 lr: 0.02\n",
            "iteration: 234370 loss: 0.0007 lr: 0.02\n",
            "iteration: 234380 loss: 0.0005 lr: 0.02\n",
            "iteration: 234390 loss: 0.0010 lr: 0.02\n",
            "iteration: 234400 loss: 0.0007 lr: 0.02\n",
            "iteration: 234410 loss: 0.0006 lr: 0.02\n",
            "iteration: 234420 loss: 0.0008 lr: 0.02\n",
            "iteration: 234430 loss: 0.0008 lr: 0.02\n",
            "iteration: 234440 loss: 0.0007 lr: 0.02\n",
            "iteration: 234450 loss: 0.0007 lr: 0.02\n",
            "iteration: 234460 loss: 0.0010 lr: 0.02\n",
            "iteration: 234470 loss: 0.0010 lr: 0.02\n",
            "iteration: 234480 loss: 0.0009 lr: 0.02\n",
            "iteration: 234490 loss: 0.0009 lr: 0.02\n",
            "iteration: 234500 loss: 0.0009 lr: 0.02\n",
            "iteration: 234510 loss: 0.0011 lr: 0.02\n",
            "iteration: 234520 loss: 0.0007 lr: 0.02\n",
            "iteration: 234530 loss: 0.0007 lr: 0.02\n",
            "iteration: 234540 loss: 0.0008 lr: 0.02\n",
            "iteration: 234550 loss: 0.0007 lr: 0.02\n",
            "iteration: 234560 loss: 0.0009 lr: 0.02\n",
            "iteration: 234570 loss: 0.0006 lr: 0.02\n",
            "iteration: 234580 loss: 0.0006 lr: 0.02\n",
            "iteration: 234590 loss: 0.0009 lr: 0.02\n",
            "iteration: 234600 loss: 0.0008 lr: 0.02\n",
            "iteration: 234610 loss: 0.0006 lr: 0.02\n",
            "iteration: 234620 loss: 0.0009 lr: 0.02\n",
            "iteration: 234630 loss: 0.0008 lr: 0.02\n",
            "iteration: 234640 loss: 0.0007 lr: 0.02\n",
            "iteration: 234650 loss: 0.0008 lr: 0.02\n",
            "iteration: 234660 loss: 0.0006 lr: 0.02\n",
            "iteration: 234670 loss: 0.0008 lr: 0.02\n",
            "iteration: 234680 loss: 0.0008 lr: 0.02\n",
            "iteration: 234690 loss: 0.0007 lr: 0.02\n",
            "iteration: 234700 loss: 0.0008 lr: 0.02\n",
            "iteration: 234710 loss: 0.0008 lr: 0.02\n",
            "iteration: 234720 loss: 0.0006 lr: 0.02\n",
            "iteration: 234730 loss: 0.0009 lr: 0.02\n",
            "iteration: 234740 loss: 0.0011 lr: 0.02\n",
            "iteration: 234750 loss: 0.0008 lr: 0.02\n",
            "iteration: 234760 loss: 0.0008 lr: 0.02\n",
            "iteration: 234770 loss: 0.0010 lr: 0.02\n",
            "iteration: 234780 loss: 0.0011 lr: 0.02\n",
            "iteration: 234790 loss: 0.0006 lr: 0.02\n",
            "iteration: 234800 loss: 0.0008 lr: 0.02\n",
            "iteration: 234810 loss: 0.0009 lr: 0.02\n",
            "iteration: 234820 loss: 0.0007 lr: 0.02\n",
            "iteration: 234830 loss: 0.0010 lr: 0.02\n",
            "iteration: 234840 loss: 0.0009 lr: 0.02\n",
            "iteration: 234850 loss: 0.0006 lr: 0.02\n",
            "iteration: 234860 loss: 0.0007 lr: 0.02\n",
            "iteration: 234870 loss: 0.0007 lr: 0.02\n",
            "iteration: 234880 loss: 0.0005 lr: 0.02\n",
            "iteration: 234890 loss: 0.0008 lr: 0.02\n",
            "iteration: 234900 loss: 0.0005 lr: 0.02\n",
            "iteration: 234910 loss: 0.0008 lr: 0.02\n",
            "iteration: 234920 loss: 0.0007 lr: 0.02\n",
            "iteration: 234930 loss: 0.0005 lr: 0.02\n",
            "iteration: 234940 loss: 0.0007 lr: 0.02\n",
            "iteration: 234950 loss: 0.0006 lr: 0.02\n",
            "iteration: 234960 loss: 0.0007 lr: 0.02\n",
            "iteration: 234970 loss: 0.0005 lr: 0.02\n",
            "iteration: 234980 loss: 0.0006 lr: 0.02\n",
            "iteration: 234990 loss: 0.0006 lr: 0.02\n",
            "iteration: 235000 loss: 0.0006 lr: 0.02\n",
            "iteration: 235010 loss: 0.0006 lr: 0.02\n",
            "iteration: 235020 loss: 0.0006 lr: 0.02\n",
            "iteration: 235030 loss: 0.0006 lr: 0.02\n",
            "iteration: 235040 loss: 0.0008 lr: 0.02\n",
            "iteration: 235050 loss: 0.0007 lr: 0.02\n",
            "iteration: 235060 loss: 0.0009 lr: 0.02\n",
            "iteration: 235070 loss: 0.0007 lr: 0.02\n",
            "iteration: 235080 loss: 0.0008 lr: 0.02\n",
            "iteration: 235090 loss: 0.0008 lr: 0.02\n",
            "iteration: 235100 loss: 0.0009 lr: 0.02\n",
            "iteration: 235110 loss: 0.0010 lr: 0.02\n",
            "iteration: 235120 loss: 0.0006 lr: 0.02\n",
            "iteration: 235130 loss: 0.0006 lr: 0.02\n",
            "iteration: 235140 loss: 0.0008 lr: 0.02\n",
            "iteration: 235150 loss: 0.0005 lr: 0.02\n",
            "iteration: 235160 loss: 0.0007 lr: 0.02\n",
            "iteration: 235170 loss: 0.0006 lr: 0.02\n",
            "iteration: 235180 loss: 0.0008 lr: 0.02\n",
            "iteration: 235190 loss: 0.0007 lr: 0.02\n",
            "iteration: 235200 loss: 0.0008 lr: 0.02\n",
            "iteration: 235210 loss: 0.0007 lr: 0.02\n",
            "iteration: 235220 loss: 0.0006 lr: 0.02\n",
            "iteration: 235230 loss: 0.0008 lr: 0.02\n",
            "iteration: 235240 loss: 0.0007 lr: 0.02\n",
            "iteration: 235250 loss: 0.0007 lr: 0.02\n",
            "iteration: 235260 loss: 0.0009 lr: 0.02\n",
            "iteration: 235270 loss: 0.0007 lr: 0.02\n",
            "iteration: 235280 loss: 0.0011 lr: 0.02\n",
            "iteration: 235290 loss: 0.0014 lr: 0.02\n",
            "iteration: 235300 loss: 0.0007 lr: 0.02\n",
            "iteration: 235310 loss: 0.0006 lr: 0.02\n",
            "iteration: 235320 loss: 0.0006 lr: 0.02\n",
            "iteration: 235330 loss: 0.0007 lr: 0.02\n",
            "iteration: 235340 loss: 0.0008 lr: 0.02\n",
            "iteration: 235350 loss: 0.0009 lr: 0.02\n",
            "iteration: 235360 loss: 0.0007 lr: 0.02\n",
            "iteration: 235370 loss: 0.0007 lr: 0.02\n",
            "iteration: 235380 loss: 0.0009 lr: 0.02\n",
            "iteration: 235390 loss: 0.0008 lr: 0.02\n",
            "iteration: 235400 loss: 0.0010 lr: 0.02\n",
            "iteration: 235410 loss: 0.0008 lr: 0.02\n",
            "iteration: 235420 loss: 0.0008 lr: 0.02\n",
            "iteration: 235430 loss: 0.0008 lr: 0.02\n",
            "iteration: 235440 loss: 0.0007 lr: 0.02\n",
            "iteration: 235450 loss: 0.0005 lr: 0.02\n",
            "iteration: 235460 loss: 0.0008 lr: 0.02\n",
            "iteration: 235470 loss: 0.0008 lr: 0.02\n",
            "iteration: 235480 loss: 0.0009 lr: 0.02\n",
            "iteration: 235490 loss: 0.0009 lr: 0.02\n",
            "iteration: 235500 loss: 0.0008 lr: 0.02\n",
            "iteration: 235510 loss: 0.0007 lr: 0.02\n",
            "iteration: 235520 loss: 0.0009 lr: 0.02\n",
            "iteration: 235530 loss: 0.0012 lr: 0.02\n",
            "iteration: 235540 loss: 0.0007 lr: 0.02\n",
            "iteration: 235550 loss: 0.0006 lr: 0.02\n",
            "iteration: 235560 loss: 0.0006 lr: 0.02\n",
            "iteration: 235570 loss: 0.0006 lr: 0.02\n",
            "iteration: 235580 loss: 0.0008 lr: 0.02\n",
            "iteration: 235590 loss: 0.0007 lr: 0.02\n",
            "iteration: 235600 loss: 0.0007 lr: 0.02\n",
            "iteration: 235610 loss: 0.0008 lr: 0.02\n",
            "iteration: 235620 loss: 0.0006 lr: 0.02\n",
            "iteration: 235630 loss: 0.0007 lr: 0.02\n",
            "iteration: 235640 loss: 0.0009 lr: 0.02\n",
            "iteration: 235650 loss: 0.0008 lr: 0.02\n",
            "iteration: 235660 loss: 0.0005 lr: 0.02\n",
            "iteration: 235670 loss: 0.0008 lr: 0.02\n",
            "iteration: 235680 loss: 0.0007 lr: 0.02\n",
            "iteration: 235690 loss: 0.0010 lr: 0.02\n",
            "iteration: 235700 loss: 0.0011 lr: 0.02\n",
            "iteration: 235710 loss: 0.0008 lr: 0.02\n",
            "iteration: 235720 loss: 0.0009 lr: 0.02\n",
            "iteration: 235730 loss: 0.0008 lr: 0.02\n",
            "iteration: 235740 loss: 0.0007 lr: 0.02\n",
            "iteration: 235750 loss: 0.0009 lr: 0.02\n",
            "iteration: 235760 loss: 0.0006 lr: 0.02\n",
            "iteration: 235770 loss: 0.0007 lr: 0.02\n",
            "iteration: 235780 loss: 0.0008 lr: 0.02\n",
            "iteration: 235790 loss: 0.0005 lr: 0.02\n",
            "iteration: 235800 loss: 0.0005 lr: 0.02\n",
            "iteration: 235810 loss: 0.0008 lr: 0.02\n",
            "iteration: 235820 loss: 0.0007 lr: 0.02\n",
            "iteration: 235830 loss: 0.0008 lr: 0.02\n",
            "iteration: 235840 loss: 0.0008 lr: 0.02\n",
            "iteration: 235850 loss: 0.0006 lr: 0.02\n",
            "iteration: 235860 loss: 0.0009 lr: 0.02\n",
            "iteration: 235870 loss: 0.0007 lr: 0.02\n",
            "iteration: 235880 loss: 0.0007 lr: 0.02\n",
            "iteration: 235890 loss: 0.0010 lr: 0.02\n",
            "iteration: 235900 loss: 0.0010 lr: 0.02\n",
            "iteration: 235910 loss: 0.0009 lr: 0.02\n",
            "iteration: 235920 loss: 0.0006 lr: 0.02\n",
            "iteration: 235930 loss: 0.0011 lr: 0.02\n",
            "iteration: 235940 loss: 0.0007 lr: 0.02\n",
            "iteration: 235950 loss: 0.0010 lr: 0.02\n",
            "iteration: 235960 loss: 0.0008 lr: 0.02\n",
            "iteration: 235970 loss: 0.0008 lr: 0.02\n",
            "iteration: 235980 loss: 0.0006 lr: 0.02\n",
            "iteration: 235990 loss: 0.0006 lr: 0.02\n",
            "iteration: 236000 loss: 0.0008 lr: 0.02\n",
            "iteration: 236010 loss: 0.0008 lr: 0.02\n",
            "iteration: 236020 loss: 0.0008 lr: 0.02\n",
            "iteration: 236030 loss: 0.0009 lr: 0.02\n",
            "iteration: 236040 loss: 0.0009 lr: 0.02\n",
            "iteration: 236050 loss: 0.0009 lr: 0.02\n",
            "iteration: 236060 loss: 0.0009 lr: 0.02\n",
            "iteration: 236070 loss: 0.0007 lr: 0.02\n",
            "iteration: 236080 loss: 0.0011 lr: 0.02\n",
            "iteration: 236090 loss: 0.0006 lr: 0.02\n",
            "iteration: 236100 loss: 0.0006 lr: 0.02\n",
            "iteration: 236110 loss: 0.0006 lr: 0.02\n",
            "iteration: 236120 loss: 0.0008 lr: 0.02\n",
            "iteration: 236130 loss: 0.0008 lr: 0.02\n",
            "iteration: 236140 loss: 0.0008 lr: 0.02\n",
            "iteration: 236150 loss: 0.0008 lr: 0.02\n",
            "iteration: 236160 loss: 0.0007 lr: 0.02\n",
            "iteration: 236170 loss: 0.0006 lr: 0.02\n",
            "iteration: 236180 loss: 0.0010 lr: 0.02\n",
            "iteration: 236190 loss: 0.0005 lr: 0.02\n",
            "iteration: 236200 loss: 0.0009 lr: 0.02\n",
            "iteration: 236210 loss: 0.0007 lr: 0.02\n",
            "iteration: 236220 loss: 0.0007 lr: 0.02\n",
            "iteration: 236230 loss: 0.0010 lr: 0.02\n",
            "iteration: 236240 loss: 0.0007 lr: 0.02\n",
            "iteration: 236250 loss: 0.0007 lr: 0.02\n",
            "iteration: 236260 loss: 0.0007 lr: 0.02\n",
            "iteration: 236270 loss: 0.0008 lr: 0.02\n",
            "iteration: 236280 loss: 0.0008 lr: 0.02\n",
            "iteration: 236290 loss: 0.0006 lr: 0.02\n",
            "iteration: 236300 loss: 0.0006 lr: 0.02\n",
            "iteration: 236310 loss: 0.0007 lr: 0.02\n",
            "iteration: 236320 loss: 0.0011 lr: 0.02\n",
            "iteration: 236330 loss: 0.0008 lr: 0.02\n",
            "iteration: 236340 loss: 0.0009 lr: 0.02\n",
            "iteration: 236350 loss: 0.0011 lr: 0.02\n",
            "iteration: 236360 loss: 0.0008 lr: 0.02\n",
            "iteration: 236370 loss: 0.0009 lr: 0.02\n",
            "iteration: 236380 loss: 0.0008 lr: 0.02\n",
            "iteration: 236390 loss: 0.0006 lr: 0.02\n",
            "iteration: 236400 loss: 0.0006 lr: 0.02\n",
            "iteration: 236410 loss: 0.0006 lr: 0.02\n",
            "iteration: 236420 loss: 0.0007 lr: 0.02\n",
            "iteration: 236430 loss: 0.0007 lr: 0.02\n",
            "iteration: 236440 loss: 0.0006 lr: 0.02\n",
            "iteration: 236450 loss: 0.0008 lr: 0.02\n",
            "iteration: 236460 loss: 0.0009 lr: 0.02\n",
            "iteration: 236470 loss: 0.0006 lr: 0.02\n",
            "iteration: 236480 loss: 0.0015 lr: 0.02\n",
            "iteration: 236490 loss: 0.0005 lr: 0.02\n",
            "iteration: 236500 loss: 0.0006 lr: 0.02\n",
            "iteration: 236510 loss: 0.0011 lr: 0.02\n",
            "iteration: 236520 loss: 0.0007 lr: 0.02\n",
            "iteration: 236530 loss: 0.0007 lr: 0.02\n",
            "iteration: 236540 loss: 0.0006 lr: 0.02\n",
            "iteration: 236550 loss: 0.0008 lr: 0.02\n",
            "iteration: 236560 loss: 0.0010 lr: 0.02\n",
            "iteration: 236570 loss: 0.0008 lr: 0.02\n",
            "iteration: 236580 loss: 0.0009 lr: 0.02\n",
            "iteration: 236590 loss: 0.0007 lr: 0.02\n",
            "iteration: 236600 loss: 0.0007 lr: 0.02\n",
            "iteration: 236610 loss: 0.0009 lr: 0.02\n",
            "iteration: 236620 loss: 0.0010 lr: 0.02\n",
            "iteration: 236630 loss: 0.0010 lr: 0.02\n",
            "iteration: 236640 loss: 0.0007 lr: 0.02\n",
            "iteration: 236650 loss: 0.0006 lr: 0.02\n",
            "iteration: 236660 loss: 0.0005 lr: 0.02\n",
            "iteration: 236670 loss: 0.0008 lr: 0.02\n",
            "iteration: 236680 loss: 0.0016 lr: 0.02\n",
            "iteration: 236690 loss: 0.0008 lr: 0.02\n",
            "iteration: 236700 loss: 0.0005 lr: 0.02\n",
            "iteration: 236710 loss: 0.0004 lr: 0.02\n",
            "iteration: 236720 loss: 0.0008 lr: 0.02\n",
            "iteration: 236730 loss: 0.0008 lr: 0.02\n",
            "iteration: 236740 loss: 0.0008 lr: 0.02\n",
            "iteration: 236750 loss: 0.0007 lr: 0.02\n",
            "iteration: 236760 loss: 0.0007 lr: 0.02\n",
            "iteration: 236770 loss: 0.0008 lr: 0.02\n",
            "iteration: 236780 loss: 0.0006 lr: 0.02\n",
            "iteration: 236790 loss: 0.0008 lr: 0.02\n",
            "iteration: 236800 loss: 0.0009 lr: 0.02\n",
            "iteration: 236810 loss: 0.0011 lr: 0.02\n",
            "iteration: 236820 loss: 0.0006 lr: 0.02\n",
            "iteration: 236830 loss: 0.0007 lr: 0.02\n",
            "iteration: 236840 loss: 0.0010 lr: 0.02\n",
            "iteration: 236850 loss: 0.0006 lr: 0.02\n",
            "iteration: 236860 loss: 0.0012 lr: 0.02\n",
            "iteration: 236870 loss: 0.0008 lr: 0.02\n",
            "iteration: 236880 loss: 0.0006 lr: 0.02\n",
            "iteration: 236890 loss: 0.0009 lr: 0.02\n",
            "iteration: 236900 loss: 0.0009 lr: 0.02\n",
            "iteration: 236910 loss: 0.0007 lr: 0.02\n",
            "iteration: 236920 loss: 0.0008 lr: 0.02\n",
            "iteration: 236930 loss: 0.0009 lr: 0.02\n",
            "iteration: 236940 loss: 0.0006 lr: 0.02\n",
            "iteration: 236950 loss: 0.0007 lr: 0.02\n",
            "iteration: 236960 loss: 0.0009 lr: 0.02\n",
            "iteration: 236970 loss: 0.0010 lr: 0.02\n",
            "iteration: 236980 loss: 0.0006 lr: 0.02\n",
            "iteration: 236990 loss: 0.0008 lr: 0.02\n",
            "iteration: 237000 loss: 0.0011 lr: 0.02\n",
            "iteration: 237010 loss: 0.0007 lr: 0.02\n",
            "iteration: 237020 loss: 0.0008 lr: 0.02\n",
            "iteration: 237030 loss: 0.0007 lr: 0.02\n",
            "iteration: 237040 loss: 0.0008 lr: 0.02\n",
            "iteration: 237050 loss: 0.0007 lr: 0.02\n",
            "iteration: 237060 loss: 0.0006 lr: 0.02\n",
            "iteration: 237070 loss: 0.0008 lr: 0.02\n",
            "iteration: 237080 loss: 0.0011 lr: 0.02\n",
            "iteration: 237090 loss: 0.0009 lr: 0.02\n",
            "iteration: 237100 loss: 0.0016 lr: 0.02\n",
            "iteration: 237110 loss: 0.0007 lr: 0.02\n",
            "iteration: 237120 loss: 0.0008 lr: 0.02\n",
            "iteration: 237130 loss: 0.0009 lr: 0.02\n",
            "iteration: 237140 loss: 0.0007 lr: 0.02\n",
            "iteration: 237150 loss: 0.0010 lr: 0.02\n",
            "iteration: 237160 loss: 0.0012 lr: 0.02\n",
            "iteration: 237170 loss: 0.0008 lr: 0.02\n",
            "iteration: 237180 loss: 0.0009 lr: 0.02\n",
            "iteration: 237190 loss: 0.0008 lr: 0.02\n",
            "iteration: 237200 loss: 0.0009 lr: 0.02\n",
            "iteration: 237210 loss: 0.0009 lr: 0.02\n",
            "iteration: 237220 loss: 0.0008 lr: 0.02\n",
            "iteration: 237230 loss: 0.0008 lr: 0.02\n",
            "iteration: 237240 loss: 0.0007 lr: 0.02\n",
            "iteration: 237250 loss: 0.0014 lr: 0.02\n",
            "iteration: 237260 loss: 0.0010 lr: 0.02\n",
            "iteration: 237270 loss: 0.0008 lr: 0.02\n",
            "iteration: 237280 loss: 0.0007 lr: 0.02\n",
            "iteration: 237290 loss: 0.0005 lr: 0.02\n",
            "iteration: 237300 loss: 0.0008 lr: 0.02\n",
            "iteration: 237310 loss: 0.0008 lr: 0.02\n",
            "iteration: 237320 loss: 0.0007 lr: 0.02\n",
            "iteration: 237330 loss: 0.0007 lr: 0.02\n",
            "iteration: 237340 loss: 0.0008 lr: 0.02\n",
            "iteration: 237350 loss: 0.0006 lr: 0.02\n",
            "iteration: 237360 loss: 0.0011 lr: 0.02\n",
            "iteration: 237370 loss: 0.0010 lr: 0.02\n",
            "iteration: 237380 loss: 0.0006 lr: 0.02\n",
            "iteration: 237390 loss: 0.0010 lr: 0.02\n",
            "iteration: 237400 loss: 0.0007 lr: 0.02\n",
            "iteration: 237410 loss: 0.0007 lr: 0.02\n",
            "iteration: 237420 loss: 0.0007 lr: 0.02\n",
            "iteration: 237430 loss: 0.0010 lr: 0.02\n",
            "iteration: 237440 loss: 0.0009 lr: 0.02\n",
            "iteration: 237450 loss: 0.0011 lr: 0.02\n",
            "iteration: 237460 loss: 0.0006 lr: 0.02\n",
            "iteration: 237470 loss: 0.0008 lr: 0.02\n",
            "iteration: 237480 loss: 0.0009 lr: 0.02\n",
            "iteration: 237490 loss: 0.0009 lr: 0.02\n",
            "iteration: 237500 loss: 0.0010 lr: 0.02\n",
            "iteration: 237510 loss: 0.0006 lr: 0.02\n",
            "iteration: 237520 loss: 0.0006 lr: 0.02\n",
            "iteration: 237530 loss: 0.0006 lr: 0.02\n",
            "iteration: 237540 loss: 0.0008 lr: 0.02\n",
            "iteration: 237550 loss: 0.0008 lr: 0.02\n",
            "iteration: 237560 loss: 0.0006 lr: 0.02\n",
            "iteration: 237570 loss: 0.0007 lr: 0.02\n",
            "iteration: 237580 loss: 0.0007 lr: 0.02\n",
            "iteration: 237590 loss: 0.0008 lr: 0.02\n",
            "iteration: 237600 loss: 0.0006 lr: 0.02\n",
            "iteration: 237610 loss: 0.0009 lr: 0.02\n",
            "iteration: 237620 loss: 0.0010 lr: 0.02\n",
            "iteration: 237630 loss: 0.0008 lr: 0.02\n",
            "iteration: 237640 loss: 0.0008 lr: 0.02\n",
            "iteration: 237650 loss: 0.0006 lr: 0.02\n",
            "iteration: 237660 loss: 0.0010 lr: 0.02\n",
            "iteration: 237670 loss: 0.0007 lr: 0.02\n",
            "iteration: 237680 loss: 0.0007 lr: 0.02\n",
            "iteration: 237690 loss: 0.0007 lr: 0.02\n",
            "iteration: 237700 loss: 0.0007 lr: 0.02\n",
            "iteration: 237710 loss: 0.0006 lr: 0.02\n",
            "iteration: 237720 loss: 0.0008 lr: 0.02\n",
            "iteration: 237730 loss: 0.0007 lr: 0.02\n",
            "iteration: 237740 loss: 0.0008 lr: 0.02\n",
            "iteration: 237750 loss: 0.0008 lr: 0.02\n",
            "iteration: 237760 loss: 0.0008 lr: 0.02\n",
            "iteration: 237770 loss: 0.0007 lr: 0.02\n",
            "iteration: 237780 loss: 0.0009 lr: 0.02\n",
            "iteration: 237790 loss: 0.0009 lr: 0.02\n",
            "iteration: 237800 loss: 0.0009 lr: 0.02\n",
            "iteration: 237810 loss: 0.0007 lr: 0.02\n",
            "iteration: 237820 loss: 0.0007 lr: 0.02\n",
            "iteration: 237830 loss: 0.0006 lr: 0.02\n",
            "iteration: 237840 loss: 0.0011 lr: 0.02\n",
            "iteration: 237850 loss: 0.0007 lr: 0.02\n",
            "iteration: 237860 loss: 0.0008 lr: 0.02\n",
            "iteration: 237870 loss: 0.0007 lr: 0.02\n",
            "iteration: 237880 loss: 0.0011 lr: 0.02\n",
            "iteration: 237890 loss: 0.0009 lr: 0.02\n",
            "iteration: 237900 loss: 0.0006 lr: 0.02\n",
            "iteration: 237910 loss: 0.0009 lr: 0.02\n",
            "iteration: 237920 loss: 0.0011 lr: 0.02\n",
            "iteration: 237930 loss: 0.0008 lr: 0.02\n",
            "iteration: 237940 loss: 0.0007 lr: 0.02\n",
            "iteration: 237950 loss: 0.0008 lr: 0.02\n",
            "iteration: 237960 loss: 0.0008 lr: 0.02\n",
            "iteration: 237970 loss: 0.0008 lr: 0.02\n",
            "iteration: 237980 loss: 0.0009 lr: 0.02\n",
            "iteration: 237990 loss: 0.0012 lr: 0.02\n",
            "iteration: 238000 loss: 0.0012 lr: 0.02\n",
            "iteration: 238010 loss: 0.0011 lr: 0.02\n",
            "iteration: 238020 loss: 0.0008 lr: 0.02\n",
            "iteration: 238030 loss: 0.0009 lr: 0.02\n",
            "iteration: 238040 loss: 0.0007 lr: 0.02\n",
            "iteration: 238050 loss: 0.0009 lr: 0.02\n",
            "iteration: 238060 loss: 0.0007 lr: 0.02\n",
            "iteration: 238070 loss: 0.0007 lr: 0.02\n",
            "iteration: 238080 loss: 0.0008 lr: 0.02\n",
            "iteration: 238090 loss: 0.0006 lr: 0.02\n",
            "iteration: 238100 loss: 0.0008 lr: 0.02\n",
            "iteration: 238110 loss: 0.0006 lr: 0.02\n",
            "iteration: 238120 loss: 0.0007 lr: 0.02\n",
            "iteration: 238130 loss: 0.0009 lr: 0.02\n",
            "iteration: 238140 loss: 0.0008 lr: 0.02\n",
            "iteration: 238150 loss: 0.0009 lr: 0.02\n",
            "iteration: 238160 loss: 0.0015 lr: 0.02\n",
            "iteration: 238170 loss: 0.0007 lr: 0.02\n",
            "iteration: 238180 loss: 0.0008 lr: 0.02\n",
            "iteration: 238190 loss: 0.0009 lr: 0.02\n",
            "iteration: 238200 loss: 0.0007 lr: 0.02\n",
            "iteration: 238210 loss: 0.0010 lr: 0.02\n",
            "iteration: 238220 loss: 0.0008 lr: 0.02\n",
            "iteration: 238230 loss: 0.0009 lr: 0.02\n",
            "iteration: 238240 loss: 0.0009 lr: 0.02\n",
            "iteration: 238250 loss: 0.0007 lr: 0.02\n",
            "iteration: 238260 loss: 0.0009 lr: 0.02\n",
            "iteration: 238270 loss: 0.0012 lr: 0.02\n",
            "iteration: 238280 loss: 0.0006 lr: 0.02\n",
            "iteration: 238290 loss: 0.0009 lr: 0.02\n",
            "iteration: 238300 loss: 0.0007 lr: 0.02\n",
            "iteration: 238310 loss: 0.0010 lr: 0.02\n",
            "iteration: 238320 loss: 0.0008 lr: 0.02\n",
            "iteration: 238330 loss: 0.0009 lr: 0.02\n",
            "iteration: 238340 loss: 0.0009 lr: 0.02\n",
            "iteration: 238350 loss: 0.0009 lr: 0.02\n",
            "iteration: 238360 loss: 0.0006 lr: 0.02\n",
            "iteration: 238370 loss: 0.0011 lr: 0.02\n",
            "iteration: 238380 loss: 0.0008 lr: 0.02\n",
            "iteration: 238390 loss: 0.0011 lr: 0.02\n",
            "iteration: 238400 loss: 0.0008 lr: 0.02\n",
            "iteration: 238410 loss: 0.0007 lr: 0.02\n",
            "iteration: 238420 loss: 0.0011 lr: 0.02\n",
            "iteration: 238430 loss: 0.0010 lr: 0.02\n",
            "iteration: 238440 loss: 0.0011 lr: 0.02\n",
            "iteration: 238450 loss: 0.0006 lr: 0.02\n",
            "iteration: 238460 loss: 0.0010 lr: 0.02\n",
            "iteration: 238470 loss: 0.0006 lr: 0.02\n",
            "iteration: 238480 loss: 0.0007 lr: 0.02\n",
            "iteration: 238490 loss: 0.0007 lr: 0.02\n",
            "iteration: 238500 loss: 0.0008 lr: 0.02\n",
            "iteration: 238510 loss: 0.0009 lr: 0.02\n",
            "iteration: 238520 loss: 0.0006 lr: 0.02\n",
            "iteration: 238530 loss: 0.0010 lr: 0.02\n",
            "iteration: 238540 loss: 0.0006 lr: 0.02\n",
            "iteration: 238550 loss: 0.0012 lr: 0.02\n",
            "iteration: 238560 loss: 0.0010 lr: 0.02\n",
            "iteration: 238570 loss: 0.0009 lr: 0.02\n",
            "iteration: 238580 loss: 0.0006 lr: 0.02\n",
            "iteration: 238590 loss: 0.0007 lr: 0.02\n",
            "iteration: 238600 loss: 0.0011 lr: 0.02\n",
            "iteration: 238610 loss: 0.0009 lr: 0.02\n",
            "iteration: 238620 loss: 0.0010 lr: 0.02\n",
            "iteration: 238630 loss: 0.0010 lr: 0.02\n",
            "iteration: 238640 loss: 0.0009 lr: 0.02\n",
            "iteration: 238650 loss: 0.0006 lr: 0.02\n",
            "iteration: 238660 loss: 0.0008 lr: 0.02\n",
            "iteration: 238670 loss: 0.0006 lr: 0.02\n",
            "iteration: 238680 loss: 0.0006 lr: 0.02\n",
            "iteration: 238690 loss: 0.0009 lr: 0.02\n",
            "iteration: 238700 loss: 0.0006 lr: 0.02\n",
            "iteration: 238710 loss: 0.0005 lr: 0.02\n",
            "iteration: 238720 loss: 0.0006 lr: 0.02\n",
            "iteration: 238730 loss: 0.0005 lr: 0.02\n",
            "iteration: 238740 loss: 0.0009 lr: 0.02\n",
            "iteration: 238750 loss: 0.0009 lr: 0.02\n",
            "iteration: 238760 loss: 0.0008 lr: 0.02\n",
            "iteration: 238770 loss: 0.0008 lr: 0.02\n",
            "iteration: 238780 loss: 0.0010 lr: 0.02\n",
            "iteration: 238790 loss: 0.0008 lr: 0.02\n",
            "iteration: 238800 loss: 0.0009 lr: 0.02\n",
            "iteration: 238810 loss: 0.0008 lr: 0.02\n",
            "iteration: 238820 loss: 0.0009 lr: 0.02\n",
            "iteration: 238830 loss: 0.0009 lr: 0.02\n",
            "iteration: 238840 loss: 0.0006 lr: 0.02\n",
            "iteration: 238850 loss: 0.0008 lr: 0.02\n",
            "iteration: 238860 loss: 0.0010 lr: 0.02\n",
            "iteration: 238870 loss: 0.0009 lr: 0.02\n",
            "iteration: 238880 loss: 0.0009 lr: 0.02\n",
            "iteration: 238890 loss: 0.0008 lr: 0.02\n",
            "iteration: 238900 loss: 0.0008 lr: 0.02\n",
            "iteration: 238910 loss: 0.0008 lr: 0.02\n",
            "iteration: 238920 loss: 0.0008 lr: 0.02\n",
            "iteration: 238930 loss: 0.0006 lr: 0.02\n",
            "iteration: 238940 loss: 0.0007 lr: 0.02\n",
            "iteration: 238950 loss: 0.0007 lr: 0.02\n",
            "iteration: 238960 loss: 0.0005 lr: 0.02\n",
            "iteration: 238970 loss: 0.0012 lr: 0.02\n",
            "iteration: 238980 loss: 0.0006 lr: 0.02\n",
            "iteration: 238990 loss: 0.0007 lr: 0.02\n",
            "iteration: 239000 loss: 0.0006 lr: 0.02\n",
            "iteration: 239010 loss: 0.0007 lr: 0.02\n",
            "iteration: 239020 loss: 0.0007 lr: 0.02\n",
            "iteration: 239030 loss: 0.0007 lr: 0.02\n",
            "iteration: 239040 loss: 0.0005 lr: 0.02\n",
            "iteration: 239050 loss: 0.0006 lr: 0.02\n",
            "iteration: 239060 loss: 0.0008 lr: 0.02\n",
            "iteration: 239070 loss: 0.0011 lr: 0.02\n",
            "iteration: 239080 loss: 0.0006 lr: 0.02\n",
            "iteration: 239090 loss: 0.0006 lr: 0.02\n",
            "iteration: 239100 loss: 0.0005 lr: 0.02\n",
            "iteration: 239110 loss: 0.0008 lr: 0.02\n",
            "iteration: 239120 loss: 0.0008 lr: 0.02\n",
            "iteration: 239130 loss: 0.0008 lr: 0.02\n",
            "iteration: 239140 loss: 0.0008 lr: 0.02\n",
            "iteration: 239150 loss: 0.0007 lr: 0.02\n",
            "iteration: 239160 loss: 0.0006 lr: 0.02\n",
            "iteration: 239170 loss: 0.0011 lr: 0.02\n",
            "iteration: 239180 loss: 0.0006 lr: 0.02\n",
            "iteration: 239190 loss: 0.0008 lr: 0.02\n",
            "iteration: 239200 loss: 0.0008 lr: 0.02\n",
            "iteration: 239210 loss: 0.0007 lr: 0.02\n",
            "iteration: 239220 loss: 0.0012 lr: 0.02\n",
            "iteration: 239230 loss: 0.0009 lr: 0.02\n",
            "iteration: 239240 loss: 0.0008 lr: 0.02\n",
            "iteration: 239250 loss: 0.0008 lr: 0.02\n",
            "iteration: 239260 loss: 0.0007 lr: 0.02\n",
            "iteration: 239270 loss: 0.0006 lr: 0.02\n",
            "iteration: 239280 loss: 0.0008 lr: 0.02\n",
            "iteration: 239290 loss: 0.0010 lr: 0.02\n",
            "iteration: 239300 loss: 0.0008 lr: 0.02\n",
            "iteration: 239310 loss: 0.0008 lr: 0.02\n",
            "iteration: 239320 loss: 0.0007 lr: 0.02\n",
            "iteration: 239330 loss: 0.0005 lr: 0.02\n",
            "iteration: 239340 loss: 0.0007 lr: 0.02\n",
            "iteration: 239350 loss: 0.0005 lr: 0.02\n",
            "iteration: 239360 loss: 0.0008 lr: 0.02\n",
            "iteration: 239370 loss: 0.0007 lr: 0.02\n",
            "iteration: 239380 loss: 0.0011 lr: 0.02\n",
            "iteration: 239390 loss: 0.0006 lr: 0.02\n",
            "iteration: 239400 loss: 0.0009 lr: 0.02\n",
            "iteration: 239410 loss: 0.0008 lr: 0.02\n",
            "iteration: 239420 loss: 0.0006 lr: 0.02\n",
            "iteration: 239430 loss: 0.0011 lr: 0.02\n",
            "iteration: 239440 loss: 0.0008 lr: 0.02\n",
            "iteration: 239450 loss: 0.0005 lr: 0.02\n",
            "iteration: 239460 loss: 0.0006 lr: 0.02\n",
            "iteration: 239470 loss: 0.0006 lr: 0.02\n",
            "iteration: 239480 loss: 0.0009 lr: 0.02\n",
            "iteration: 239490 loss: 0.0008 lr: 0.02\n",
            "iteration: 239500 loss: 0.0007 lr: 0.02\n",
            "iteration: 239510 loss: 0.0007 lr: 0.02\n",
            "iteration: 239520 loss: 0.0006 lr: 0.02\n",
            "iteration: 239530 loss: 0.0008 lr: 0.02\n",
            "iteration: 239540 loss: 0.0006 lr: 0.02\n",
            "iteration: 239550 loss: 0.0008 lr: 0.02\n",
            "iteration: 239560 loss: 0.0008 lr: 0.02\n",
            "iteration: 239570 loss: 0.0005 lr: 0.02\n",
            "iteration: 239580 loss: 0.0007 lr: 0.02\n",
            "iteration: 239590 loss: 0.0009 lr: 0.02\n",
            "iteration: 239600 loss: 0.0007 lr: 0.02\n",
            "iteration: 239610 loss: 0.0013 lr: 0.02\n",
            "iteration: 239620 loss: 0.0008 lr: 0.02\n",
            "iteration: 239630 loss: 0.0009 lr: 0.02\n",
            "iteration: 239640 loss: 0.0008 lr: 0.02\n",
            "iteration: 239650 loss: 0.0008 lr: 0.02\n",
            "iteration: 239660 loss: 0.0008 lr: 0.02\n",
            "iteration: 239670 loss: 0.0005 lr: 0.02\n",
            "iteration: 239680 loss: 0.0007 lr: 0.02\n",
            "iteration: 239690 loss: 0.0009 lr: 0.02\n",
            "iteration: 239700 loss: 0.0007 lr: 0.02\n",
            "iteration: 239710 loss: 0.0007 lr: 0.02\n",
            "iteration: 239720 loss: 0.0007 lr: 0.02\n",
            "iteration: 239730 loss: 0.0008 lr: 0.02\n",
            "iteration: 239740 loss: 0.0006 lr: 0.02\n",
            "iteration: 239750 loss: 0.0009 lr: 0.02\n",
            "iteration: 239760 loss: 0.0010 lr: 0.02\n",
            "iteration: 239770 loss: 0.0007 lr: 0.02\n",
            "iteration: 239780 loss: 0.0011 lr: 0.02\n",
            "iteration: 239790 loss: 0.0006 lr: 0.02\n",
            "iteration: 239800 loss: 0.0009 lr: 0.02\n",
            "iteration: 239810 loss: 0.0007 lr: 0.02\n",
            "iteration: 239820 loss: 0.0009 lr: 0.02\n",
            "iteration: 239830 loss: 0.0008 lr: 0.02\n",
            "iteration: 239840 loss: 0.0008 lr: 0.02\n",
            "iteration: 239850 loss: 0.0007 lr: 0.02\n",
            "iteration: 239860 loss: 0.0009 lr: 0.02\n",
            "iteration: 239870 loss: 0.0010 lr: 0.02\n",
            "iteration: 239880 loss: 0.0007 lr: 0.02\n",
            "iteration: 239890 loss: 0.0010 lr: 0.02\n",
            "iteration: 239900 loss: 0.0008 lr: 0.02\n",
            "iteration: 239910 loss: 0.0008 lr: 0.02\n",
            "iteration: 239920 loss: 0.0007 lr: 0.02\n",
            "iteration: 239930 loss: 0.0005 lr: 0.02\n",
            "iteration: 239940 loss: 0.0008 lr: 0.02\n",
            "iteration: 239950 loss: 0.0007 lr: 0.02\n",
            "iteration: 239960 loss: 0.0006 lr: 0.02\n",
            "iteration: 239970 loss: 0.0007 lr: 0.02\n",
            "iteration: 239980 loss: 0.0008 lr: 0.02\n",
            "iteration: 239990 loss: 0.0009 lr: 0.02\n",
            "iteration: 240000 loss: 0.0008 lr: 0.02\n",
            "iteration: 240010 loss: 0.0009 lr: 0.02\n",
            "iteration: 240020 loss: 0.0008 lr: 0.02\n",
            "iteration: 240030 loss: 0.0008 lr: 0.02\n",
            "iteration: 240040 loss: 0.0007 lr: 0.02\n",
            "iteration: 240050 loss: 0.0011 lr: 0.02\n",
            "iteration: 240060 loss: 0.0008 lr: 0.02\n",
            "iteration: 240070 loss: 0.0010 lr: 0.02\n",
            "iteration: 240080 loss: 0.0006 lr: 0.02\n",
            "iteration: 240090 loss: 0.0013 lr: 0.02\n",
            "iteration: 240100 loss: 0.0006 lr: 0.02\n",
            "iteration: 240110 loss: 0.0006 lr: 0.02\n",
            "iteration: 240120 loss: 0.0006 lr: 0.02\n",
            "iteration: 240130 loss: 0.0009 lr: 0.02\n",
            "iteration: 240140 loss: 0.0008 lr: 0.02\n",
            "iteration: 240150 loss: 0.0008 lr: 0.02\n",
            "iteration: 240160 loss: 0.0006 lr: 0.02\n",
            "iteration: 240170 loss: 0.0008 lr: 0.02\n",
            "iteration: 240180 loss: 0.0007 lr: 0.02\n",
            "iteration: 240190 loss: 0.0007 lr: 0.02\n",
            "iteration: 240200 loss: 0.0008 lr: 0.02\n",
            "iteration: 240210 loss: 0.0008 lr: 0.02\n",
            "iteration: 240220 loss: 0.0008 lr: 0.02\n",
            "iteration: 240230 loss: 0.0008 lr: 0.02\n",
            "iteration: 240240 loss: 0.0006 lr: 0.02\n",
            "iteration: 240250 loss: 0.0010 lr: 0.02\n",
            "iteration: 240260 loss: 0.0009 lr: 0.02\n",
            "iteration: 240270 loss: 0.0006 lr: 0.02\n",
            "iteration: 240280 loss: 0.0008 lr: 0.02\n",
            "iteration: 240290 loss: 0.0007 lr: 0.02\n",
            "iteration: 240300 loss: 0.0007 lr: 0.02\n",
            "iteration: 240310 loss: 0.0009 lr: 0.02\n",
            "iteration: 240320 loss: 0.0007 lr: 0.02\n",
            "iteration: 240330 loss: 0.0008 lr: 0.02\n",
            "iteration: 240340 loss: 0.0006 lr: 0.02\n",
            "iteration: 240350 loss: 0.0008 lr: 0.02\n",
            "iteration: 240360 loss: 0.0012 lr: 0.02\n",
            "iteration: 240370 loss: 0.0005 lr: 0.02\n",
            "iteration: 240380 loss: 0.0011 lr: 0.02\n",
            "iteration: 240390 loss: 0.0007 lr: 0.02\n",
            "iteration: 240400 loss: 0.0008 lr: 0.02\n",
            "iteration: 240410 loss: 0.0009 lr: 0.02\n",
            "iteration: 240420 loss: 0.0009 lr: 0.02\n",
            "iteration: 240430 loss: 0.0016 lr: 0.02\n",
            "iteration: 240440 loss: 0.0006 lr: 0.02\n",
            "iteration: 240450 loss: 0.0006 lr: 0.02\n",
            "iteration: 240460 loss: 0.0009 lr: 0.02\n",
            "iteration: 240470 loss: 0.0008 lr: 0.02\n",
            "iteration: 240480 loss: 0.0008 lr: 0.02\n",
            "iteration: 240490 loss: 0.0008 lr: 0.02\n",
            "iteration: 240500 loss: 0.0007 lr: 0.02\n",
            "iteration: 240510 loss: 0.0007 lr: 0.02\n",
            "iteration: 240520 loss: 0.0006 lr: 0.02\n",
            "iteration: 240530 loss: 0.0008 lr: 0.02\n",
            "iteration: 240540 loss: 0.0007 lr: 0.02\n",
            "iteration: 240550 loss: 0.0005 lr: 0.02\n",
            "iteration: 240560 loss: 0.0006 lr: 0.02\n",
            "iteration: 240570 loss: 0.0010 lr: 0.02\n",
            "iteration: 240580 loss: 0.0006 lr: 0.02\n",
            "iteration: 240590 loss: 0.0005 lr: 0.02\n",
            "iteration: 240600 loss: 0.0005 lr: 0.02\n",
            "iteration: 240610 loss: 0.0007 lr: 0.02\n",
            "iteration: 240620 loss: 0.0007 lr: 0.02\n",
            "iteration: 240630 loss: 0.0010 lr: 0.02\n",
            "iteration: 240640 loss: 0.0009 lr: 0.02\n",
            "iteration: 240650 loss: 0.0005 lr: 0.02\n",
            "iteration: 240660 loss: 0.0006 lr: 0.02\n",
            "iteration: 240670 loss: 0.0009 lr: 0.02\n",
            "iteration: 240680 loss: 0.0006 lr: 0.02\n",
            "iteration: 240690 loss: 0.0008 lr: 0.02\n",
            "iteration: 240700 loss: 0.0010 lr: 0.02\n",
            "iteration: 240710 loss: 0.0006 lr: 0.02\n",
            "iteration: 240720 loss: 0.0009 lr: 0.02\n",
            "iteration: 240730 loss: 0.0006 lr: 0.02\n",
            "iteration: 240740 loss: 0.0007 lr: 0.02\n",
            "iteration: 240750 loss: 0.0008 lr: 0.02\n",
            "iteration: 240760 loss: 0.0006 lr: 0.02\n",
            "iteration: 240770 loss: 0.0006 lr: 0.02\n",
            "iteration: 240780 loss: 0.0006 lr: 0.02\n",
            "iteration: 240790 loss: 0.0017 lr: 0.02\n",
            "iteration: 240800 loss: 0.0008 lr: 0.02\n",
            "iteration: 240810 loss: 0.0007 lr: 0.02\n",
            "iteration: 240820 loss: 0.0005 lr: 0.02\n",
            "iteration: 240830 loss: 0.0007 lr: 0.02\n",
            "iteration: 240840 loss: 0.0007 lr: 0.02\n",
            "iteration: 240850 loss: 0.0010 lr: 0.02\n",
            "iteration: 240860 loss: 0.0009 lr: 0.02\n",
            "iteration: 240870 loss: 0.0007 lr: 0.02\n",
            "iteration: 240880 loss: 0.0007 lr: 0.02\n",
            "iteration: 240890 loss: 0.0007 lr: 0.02\n",
            "iteration: 240900 loss: 0.0010 lr: 0.02\n",
            "iteration: 240910 loss: 0.0007 lr: 0.02\n",
            "iteration: 240920 loss: 0.0006 lr: 0.02\n",
            "iteration: 240930 loss: 0.0008 lr: 0.02\n",
            "iteration: 240940 loss: 0.0007 lr: 0.02\n",
            "iteration: 240950 loss: 0.0010 lr: 0.02\n",
            "iteration: 240960 loss: 0.0005 lr: 0.02\n",
            "iteration: 240970 loss: 0.0007 lr: 0.02\n",
            "iteration: 240980 loss: 0.0007 lr: 0.02\n",
            "iteration: 240990 loss: 0.0006 lr: 0.02\n",
            "iteration: 241000 loss: 0.0007 lr: 0.02\n",
            "iteration: 241010 loss: 0.0007 lr: 0.02\n",
            "iteration: 241020 loss: 0.0009 lr: 0.02\n",
            "iteration: 241030 loss: 0.0009 lr: 0.02\n",
            "iteration: 241040 loss: 0.0007 lr: 0.02\n",
            "iteration: 241050 loss: 0.0005 lr: 0.02\n",
            "iteration: 241060 loss: 0.0006 lr: 0.02\n",
            "iteration: 241070 loss: 0.0007 lr: 0.02\n",
            "iteration: 241080 loss: 0.0007 lr: 0.02\n",
            "iteration: 241090 loss: 0.0007 lr: 0.02\n",
            "iteration: 241100 loss: 0.0008 lr: 0.02\n",
            "iteration: 241110 loss: 0.0011 lr: 0.02\n",
            "iteration: 241120 loss: 0.0008 lr: 0.02\n",
            "iteration: 241130 loss: 0.0007 lr: 0.02\n",
            "iteration: 241140 loss: 0.0006 lr: 0.02\n",
            "iteration: 241150 loss: 0.0010 lr: 0.02\n",
            "iteration: 241160 loss: 0.0008 lr: 0.02\n",
            "iteration: 241170 loss: 0.0005 lr: 0.02\n",
            "iteration: 241180 loss: 0.0011 lr: 0.02\n",
            "iteration: 241190 loss: 0.0007 lr: 0.02\n",
            "iteration: 241200 loss: 0.0013 lr: 0.02\n",
            "iteration: 241210 loss: 0.0009 lr: 0.02\n",
            "iteration: 241220 loss: 0.0011 lr: 0.02\n",
            "iteration: 241230 loss: 0.0009 lr: 0.02\n",
            "iteration: 241240 loss: 0.0010 lr: 0.02\n",
            "iteration: 241250 loss: 0.0007 lr: 0.02\n",
            "iteration: 241260 loss: 0.0008 lr: 0.02\n",
            "iteration: 241270 loss: 0.0010 lr: 0.02\n",
            "iteration: 241280 loss: 0.0007 lr: 0.02\n",
            "iteration: 241290 loss: 0.0014 lr: 0.02\n",
            "iteration: 241300 loss: 0.0008 lr: 0.02\n",
            "iteration: 241310 loss: 0.0007 lr: 0.02\n",
            "iteration: 241320 loss: 0.0007 lr: 0.02\n",
            "iteration: 241330 loss: 0.0010 lr: 0.02\n",
            "iteration: 241340 loss: 0.0007 lr: 0.02\n",
            "iteration: 241350 loss: 0.0006 lr: 0.02\n",
            "iteration: 241360 loss: 0.0008 lr: 0.02\n",
            "iteration: 241370 loss: 0.0006 lr: 0.02\n",
            "iteration: 241380 loss: 0.0007 lr: 0.02\n",
            "iteration: 241390 loss: 0.0007 lr: 0.02\n",
            "iteration: 241400 loss: 0.0009 lr: 0.02\n",
            "iteration: 241410 loss: 0.0006 lr: 0.02\n",
            "iteration: 241420 loss: 0.0008 lr: 0.02\n",
            "iteration: 241430 loss: 0.0008 lr: 0.02\n",
            "iteration: 241440 loss: 0.0009 lr: 0.02\n",
            "iteration: 241450 loss: 0.0006 lr: 0.02\n",
            "iteration: 241460 loss: 0.0006 lr: 0.02\n",
            "iteration: 241470 loss: 0.0008 lr: 0.02\n",
            "iteration: 241480 loss: 0.0008 lr: 0.02\n",
            "iteration: 241490 loss: 0.0007 lr: 0.02\n",
            "iteration: 241500 loss: 0.0011 lr: 0.02\n",
            "iteration: 241510 loss: 0.0010 lr: 0.02\n",
            "iteration: 241520 loss: 0.0009 lr: 0.02\n",
            "iteration: 241530 loss: 0.0009 lr: 0.02\n",
            "iteration: 241540 loss: 0.0009 lr: 0.02\n",
            "iteration: 241550 loss: 0.0010 lr: 0.02\n",
            "iteration: 241560 loss: 0.0006 lr: 0.02\n",
            "iteration: 241570 loss: 0.0009 lr: 0.02\n",
            "iteration: 241580 loss: 0.0006 lr: 0.02\n",
            "iteration: 241590 loss: 0.0008 lr: 0.02\n",
            "iteration: 241600 loss: 0.0007 lr: 0.02\n",
            "iteration: 241610 loss: 0.0009 lr: 0.02\n",
            "iteration: 241620 loss: 0.0008 lr: 0.02\n",
            "iteration: 241630 loss: 0.0011 lr: 0.02\n",
            "iteration: 241640 loss: 0.0007 lr: 0.02\n",
            "iteration: 241650 loss: 0.0007 lr: 0.02\n",
            "iteration: 241660 loss: 0.0008 lr: 0.02\n",
            "iteration: 241670 loss: 0.0007 lr: 0.02\n",
            "iteration: 241680 loss: 0.0009 lr: 0.02\n",
            "iteration: 241690 loss: 0.0006 lr: 0.02\n",
            "iteration: 241700 loss: 0.0010 lr: 0.02\n",
            "iteration: 241710 loss: 0.0011 lr: 0.02\n",
            "iteration: 241720 loss: 0.0008 lr: 0.02\n",
            "iteration: 241730 loss: 0.0007 lr: 0.02\n",
            "iteration: 241740 loss: 0.0009 lr: 0.02\n",
            "iteration: 241750 loss: 0.0008 lr: 0.02\n",
            "iteration: 241760 loss: 0.0010 lr: 0.02\n",
            "iteration: 241770 loss: 0.0007 lr: 0.02\n",
            "iteration: 241780 loss: 0.0006 lr: 0.02\n",
            "iteration: 241790 loss: 0.0007 lr: 0.02\n",
            "iteration: 241800 loss: 0.0009 lr: 0.02\n",
            "iteration: 241810 loss: 0.0006 lr: 0.02\n",
            "iteration: 241820 loss: 0.0006 lr: 0.02\n",
            "iteration: 241830 loss: 0.0005 lr: 0.02\n",
            "iteration: 241840 loss: 0.0008 lr: 0.02\n",
            "iteration: 241850 loss: 0.0008 lr: 0.02\n",
            "iteration: 241860 loss: 0.0006 lr: 0.02\n",
            "iteration: 241870 loss: 0.0007 lr: 0.02\n",
            "iteration: 241880 loss: 0.0008 lr: 0.02\n",
            "iteration: 241890 loss: 0.0007 lr: 0.02\n",
            "iteration: 241900 loss: 0.0009 lr: 0.02\n",
            "iteration: 241910 loss: 0.0008 lr: 0.02\n",
            "iteration: 241920 loss: 0.0010 lr: 0.02\n",
            "iteration: 241930 loss: 0.0007 lr: 0.02\n",
            "iteration: 241940 loss: 0.0007 lr: 0.02\n",
            "iteration: 241950 loss: 0.0012 lr: 0.02\n",
            "iteration: 241960 loss: 0.0007 lr: 0.02\n",
            "iteration: 241970 loss: 0.0009 lr: 0.02\n",
            "iteration: 241980 loss: 0.0006 lr: 0.02\n",
            "iteration: 241990 loss: 0.0007 lr: 0.02\n",
            "iteration: 242000 loss: 0.0007 lr: 0.02\n",
            "iteration: 242010 loss: 0.0008 lr: 0.02\n",
            "iteration: 242020 loss: 0.0007 lr: 0.02\n",
            "iteration: 242030 loss: 0.0005 lr: 0.02\n",
            "iteration: 242040 loss: 0.0008 lr: 0.02\n",
            "iteration: 242050 loss: 0.0007 lr: 0.02\n",
            "iteration: 242060 loss: 0.0008 lr: 0.02\n",
            "iteration: 242070 loss: 0.0008 lr: 0.02\n",
            "iteration: 242080 loss: 0.0007 lr: 0.02\n",
            "iteration: 242090 loss: 0.0008 lr: 0.02\n",
            "iteration: 242100 loss: 0.0009 lr: 0.02\n",
            "iteration: 242110 loss: 0.0006 lr: 0.02\n",
            "iteration: 242120 loss: 0.0008 lr: 0.02\n",
            "iteration: 242130 loss: 0.0005 lr: 0.02\n",
            "iteration: 242140 loss: 0.0009 lr: 0.02\n",
            "iteration: 242150 loss: 0.0008 lr: 0.02\n",
            "iteration: 242160 loss: 0.0009 lr: 0.02\n",
            "iteration: 242170 loss: 0.0014 lr: 0.02\n",
            "iteration: 242180 loss: 0.0011 lr: 0.02\n",
            "iteration: 242190 loss: 0.0006 lr: 0.02\n",
            "iteration: 242200 loss: 0.0009 lr: 0.02\n",
            "iteration: 242210 loss: 0.0007 lr: 0.02\n",
            "iteration: 242220 loss: 0.0007 lr: 0.02\n",
            "iteration: 242230 loss: 0.0013 lr: 0.02\n",
            "iteration: 242240 loss: 0.0009 lr: 0.02\n",
            "iteration: 242250 loss: 0.0009 lr: 0.02\n",
            "iteration: 242260 loss: 0.0008 lr: 0.02\n",
            "iteration: 242270 loss: 0.0005 lr: 0.02\n",
            "iteration: 242280 loss: 0.0007 lr: 0.02\n",
            "iteration: 242290 loss: 0.0006 lr: 0.02\n",
            "iteration: 242300 loss: 0.0010 lr: 0.02\n",
            "iteration: 242310 loss: 0.0012 lr: 0.02\n",
            "iteration: 242320 loss: 0.0010 lr: 0.02\n",
            "iteration: 242330 loss: 0.0010 lr: 0.02\n",
            "iteration: 242340 loss: 0.0007 lr: 0.02\n",
            "iteration: 242350 loss: 0.0008 lr: 0.02\n",
            "iteration: 242360 loss: 0.0007 lr: 0.02\n",
            "iteration: 242370 loss: 0.0007 lr: 0.02\n",
            "iteration: 242380 loss: 0.0008 lr: 0.02\n",
            "iteration: 242390 loss: 0.0009 lr: 0.02\n",
            "iteration: 242400 loss: 0.0007 lr: 0.02\n",
            "iteration: 242410 loss: 0.0008 lr: 0.02\n",
            "iteration: 242420 loss: 0.0010 lr: 0.02\n",
            "iteration: 242430 loss: 0.0006 lr: 0.02\n",
            "iteration: 242440 loss: 0.0011 lr: 0.02\n",
            "iteration: 242450 loss: 0.0007 lr: 0.02\n",
            "iteration: 242460 loss: 0.0010 lr: 0.02\n",
            "iteration: 242470 loss: 0.0007 lr: 0.02\n",
            "iteration: 242480 loss: 0.0010 lr: 0.02\n",
            "iteration: 242490 loss: 0.0006 lr: 0.02\n",
            "iteration: 242500 loss: 0.0007 lr: 0.02\n",
            "iteration: 242510 loss: 0.0006 lr: 0.02\n",
            "iteration: 242520 loss: 0.0011 lr: 0.02\n",
            "iteration: 242530 loss: 0.0004 lr: 0.02\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-79d7cc38f692>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#more info and there are more things you can set: https://github.com/AlexEMG/DeepLabCut/blob/master/docs/functionDetails.md#g-train-the-network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdeeplabcut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_config_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplayiters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msaveiters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#this will run until you stop it (CTRL+C), or hit \"STOP\" icon, or when it hits the end (default, 1.03M iterations).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/deeplabcut/pose_estimation_tensorflow/training.py\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(config, shuffle, trainingsetindex, max_snapshots_to_keep, displayiters, saveiters, maxiters, allow_growth, gputouse, autotune, keepdeconvweights)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposeconfigfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdisplayiters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msaveiters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxiters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_to_keep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_snapshots_to_keep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeepdeconvweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdeconvweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mallow_growth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_growth\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#pass on path and file name for pose_cfg.yaml!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/deeplabcut/pose_estimation_tensorflow/training.py\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(config, shuffle, trainingsetindex, max_snapshots_to_keep, displayiters, saveiters, maxiters, allow_growth, gputouse, autotune, keepdeconvweights)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CUDA_VISIBLE_DEVICES'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgputouse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposeconfigfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdisplayiters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msaveiters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxiters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_to_keep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_snapshots_to_keep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeepdeconvweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdeconvweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mallow_growth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_growth\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#pass on path and file name for pose_cfg.yaml!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/deeplabcut/pose_estimation_tensorflow/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(config_yaml, displayiters, saveiters, maxiters, max_to_keep, keepdeconvweights, allow_growth)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mcurrent_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         [_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],\n\u001b[0;32m--> 190\u001b[0;31m                                           feed_dict={learning_rate: current_lr})\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0mcum_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mtrain_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RiDwIVf5-3H_"
      },
      "source": [
        "**When you hit \"STOP\" you will get a KeyInterrupt \"error\"! No worries! :)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xZygsb2DoEJc"
      },
      "source": [
        "## Start evaluating:\n",
        "This funtion evaluates a trained model for a specific shuffle/shuffles at a particular state or all the states on the data set (images)\n",
        "and stores the results as .csv file in a subdirectory under **evaluation-results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nv4zlbrnoEJg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7d99aaf5-1432-4c19-c96c-ff0215250a98"
      },
      "source": [
        "%matplotlib notebook\n",
        "deeplabcut.evaluate_network(path_config_file)\n",
        "path_config_file\n",
        "# Here you want to see a low pixel error! Of course, it can only be as good as the labeler, \n",
        "#so be sure your labels are good! (And you have trained enough ;)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Config:\n",
            "{'all_joints': [[0], [1]],\n",
            " 'all_joints_names': ['red_led', 'green_led'],\n",
            " 'batch_size': 1,\n",
            " 'bottomheight': 400,\n",
            " 'crop': True,\n",
            " 'crop_pad': 0,\n",
            " 'cropratio': 0.4,\n",
            " 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_ephysSep18/ephys_Berkowitz95shuffle1.mat',\n",
            " 'dataset_type': 'imgaug',\n",
            " 'deconvolutionstride': 2,\n",
            " 'deterministic': False,\n",
            " 'display_iters': 1000,\n",
            " 'fg_fraction': 0.25,\n",
            " 'global_scale': 0.8,\n",
            " 'init_weights': '/usr/local/lib/python3.6/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_101.ckpt',\n",
            " 'intermediate_supervision': False,\n",
            " 'intermediate_supervision_layer': 12,\n",
            " 'leftwidth': 400,\n",
            " 'location_refinement': True,\n",
            " 'locref_huber_loss': True,\n",
            " 'locref_loss_weight': 0.05,\n",
            " 'locref_stdev': 7.2801,\n",
            " 'log_dir': 'log',\n",
            " 'max_input_size': 1500,\n",
            " 'mean_pixel': [123.68, 116.779, 103.939],\n",
            " 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_ephysSep18/Documentation_data-ephys_95shuffle1.pickle',\n",
            " 'min_input_size': 64,\n",
            " 'minsize': 100,\n",
            " 'mirror': False,\n",
            " 'multi_step': [[0.005, 10000],\n",
            "                [0.02, 430000],\n",
            "                [0.002, 730000],\n",
            "                [0.001, 1030000]],\n",
            " 'net_type': 'resnet_101',\n",
            " 'num_joints': 2,\n",
            " 'optimizer': 'sgd',\n",
            " 'output_stride': 16,\n",
            " 'pos_dist_thresh': 17,\n",
            " 'project_path': '/content/drive/My '\n",
            "                 'Drive/DLC_analysis/ephys-Berkowitz-2020-09-18',\n",
            " 'regularize': False,\n",
            " 'rightwidth': 400,\n",
            " 'save_iters': 50000,\n",
            " 'scale_jitter_lo': 0.5,\n",
            " 'scale_jitter_up': 1.25,\n",
            " 'scoremap_dir': 'test',\n",
            " 'shuffle': True,\n",
            " 'snapshot_prefix': '/content/drive/My '\n",
            "                    'Drive/DLC_analysis/ephys-Berkowitz-2020-09-18/dlc-models/iteration-0/ephysSep18-trainset95shuffle1/test/snapshot',\n",
            " 'stride': 8.0,\n",
            " 'topheight': 400,\n",
            " 'weigh_negatives': False,\n",
            " 'weigh_only_present_joints': False,\n",
            " 'weigh_part_predictions': False,\n",
            " 'weight_decay': 0.0001}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/DLC_analysis/ephys-Berkowitz-2020-09-18/evaluation-results/  already exists!\n",
            "/content/drive/My Drive/DLC_analysis/ephys-Berkowitz-2020-09-18/evaluation-results/iteration-0/ephysSep18-trainset95shuffle1  already exists!\n",
            "Running  DLC_resnet101_ephysSep18shuffle1_242500  with # of trainingiterations: 242500\n",
            "Initializing ResNet\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/DLC_analysis/ephys-Berkowitz-2020-09-18/dlc-models/iteration-0/ephysSep18-trainset95shuffle1/train/snapshot-242500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Analyzing data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "120it [00:07, 16.92it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done and results stored for snapshot:  snapshot-242500\n",
            "Results for 242500  training iterations: 95 1 train error: 1.3 pixels. Test error: 1.8  pixels.\n",
            "With pcutoff of 0.6  train error: 1.3 pixels. Test error: 1.8 pixels\n",
            "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
            "The network is evaluated and the results are stored in the subdirectory 'evaluation_results'.\n",
            "If it generalizes well, choose the best model for prediction and update the config file with the appropriate index for the 'snapshotindex'.\n",
            "Use the function 'analyze_video' to make predictions on new videos.\n",
            "Otherwise consider retraining the network (see DeepLabCut workflow Fig 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/DLC_analysis/ephys-Berkowitz-2020-09-18/config.yaml'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BaLBl3TQtrfB"
      },
      "source": [
        "## There is an optional refinement step you can do outside of Colab:\n",
        "- if your pixel errors are not low enough, please check out the protocol guide on how to refine your network!\n",
        "- You will need to adjust the labels **outside of Colab!** We recommend coming back to train and analyze videos... \n",
        "- pplease see the repo and protocol instructions on how to refine your data!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OVFLSKKfoEJk"
      },
      "source": [
        "## Start Analyzing videos: \n",
        "This function analyzes the new video. The user can choose the best model from the evaluation results and specify the correct snapshot index for the variable **snapshotindex** in the **config.yaml** file. Otherwise, by default the most recent snapshot is used to analyse the video.\n",
        "\n",
        "The results are stored in hd5 file in the same directory where the video resides. \n",
        "\n",
        "* dynamic set to true to maintain tracking around targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y_LZiS_0oEJl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7e46bd70-4e00-4a41-a552-b1d80df9bd5c"
      },
      "source": [
        "deeplabcut.analyze_videos(path_config_file,videofile_path, videotype=VideoType,save_as_csv=True,dynamic = (True,.1,90))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Config:\n",
            "{'all_joints': [[0], [1]],\n",
            " 'all_joints_names': ['red_led', 'green_led'],\n",
            " 'batch_size': 1,\n",
            " 'bottomheight': 400,\n",
            " 'crop': True,\n",
            " 'crop_pad': 0,\n",
            " 'cropratio': 0.4,\n",
            " 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_ephysSep18/ephys_Berkowitz95shuffle1.mat',\n",
            " 'dataset_type': 'imgaug',\n",
            " 'deconvolutionstride': 2,\n",
            " 'deterministic': False,\n",
            " 'display_iters': 1000,\n",
            " 'fg_fraction': 0.25,\n",
            " 'global_scale': 0.8,\n",
            " 'init_weights': '/usr/local/lib/python3.6/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_101.ckpt',\n",
            " 'intermediate_supervision': False,\n",
            " 'intermediate_supervision_layer': 12,\n",
            " 'leftwidth': 400,\n",
            " 'location_refinement': True,\n",
            " 'locref_huber_loss': True,\n",
            " 'locref_loss_weight': 0.05,\n",
            " 'locref_stdev': 7.2801,\n",
            " 'log_dir': 'log',\n",
            " 'max_input_size': 1500,\n",
            " 'mean_pixel': [123.68, 116.779, 103.939],\n",
            " 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_ephysSep18/Documentation_data-ephys_95shuffle1.pickle',\n",
            " 'min_input_size': 64,\n",
            " 'minsize': 100,\n",
            " 'mirror': False,\n",
            " 'multi_step': [[0.005, 10000],\n",
            "                [0.02, 430000],\n",
            "                [0.002, 730000],\n",
            "                [0.001, 1030000]],\n",
            " 'net_type': 'resnet_101',\n",
            " 'num_joints': 2,\n",
            " 'optimizer': 'sgd',\n",
            " 'output_stride': 16,\n",
            " 'pos_dist_thresh': 17,\n",
            " 'project_path': '/content/drive/My '\n",
            "                 'Drive/DLC_analysis/ephys-Berkowitz-2020-09-18',\n",
            " 'regularize': False,\n",
            " 'rightwidth': 400,\n",
            " 'save_iters': 50000,\n",
            " 'scale_jitter_lo': 0.5,\n",
            " 'scale_jitter_up': 1.25,\n",
            " 'scoremap_dir': 'test',\n",
            " 'shuffle': True,\n",
            " 'snapshot_prefix': '/content/drive/My '\n",
            "                    'Drive/DLC_analysis/ephys-Berkowitz-2020-09-18/dlc-models/iteration-0/ephysSep18-trainset95shuffle1/test/snapshot',\n",
            " 'stride': 8.0,\n",
            " 'topheight': 400,\n",
            " 'weigh_negatives': False,\n",
            " 'weigh_only_present_joints': False,\n",
            " 'weigh_part_predictions': False,\n",
            " 'weight_decay': 0.0001}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Using snapshot-242500 for model /content/drive/My Drive/DLC_analysis/ephys-Berkowitz-2020-09-18/dlc-models/iteration-0/ephysSep18-trainset95shuffle1\n",
            "Starting analysis in dynamic cropping mode with parameters: (True, 0.1, 90)\n",
            "Switching batchsize to 1, num_outputs (per animal) to 1 and TFGPUinference to False (all these features are not supported in this mode).\n",
            "Initializing ResNet\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/DLC_analysis/ephys-Berkowitz-2020-09-18/dlc-models/iteration-0/ephysSep18-trainset95shuffle1/train/snapshot-242500\n",
            "Analyzing all the videos in the directory\n",
            "Starting to analyze %  LB11_VT1.avi\n",
            "Loading  LB11_VT1.avi\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/12945 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Duration of video [s]:  431.93 , recorded with  29.97 fps!\n",
            "Overall # of frames:  12945  found with (before cropping) frame dimensions:  720 480\n",
            "Starting to extract posture\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "13029it [03:46, 57.51it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saving results in ....\n",
            "Saving csv poses!\n",
            "Starting to analyze %  2020-04-06_16-17-15_VT1.avi\n",
            "Loading  2020-04-06_16-17-15_VT1.avi\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/130127 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Duration of video [s]:  4341.9 , recorded with  29.97 fps!\n",
            "Overall # of frames:  130127  found with (before cropping) frame dimensions:  720 480\n",
            "Starting to extract posture\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 14%|█▍        | 18214/130127 [14:20<1:23:22, 22.37it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8GTiuJESoEKH"
      },
      "source": [
        "## Plot the trajectories of the analyzed videos:\n",
        "This function plots the trajectories of all the body parts across the entire video. Each body part is identified by a unique color."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gX21zZbXoEKJ",
        "colab": {}
      },
      "source": [
        "deeplabcut.plot_trajectories(path_config_file,videofile_path, videotype=VideoType)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pqaCw15v8EmB"
      },
      "source": [
        "Now you can look at the plot-poses file and check the \"plot-likelihood.png\" might want to change the \"p-cutoff\" in the config.yaml file so that you have only high confidnece points plotted in the video. i.e. ~0.8 or 0.9. The current default is 0.4. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pCrUvQIvoEKD"
      },
      "source": [
        "## Create labeled video:\n",
        "This funtion is for visualiztion purpose and can be used to create a video in .mp4 format with labels predicted by the network. This video is saved in the same directory where the original video resides. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6aDF7Q7KoEKE",
        "colab": {}
      },
      "source": [
        "deeplabcut.create_labeled_video(path_config_file,videofile_path, videotype=VideoType)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}